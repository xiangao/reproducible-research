[
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/",
	"title": "Tools for Reproducible Research",
	"tags": [],
	"description": "",
	"content": " This is Andrew Marder\u0026rsquo;s adaptation of Karl Broman\u0026rsquo;s Tools for Reproducible Research.\nA minimal standard for data analysis and other scientific computations is that they be reproducible: that the code and data are assembled in a way so that another group can re-create all of the results (e.g., the figures in a paper). The importance of such reproducibility is now widely recognized, but it is still not as widely practiced as it should be, in large part because many computational scientists (and particularly statisticians) have not fully adopted the required tools for reproducible research.\nIn this course, we will discuss general principles for reproducible research but will focus primarily on the use of relevant tools (particularly Make, Git, and R Markdown), with the goal that the students leave the course ready and willing to ensure that all aspects of their computational research (software, data analyses, papers, presentations, posters) are reproducible.\nDetails Prerequisite: Some knowledge of R.\nThe source for this website is on GitHub.\nRecommended Books  Christopher Gandrud\u0026rsquo;s Reproducible Research with R and RStudio\n Yihui Xie\u0026rsquo;s Dynamic Documents with R and knitr\n  Project By the end of the course, each student will have designed and completed a small project:\n Implement something in R (e.g., simulation + fancy plot). Develop it in a Git repository on GitHub. Make it an R package. Use rmarkdown to make a vignette. Use testthat to include a unit test. Make sure it passes R CMD check.  "
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/unix/",
	"title": "Unix",
	"tags": [],
	"description": "",
	"content": " My goal in this lecture is to convince you that\n command-line-based tools are the things to focus on, you need to choose a powerful, universal text editor (you\u0026rsquo;ll use it a lot), you want to be comfortable and skilled with each.  For your work to be reproducible, it needs to be code-based; don\u0026rsquo;t touch that mouse!\nWindows vs Mac OSX vs Linux Remote vs. Not\nThe Windows operating system is not very programmer-friendly.\nMac OSX isn\u0026rsquo;t either, but under the hood, it\u0026rsquo;s just unix.\nDon\u0026rsquo;t touch the mouse! Open a terminal window and start typing.\nI do most of my work directly on my desktop or laptop. You might prefer to work remotely on a server, instead. But I can\u0026rsquo;t stand having any lag in looking at graphics.\nIf you use Windows\u0026hellip; Consider Git Bash (or Cygwin)\n{\\color{lolit} (and perhaps Mintty or ComEmu)}\nCygwin is an effort to get Unix command-line tools in Windows.\nGit Bash combines git (for version control) and bash (the unix shell); it\u0026rsquo;s simpler to deal with than Cygwin.\nMintty and ConEmu are terminal emulators.\nIf you use a Mac\u0026hellip; \\centering \\Large\nConsider Homebrew and iTerm2\nAlso the XCode command line tools\nHomebrew is a packaging system; iTerm2 is a Terminal replacement.\nThe XCode command line tools are a must for most unixy things on a Mac.\nI do all of my work on a Mac (except really big computational jobs), and there are a lot of different tools that I like and would recommend:\ndivvy, http://mizage.com/divvy caffeine, http://lightheadsw.com/caffeine bartender, http://www.macbartender.com hazel, http://www.noodlesoft.com/hazel.php launchbar, http://www.obdev.at/products/launchbar/index.html simplenote, http://simplenote.com jumpcut, http://jumpcut.sourceforge.net color oracle, http://colororacle.org textexpander, http://smilesoftware.com/TextExpander\nThe command line is your friend  Don\u0026rsquo;t touch that mouse! Scriptable Flexible In the long run, you\u0026rsquo;ll be happier, having conquered the command line.  Pointing-and-clicking is not reproducible, and every time you take your hands off the keyboard, there\u0026rsquo;s a loss of efficiency.\nThe command line allows you to piece together multiple tools and so do things that weren\u0026rsquo;t anticipated by the developer of the GUI.\nAnd it\u0026rsquo;s only through scripts that you\u0026rsquo;ll have truly reproducible analyses.\nThe shell \\centerline{\\includegraphics[width=\\textwidth]{Figs/shell.png}}\nOptions: tcsh, bash, zsh\nThe shell is a program \u0026ndash; an interface to the operating system.\nThere are a number to choose from. I use bash; I\u0026rsquo;ve heard great things about zsh.\nBasics  sep18pt Directory structure  {\\color{lolit} Absolute vs. relative paths\n\\tt ls -l {~}/Figs ../Rawdata/}\n Creating, removing, changing directories  \\color{lolit mkdir\nrmdir\ncd\ncd -}\n Moving, copying, removing files  \\color{lolit mv\ncp\nrm -i}\nThis stuff is too boring to spend much time on.\nBut I should emphasize the importance of using relative paths (e.g., ../Figs/fig1.pdf) in a project; reliance on absolute paths (e.g., {~/Projects/Blah/Figs/fig1.pdf) make life difficult when you move the project to a different system. }\n{~/.bash_profile }\nexport PATH=.:/usr/local/bin:$PATH export LD_LIBRARY_PATH=/usr/local/lib noclobber=1 # prevent overwriting of files IGNOREEOF=1 # disable Ctrl-D as a way to exit HISTCONTROL=ignoredups alias cl='clear;cd' alias rm='rm -i' alias mv='mv -i' alias cp='cp -i' alias ls='ls -GF' alias 'l.'='ls -d .[a-zA-Z]*' alias ll='ls -lh' alias md='mkdir' alias rd='rmdir' alias rmb='rm .*~ *~ *.bak *.bk!' alias Rb='R CMD build --force --resave-data' alias Ri='R CMD INSTALL --library=/Users/kbroman/Rlibs' alias Rc='R CMD check --library=/Users/kbroman/Rlibs' alias Rcc='R CMD check --as-cran --library=/Users/kbroman/Rlibs'  Use the .bash_profile file to define various variables and aliases to make your life easier.\nThe most important variable is PATH: it defines the set of directories where the shell will look for executable programs. If \u0026ldquo;.\u0026rdquo; isn\u0026rsquo;t part of your PATH, you\u0026rsquo;ll need to type something like ./myscript.py to execute a script in your working directory. So put \u0026ldquo;.\u0026rdquo; in your PATH.\nMy .bash_profile file sources a .bashrc file; I don\u0026rsquo;t quite understand when one is used versus the other. Google \u0026ldquo;.bashrc vs .bash_profile.\u0026rdquo; There are links to my .bash_profile and .bashrc files on the resources page at the course web site; some of it might just be total crap.\nIf you\u0026rsquo;re using Windows and Git Bash, the .bash_profile file will be in your Documents folder (I think).\nImportant note: use of aliases within your code will create reproducibility issues; another user will need those same aliases. Consider testing your code on a more basic account.\nPATH in Windows \\only{\\figw{Figs/win_path_1.png}{0.95}} \\only{\\figw{Figs/win_path_2.png}{0.95}} \\only{\\figw{Figs/win_path_3.png}{0.95}}\nWith Git Bash, you can have a {~/.bash_profile file that adds stuff to your PATH, just as in Mac OS X and Linux.\nBut things will also be added to the PATH variable via the Path system variable and/or a Path user variable. You can get to these via the \u0026ldquo;Control panel,\u0026rdquo; but it\u0026rsquo;s a bit cumbersome.\nThe simplest way to get to the relevant dialog box seems to be to click Win-w (the little windows key and the w key) and searching for \u0026ldquo;path\u0026rdquo;. }\nRedirection and pipes $ curl -L http://bit.ly/hamlet_txt \u0026gt; hamlet.txt $ wc -l hamlet.txt $ grep Ham hamlet.txt \u0026gt; tmp.txt $ wc -l tmp.txt $ grep Ham hamlet.txt | wc -l $ grep Ham hamlet.txt | less $ cat file1.txt file2.txt \u0026gt; combined.txt $ cat file3.txt \u0026gt;\u0026gt; combined.txt  Use $\u0026gt;$ to redirect output \u0026ldquo;stdout\u0026rdquo; to a file.\nUse $\u0026gt;\u0026gt;$ to redirect output and append to the file.\nUse $\u0026lt;$ to have input \u0026ldquo;stdin\u0026rdquo; come from a file.\nUse $|$ to have the output of one command made the input to another.\nA key design principle in Unix is the piecing together of small commands using this sort of technique. There are lots of little commands (often with short, cryptic names) that can be combined together with great flexibility.\nImportant tools mentioned here: curl (for downloading web stuff on the command line; -L is to follow any re-direction; see also wget), grep (search for patterns in a file), less (look through long files a page at a time), wc (count the number of words, lines and/or characters in a file; -l is for the number of lines), cat (print contents or concatenate text files)\nWild cards $ grep blah *.txt $ ls blah.??? $ ls [a-z]* $ ls /usr/bin/[auz]* $ ls /usr/bin/[auz]*.* $ ls -l *.txt | wc -l $ wc -l *.txt | grep total  * stands for anything\n? stands for a single character\nUse [] to match some specific set or range of characters\nSuspend/foreground/background $ R CMD BATCH input.R output.txt \u0026amp; $ R CMD BATCH input.R output.txt [ctrl-Z] $ bg $ emacs afile.txt [ctrl-Z] $ fg  Use \\\u0026amp; to run a job in the background.\nUse ctrl-Z to suspend the current job (but this doesn\u0026rsquo;t work in Windows). Then use bg to then put it in the background or fg to bring it back to the foreground.\nI use ctrl-Z and bg if I had forgotten to use \\\u0026amp;.\nI use ctrl-Z with emacs sometimes, to do some command-line things without opening another shell/terminal; I\u0026rsquo;ll then use fg to bring emacs back. Or I\u0026rsquo;ll forget about it and muck a bunch of stuff up.\nMoving around the command line \\renewcommand{\\arraystretch}{1.3} \\begin{tabular}{ll} ctrl-f, ctrl-b \u0026amp; \\color{lolit} move forward and back ctrl-a, ctrl-e \u0026amp; \\color{lolit} move to beginning and end of line ctrl-k, ctrl-u \u0026amp; \\color{lolit} delete rest of line, or to the start ctrl-l \u0026amp; \\color{lolit} clear the screen ctrl-c \u0026amp; \\color{lolit} cancel what you\u0026rsquo;ve typed tab \u0026amp; \\color{lolit} autocomplete command or file ctrl-p, ctrl-n \u0026amp; \\color{lolit} forward and backward in history ctrl-r \u0026amp; \\color{lolit} search for a previous command \\end{tabular}\nThese are mostly emacs-like key \u0026ldquo;bindings\u0026rdquo;.\nHow to solve computing problems  {\\color{vhilit} Try stuff!} man pages and help files blah -h or blah --help Google Stackoverflow and other StackExchange sites Google with site:stackoverflow.com email lists and google groups friends or colleagues Twitter Buy a book. Buy {\\color{vhilit} all} of the books. You will run into crazy and mysterious errors. Will you give up, or figure them out?  Rule number 1: try stuff. Figure out how something works by trying it out in different ways.\nRule number 2: Google. Google the exact error message, or a part of an error message. You\u0026rsquo;ll often get to a stackexchange site; don\u0026rsquo;t forget to read the comments as well as the answers. Often the best answer is in a comment.\nRule number 3: Ask for help. Talk to your friends. Talk to me. Post a question at a stackexchange site.\nI\u0026rsquo;m a relatively recent convert to Twitter. I focus on just a few things that interest me (mostly academic publishing, reproducible research, and interactive graphics). If you tweet a question, you\u0026rsquo;ll be surprised at how quickly you get an answer.\nI do tend to buy all possible books on a topic that is of even passing interest to me. I read at least part of each of them.\nExamples  How do you suppress warnings in knitr? What symbol corresponds to the unicode {/u00B1}? What\u0026rsquo;s the difference between curl and wget? What does \u0026ldquo;502 Bad Gateway\u0026rdquo; mean? \u0026ldquo;To open gs you need to install X11\u0026ldquo; mclapply isn\u0026rsquo;t working in Windows How to ping a server in Python? Font shapeEU1/pplx/m/n\u0026rsquo; undefined` except KeyError, k: raise AttributeError, k These are examples of things you might search for.  If you don\u0026rsquo;t understand an error message, start by pasting it into google.\nImportant principle \\centerline{Learn to code by looking at good code.}\nIdentify programmers that you respect (e.g., Hadley Wickham), and study what they do.\nChoose a good editor  sep12pt Emacs VIM RStudio Textwrangler Notepad++ Sublime Text Atom I use emacs; I should probably use vim.  RStudio is increasingly useful, but as a general editor (for things that aren\u0026rsquo;t R), I think it\u0026rsquo;s insufficient.\nThe choice of editor is very personal.\nA good editor  sep12pt Doesn\u0026rsquo;t require pointing-and-clicking Easy to get code between R and a script Syntax highlighting of code Automatic indentation Close parentheses/brackets/braces Browse code across files Integrated with other tools (e.g., version control) I\u0026rsquo;ve not figured out how to explore code across a set of files in emacs; otherwise I\u0026rsquo;m very happy with it.  Other useful tools $ find . -name *.py $ locate article.cls $ ps ux $ top $ df -hk $ du -h $ du -hd2 $ ln -s ~/Projects/SomeFriend/Data $ ln -s ~/Projects/SomeFriend/Data SomeFriend_Data $ tar xzvf qtl_1.29-2.tar.gz $ tar czvf blah.tgz Blah/ $ tar tzvf blah.tgz  find and locate for finding files.\nps ux to see what processes are running.\ntop gives an interactive view of what processes are running.\ndf -hk shows disk usage\ndu -hd2 shows disk usage in a directory and its subdirectories; the d2 bit says go no more than 2 levels down through the subdirectories.\nln -s makes a \u0026ldquo;soft link\u0026rdquo; to a file or directory. It acts like there\u0026rsquo;s a copy, but it\u0026rsquo;s not really copied.\ntar is used to archive a bunch of files within a single file. x for extract, c for combine, t for test, {\\tt z for compress/zip, v for verbose, f for \u0026ldquo;file name to follow.\u0026rdquo; }\nFurther useful tools $ whereis bash $ type rm $ type emacs $ pwd $ head afile.txt $ tail afile.txt $ head -n20 afile.txt $ man head $ kill 8453 $ kill -9 8453 $ history $ !! $ !-2 $ !503 $ ping www.google.com $ ispell afile.txt  whereis for finding a program. type for figuring out the location of a program or the definition of an alias.\npwd \u0026ndash; print working directory\nhead \u0026ndash; print first few lines of a file tail \u0026ndash; print the last few lines of a file\nman \u0026ndash; view manual page\nkill \u0026ndash; kill a job\nhistory \u0026ndash; view command history ! \u0026ndash; execute past commands\nping \u0026ndash; see if you can connect to some server\nispell \u0026ndash; spell checker\nOpening a file from the command line Windows:\n$ start mypaper.pdf $ start http://google.com  Mac:\n$ open mypaper.pdf $ open http://google.com  I often like to open a file from the command line. If the file extension is known, you can use start in Windows or open in Mac OS X.\nIn Linux, you may have xdg-open (in the xdg-utils package on Ubuntu). You might make an alias (e.g. open) for that in your .bash_profile file.\nFile modes \\centerline{\\includegraphics[width=\\textwidth]{Figs/chmod.png}}\nNote the mode, owner, and group for each file.\nmode = read/write/executable for owner/group/everyone\nr = readable; w = writable; x = executable (for a directory, enter-able)\nFile modes/owner/group \\tt sudo chown kbroman .\nchgrp -R staff .\nchmod +x createVersionWithNotes.rb\nchmod 755 02_Unix\nchmod 644 02_Unix/02_unix.tex\nchmod 700 Private_stuff\nYou don\u0026rsquo;t usually need to change the owner or group assigned to a file or directory, but it\u0026rsquo;s good to be aware of the possibility.\nGroups are useful if you want a file accessible by some set of people but not everyone. You need a system admin to set up the group.\nYou often want to make scripts executable, or make files/directories unreadable or unwriteable.\nFor example, primary raw data files should not be writable. Large Excel-based data files often contain screwed up cells where someone was typing in some random spot without realizing it. I found myself doing that yesterday!\nThe octal codes (e.g, 755 and 644) are convenient, once you get the hang of them.\nDon\u0026rsquo;t forget to look at the resources page \\centerline{\\tt kbroman.org/Tools4RR/pages/resources.html}\nIf you find other useful resources, let me know.\nWhen we get to git and GitHub, make a pull request!\n"
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/make/",
	"title": "Make",
	"tags": [],
	"description": "",
	"content": " A minimal standard for data analysis and other scientific computations is that they be reproducible: that the code and data are assembled in a way so that another group can re-create all of the results (e.g., the figures in a paper). The importance of such reproducibility is now widely recognized, but it is not so widely practiced as it should be, in large part because many computational scientists (and particularly statisticians) have not fully adopted the required tools for reproducible research.\nIn this course, we will discuss general principles for reproducible research but will focus primarily on the use of relevant tools (particularly \\tt make, \\tt git, and \\tt knitr), with the goal that the students leave the course ready and willing to ensure that all aspects of their computational research (software, data analyses, papers, presentations, posters) are reproducible.\n Karl \u0026ndash; this is very interesting, however you used an old version of the data (n=143 rather than n=226).\nI\u0026rsquo;m really sorry you did all that work on the incomplete dataset.\nBruce\n This is an edited version of an email I got from a collaborator, in response to an analysis report that I had sent him.\nI try to always include some brief data summaries at the start of such reports. By doing so, he immediately saw that I had an old version of the data.\nBecause I\u0026rsquo;d set things up carefully, I could just substitute in the newer dataset, type \u0026ldquo;make\u0026rdquo;, and get the revised report.\nThis is a reproducibility success story. But it took me a long time to get to this point.\nReproducible vs. Replicable Computational work is reproducible if one can take the data and code and produce the same set of results. Replicable is more stringent: can someone repeat the experiment and get the same results?\nReproducibility is a minimal standard. That something is reproducible doesn\u0026rsquo;t imply that it is correct. The code may have bugs. The methods may be poorly behaved. There could be experimental artifacts.\n(But reproducibility is probably correlated with correctness.)\nNote that some scientists say replicable for what I call reproducible, and vice versa.\nLevels of quality  Are the tables and figures reproducible from the code and data? Does the code actually do what you think it does? In addition to what was done, is it clear why it was done?  (e.g., how were parameter settings chosen?)  Can the code be used for other data? Can you extend the code to do other things?  Reproducibility is not black and white. And the ideal is hard to achieve.\nBasic principles *sep12pt * Everything via code * Everything automated *[] Workflow and dependencies clearly documented * Get the data in the most-raw form possible * Get any/all data and meta-data possible * Keep track of the provenance of all data files * Be self-sufficient Pointing and clicking is not reproducible. Ideally, you press just one button.\nMake sure you have all of the data and that you know exactly where it came from.\nBut what is raw data? How far back should you go? Data that I get from collaborators has usually gone through a considerable amount of pre-processing. Should we have captured that, in order for the work to be considered reproducible?\nIf your collaborator asks, \u0026ldquo;In what form would you like the data?\u0026rdquo; you should respond, \u0026ldquo;In its current form.\u0026rdquo;\nWhy do we care?  Avoid embarrassment More likely correct Save time, in the long run Greater potential for extensions; higher impact  Doing things properly (writing clear, documented, well-tested code) is time consuming, but it could save you a ton of aggravation down the road. Ultimately, you\u0026rsquo;ll be more efficient, and your work will have greater impact.\nYour code and analyses will be easier to debug, maintain, and extend.\nYour closest collaborator is you six months ago, but you don\u0026rsquo;t reply to emails.\nI heard this from Paul Wilson, UW-Madison.\nI think he got it from a tweet by Karen Cranston: http://bit.ly/motivate_git\nWhat could go wrong?  \u0026ldquo;The attached is similar to the code we used.\u0026rdquo; \u0026ldquo;Where did this data file come from?!\u0026rdquo; \u0026ldquo;Can you repeat the analysis, omitting subject X?\u0026rdquo; \u0026ldquo;This part of your script is now giving an error.\u0026rdquo;  If you\u0026rsquo;ve not heard any of these things, it\u0026rsquo;s just a matter of time.\nNeed to avoid  Open a file to extract as CSV Open a data file to do even a slight edit Paste results into the text of a manuscript Copy-paste-edit tables Copy-paste-adjust figures  If you do anything \u0026ldquo;by hand\u0026rdquo; once, you\u0026rsquo;ll have to do it 100 times.\nBasic tools  Automation with Make Unix command line Latex and Markdown Knitr Version control with git R packages Python (or Ruby or Perl)  These are the basic tools that I think are important for reproducible computational research; they form the core topics for the course.\nMake is for automation and for documenting dependencies. For reproducibility, the command line is your best friend. Latex and Markdown allow preparation of beautiful documents without pointing or clicking. Knitr is for combining code and text; knitr and make are the key tools for reproducibility. Version control isn\u0026rsquo;t strictly necessary for reproducibility, but once you get the hang of it, you\u0026rsquo;ll never go back. R\u0026rsquo;s packaging system is among its best features. A scripting language like Python is invaluable for manipulating data files. Many things that are awkward in R are easy in Python.\nOther topics  Organizing projects Writing clear code Don\u0026rsquo;t Repeat Yourself (DRY) Testing and debugging Handling big jobs Licenses; human subjects data  We\u0026rsquo;ll also cover all of these things.\nThe organization of the data and code for a project is a major determinant of whether others will be able to make sense of it. Good code is not just correct but is clearly written. Code is easier to maintain and understand if it is modular. Adding good tests will help you to find problems in your code earlier rather than later. But you\u0026rsquo;ll spend a lot of time debugging, so we should talk about debugging strategies. Big computational jobs (particularly big computer simulations) raise additional issues; reproducibility is especially tricky. Finally, code that you distribute should be licensed. And if you\u0026rsquo;re working with data on human subjects, you need to be extra careful.\nDon\u0026rsquo;t Repeat Yourself  In code, in documentation, etc. Repeated bits of code are harder to maintain *[] Write a function Use documentation systems like Roxygen2 *[] Documentation in just one place Make use of others\u0026rsquo; code  DRY is among the more important concepts/techniques.\nFor example, I helped organize a meeting in Madison in 2013. The program book and website both drew information from a single basic source. Change that one document and both the program and web site are updated.\nMy R/qtl package is an anti-example.\nThis course  Brief intro to various tools and concepts Try everything out as we go along *[] Ask questions! I don\u0026rsquo;t know everything *[] Make suggestions! Project  Write a bit of R code Use version control Make it an R package Write a vignette   About this course: I\u0026rsquo;m trying to get you started; pointing you in the right direction. But I don\u0026rsquo;t know everything, and I don\u0026rsquo;t always do things in the most efficient way possible: please offer me suggestions!\nWe won\u0026rsquo;t have time for a comprehensive introduction to the tools. My main goal is to convince you of their importance: to motivate you to adopt them.\nThe most important tool is the mindset,\nwhen starting, that the end product\nwill be reproducible.\n Keith Baggerly  So true. Desire for reproducibility is step one.\nAutomation with GNU Make  Make is for more than just compiling software The essence of what we\u0026rsquo;re trying to do Automates a workflow Documents the workflow Documents the dependencies among data files, code Re-runs only the necessary code, based on what has changed  People usually think of Make as a tool for automating the compilation of software, but it can be used much more generally.\nTo me, Make is the essential tool for reproducible research: automation plus the documentation of dependencies and workflows. }\nExample Makefile \\begin{semiverbatim}\n# Example Makefile for a paper mypaper.pdf: mypaper.bib mypaper.tex Figs/fig1.pdf Figs/fig2.pdf pdflatex mypaper bibtex mypaper pdflatex mypaper pdflatex mypaper # cd R has to be on the same line as R CMD BATCH Figs/fig1.pdf: R/fig1.R cd R;R CMD BATCH fig1.R fig1.Rout Figs/fig2.pdf: R/fig2.R cd R;R CMD BATCH fig2.R fig2.Rout  \\end{semiverbatim}\nYou can get really fancy with make, but this example shows you the basics.\nRecords look like target: dependencies and are followed by a set of lines of code for creating the target from the dependencies. Those lines of code must start with a tab character ({\\color{nvhilit not} spaces), and if you need to change directories, you have to do that on the same line as the command.\nIf you type make (or make Makefile), the {\\tt mypaper.pdf} file will be created; but first, any dependencies will be updated, if necessary, based on the time the files were last modified.\nSo, for example, if fig1.R had been edited, then the commands to construct fig1.pdf would be constructed, followed by the commands to construct mypaper.pdf. }\nFancier example \\begin{semiverbatim}\nFIG_DIR = Figs mypaper.pdf: mypaper.tex ${FIG_DIR}/fig1.pdf ${FIG_DIR}/fig2.pdf pdflatex mypaper # One line for both figures ${FIG_DIR}/%.pdf: R/%.R cd R;R CMD BATCH $(\u0026lt;F) # Use \u0026quot;make clean\u0026quot; to remove the PDFs clean: rm *.pdf Figs/*.pdf  \\end{semiverbatim}\nAs I said, you can get really fancy with GNU Make.\nUse variables for directory names or compiler flags. (This example is not a good one.)\nUse pattern rules and automatic variables to avoid repeating yourself. With \\%, we have one line covering both fig1.pdf and fig2.pdf. The \\$(\u0026lt;F) is the file part of the first dependency.\nLook at the manual for make and the many online tutorials, such as the one from Software Carpentry.\nHow do you use make? {\\small * If you name your make file Makefile, then just go into the directory containing that file and type \\color{hilit make}\n If you name your make file something.else, then type \\color{hilit make -f something.else}\n Actually, the commands above will build the {\\color{vhilit} first} target listed in the make file. So I\u0026rsquo;ll often include something like the following.\n  \\begin{quote} \\color{hilit all: target1 target2 target3} \\end{quote}\nThen typing \\color{hilit make all} (or just {\\tt \\color{hilit} make}, if \\color{hilit all} is listed first in the file) will build all of those things.\n To be build a specific target, type \\color{hilit make target}. For example, \\color{hilit make Figs/fig1.pdf} }  I can\u0026rsquo;t believe that I forgot to explain this the first time I gave this lecture.\n"
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/rmarkdown/",
	"title": "R Markdown",
	"tags": [],
	"description": "",
	"content": " Statisticians write a lot of reports, describing the results of data analyses. It\u0026rsquo;s best if such reports are fully reproducible: that the data and code are available, and that there\u0026rsquo;s a clear and automatic path from data and code to the final report.\nknitr is ideal for this effort. It\u0026rsquo;s a system for combining code and text into a single document. Process the document, and the code is replaced with the results and figures that it generates.\nI\u0026rsquo;ve found it most efficient to produce informal analysis reports as web pages. Markdown is a system for writing simple, readable text, with the sort of marks that you might use in an email message, that gets converted to nicely formatted html-based web pages.\nMy goal in this lecture is to show you how to use knitr with R Markdown (a variant of Markdown) to make such reproducible reports, and to convince you that this is the way that you should be constructing such analysis reports.\nI\u0026rsquo;d originally planned to also cover knitr with AsciiDoc, but I decided to drop it; it\u0026rsquo;s best to focus on Markdown.\nknitr in a knutshell \\centerline{\\Large \\tt kbroman.org/knitr_knutshell}\nI wrote a short tutorial on knitr, covering a bit more than I\u0026rsquo;ll cover in this lecture.\nI\u0026rsquo;d be glad for suggestions, corrections, or questions.\nData analysis reports  Figures/tables + email Static \\LaTeX\\ or Word document knitr/Sweave + \\LaTeX\\ -\u0026gt; PDF knitr + Markdown -\u0026gt; Web page Statisticians write a lot of reports. You do a bunch of analyses, create a bunch of figures and tables, and you want to describe what you\u0026rsquo;ve done to a collaborator.  When I was first starting out, I\u0026rsquo;d create a bunch of figures and tables and email them to my collaborator with a description of the findings in the body of the email. That was cumbersome for me and for the collaborator. (\u0026ldquo;Which figure are we talking about, again?\u0026rdquo;)\nI moved towards writing formal reports in \\LaTeX\\ and sending my collaborator a PDF. But that was a lot of work, and if I later wanted to re-run things (e.g., if additional data were added), it was a real hassle.\nSweave + \\LaTeX\\ was a big help, but it\u0026rsquo;s a pain to deal with page breaks.\nWeb pages, produced with knitr and Markdown, are ideal. You can make super-tall multi-panel figures that show the full details, without worrying page breaks. And hyperlinks are more convenient, too.\n\\begin{frame}[c]{}\n\\centering What if the data change?\nWhat if you used the wrong version of the data?\nIf data are added, will it be easy to go back and re-do your analyses, or is there a lot of copying-and-pasting and editing to be done?\nI usually start an analysis report with a summary of the experiment, scientific questions, and the data. Recently, a collaborator noticed that I\u0026rsquo;d used an old version of the data. (I\u0026rsquo;d cited sample sizes, and so he could see that I didn\u0026rsquo;t have the full set.)\nHe said, \u0026ldquo;I\u0026rsquo;m really sorry you did all that work on the incomplete dataset.\u0026rdquo;\nBut actually, it didn\u0026rsquo;t take long to find the right file, and the revised analysis was derived instantaneously, as I\u0026rsquo;d used knitr.\nknitr code chunks Input to knitr:\nWe see that this is an intercross with `r nind(sug)` individuals. There are `r nphe(sug)` phenotypes, and genotype data at `r totmar(sug)` markers across the `r nchr(sug)` autosomes. The genotype data is quite complete. \u0026quot;`{r summary_plot, fig.height=8} plot(sug) \u0026quot;`  Output from knitr:\nWe see that this is an intercross with 163 individuals. There are 6 phenotypes, and genotype data at 93 markers across the 19 autosomes. The genotype data is quite complete. \u0026quot;`r plot(sug) \u0026quot;` ![plot of chunk summary_plot](RmdFigs/summary_plot.png)  The basic idea in knitr is that your regular text document will be interrupted by chunks of code delimited in a special way.\nThis example is with R Markdown.\nThere are in-line bits of code indicated with backticks. When the document is processed by knitr, they\u0026rsquo;ll be evaluated and replaced by the result.\nLarger code chunks with three backticks. This one will produce a plot. When processed by knitr, an image file will be created and a link to the image will be inserted at that location.\nIn knitr, different types of text have different ways of delimiting code chunks, because it\u0026rsquo;s basically going to do a search-and-replace and depending on the form of text, different patterns will be easier to find.\nhtml \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=utf-8\u0026quot;/\u0026gt; \u0026lt;title\u0026gt;Example html file\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Markdown example\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Use a bit of \u0026lt;strong\u0026gt;bold\u0026lt;/strong\u0026gt; or \u0026lt;em\u0026gt;italics\u0026lt;/em\u0026gt;. Use backticks to indicate \u0026lt;code\u0026gt;code\u0026lt;/code\u0026gt; that will be rendered in monospace.\u0026lt;/p\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;This is part of a list\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;another item\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;  [Example]\nIt\u0026rsquo;s helpful to know a bit of html, which is the markup language that web pages are written in. html really isn\u0026rsquo;t that hard; it\u0026rsquo;s just cumbersome.\nAn html document contains pairs of tags to indicate content, like \u0026lt;h1\u0026gt; and \u0026lt;/h1\u0026gt; to indicate that the enclosed text is a \u0026ldquo;level one header\u0026rdquo;, or \u0026lt;em\u0026gt; and \u0026lt;/em\u0026gt; to indicate emphasis (generally italics). A web browser will parse the html tags and render the web page, often using a cascading style sheet (CSS) to define the precise style of the different elements.\nNote that there are six levels of headers, with tags \u0026lt;h1\u0026gt;, \u0026lt;h2\u0026gt;, \u0026lt;h3\u0026gt;, \\dots, \u0026lt;h6\u0026gt;. Think of these as the title, section, subsection, sub-subsection, \\dots\nCSS ul,ol { margin: 0 0 0 35px; } a { color: purple; text-decoration: none; background-color: transparent; } a:hover { color: purple; background: #CAFFFF; }  {\\footnotesize \\lolit [Example]}\nI don\u0026rsquo;t really want to talk about CSS, but I thought I should at least acknowledge its existence.\nCSS is really important for defining how your document will appear. Much of the time, you just want to find someone else\u0026rsquo;s CSS document that is satisfactory to you.\nMarkdown # Markdown example Use a bit of **bold** or _italics_. Use backticks to indicate `code` that will be rendered in monospace. - This is part of a list - another item Include blocks of code using three backticks: \u0026quot;` x \u0026lt;- rnorm(100) \u0026quot;` Or indent four spaces: mean(x) sd(x) And it's easy to create links, like to [Markdown](http://daringfireball.net/projects/markdown/).  {\\footnotesize \\lolit [Example | MD cheat sheet]}\nMarkdown is a system for writing simple, readable text that is easily converted into html. The reason it\u0026rsquo;s useful to know a bit of html is that then you have a better idea how the final product will look. (Plus, if you want to get fancy, you can just insert a bit of html within the Markdown document.)\nMarkdown is just a system of marks that will get searched-and- replaced to create an html document. A big advantage of the Markdown marks is that the source document is much like what you might write in an email, and so it\u0026rsquo;s much more human-readable.\nGithub (which we\u0026rsquo;ll talk about next week) automatically renders Markdown files as html, and you can use Markdown for ReadMe files. And the website for this course is mostly in Markdown.\nR Markdown  sep12pt R Markdown is a variant of Markdown, developed at RStudio.com Markdown + knitr + extras A few extra marks \\LaTeX\\ equations Bundle images into the final html file R Markdown is a variant of Markdown developed by the folks at RStudio.  It\u0026rsquo;s Markdown with knitr code chunks, but there are a number of added features, most importantly the ability to use \\LaTeX\\ equations.\nCode chunks, again \u0026quot;`{r knitr_options, include=FALSE} knitr::opts_chunk$set(fig.width=12, fig.height=4, fig.path='Figs/', warning=FALSE, message=FALSE) set.seed(53079239) \u0026quot;` ### Preliminaries Load the R/qtl package using the `library` function: \u0026quot;`{r load_qtl} library(qtl) \u0026quot;` To get help on the read.cross function in R, type the following: \u0026quot;`{r help, eval=FALSE} ?read.cross \u0026quot;`  {\\footnotesize \\lolit [Example]}\nA couple of additional points about code chunks.\nYou can (and should) assign names to the code chunks. It will make it easier to fix errors, and figure files will be named based on the name of the chunk that produces them.\nCode chunks can also have options, like include=FALSE and eval=FALSE. And you can define global options, which will apply to all subsequent chunks.\nChunk options \\renewcommand{\\arraystretch}{1.3} \\begin{tabular}{ll} echo=FALSE \u0026amp; \\lolit Don\u0026rsquo;t include the code results=\u0026quot;hide\u0026quot; \u0026amp; \\lolit Don\u0026rsquo;t include the output include=FALSE \u0026amp; \\lolit Don\u0026rsquo;t show code or output eval=FALSE \u0026amp; \\lolit Don\u0026rsquo;t evaluate the code at all warning=FALSE \u0026amp; \\lolit Don\u0026rsquo;t show R warnings message=FALSE \u0026amp; \\lolit Don\u0026rsquo;t show R messages fig.width=\\# \u0026amp; \\lolit Width of figure fig.height=\\# \u0026amp; \\lolit Height of figure fig.path=\u0026quot;Figs/\u0026quot; \u0026amp; \\lolit Path for figure files \\end{tabular}\nThere are lots of chunk options.\nThese are the chunk options that I use most, but there are lots more. Each should be valid R code, and can be basically any valid R code, so you can get pretty fancy.\nThe ending slash in fig.path is important, as this is just pasted to the front of the figure file names. If not included, the figures would be in the main directory but with names starting with \u0026ldquo;Figs\u0026rdquo;.\nGlobal chunk options \u0026quot;`{r knitr_options, include=FALSE} knitr::opts_chunk$set(fig.width=12, fig.height=4, fig.path='Figs/', warning=FALSE, message=FALSE, include=FALSE, echo=FALSE) set.seed(53079239) \u0026quot;` \u0026quot;`{r make_plot, fig.width=8, include=TRUE} x \u0026lt;- rnorm(100) y \u0026lt;- 2*x + rnorm(100) plot(x, y) \u0026quot;`   sep12pt Use global chunk options rather than repeat the same options over and over. You can override the global values in specific chunks. I\u0026rsquo;ll often use include=FALSE and echo=FALSE in a report to a collaborator, as they won\u0026rsquo;t want to see the code and raw results. I\u0026rsquo;ll then use include=TRUE for the figure chunks.  And I\u0026rsquo;ll set some default choice for figure heights and widths but then adjust them a bit in particular figures.\nYou may need to include {/library(knitr) before the opts_chunk\\$set() (for example, within RStudio). }\nPackage options \u0026quot;`{r package_options, include=FALSE} knitr::opts_knit$set(progress = TRUE, verbose = TRUE) \u0026quot;`   sep12pt It\u0026rsquo;s easy to confuse global chunk options with package options. I\u0026rsquo;ve not used package options. So focus on opts_chunk\\$set() not {\\tt \\lolit opts_knit\\$set()}. If you are doing something fancy, you may need knitr package options, but I\u0026rsquo;ve not used them.  I\u0026rsquo;ve gotten confused about them, though: opts_chunk\\$set vs. opts_knit\\$set.\nIn-line code We see that this is an intercross with `r nind(sug)` individuals. There are `r nphe(sug)` phenotypes, and genotype data at `r totmar(sug)` markers across the `r nchr(sug)` autosomes. The genotype data is quite complete.   sep12pt Each bit of in-line code needs to be within one line; they can\u0026rsquo;t span across lines. I\u0026rsquo;ll often precede a paragraph with a code chunk with {\\tt include=FALSE}, defining various variables, to simplify the in-line code. Never hard-code a result or summary statistic again! In-line code to insert summary statistics and such is a key feature of knitr.  Even if you wanted the code for your figures or data analysis to be separate, you\u0026rsquo;d still want to make use of this feature.\nRemember my anecdote earlier in this lecture: if I hadn\u0026rsquo;t mentioned sample sizes, my collaborator wouldn\u0026rsquo;t have noticed that I was using an old version of the data.\nYAML header --- title: \u0026quot;knitr/R Markdown example\u0026quot; author: \u0026quot;Karl Broman\u0026quot; date: \u0026quot;28 January 2015\u0026quot; output: html_document ---  --- title: \u0026quot;Another knitr/R Markdown example\u0026quot; author: \u0026quot;[Karl Broman](http://kbroman.org)\u0026quot; date: \u0026quot;`r Sys.Date()`\u0026quot; output: word_document ---  At the top of your Rmd file, it\u0026rsquo;s best to include a header like the above examples. (YAML is a simple text-based format for specifying data, sort of like JSON but more human-readable.)\nYou don\u0026rsquo;t have to include any of these things, but it\u0026rsquo;s good to at least specify output: (which can be also be {\\tt pdf_document). There are a lot more options; see rmarkdown.rstudio.com.\nNote my use of a hyperlink and some R code in the second example. These will carry over to the final document. }\nRounding  sep18pt cor(x,y) might produce \\vhilit 0.8992877, but I want 0.90.\n round(cor(x,y), 2), would give \\vhilit 0.9, but I want 0.90.\n You could use sprintf(\u0026quot;\\%.2f\u0026quot;, cor(x,y)), but sprintf(\u0026quot;\\%.2f\u0026quot;, -0.001) gives \\vhilit -0.00.\n Use the myround function in my R/broman package.\n myround(cor(x,y), 2) solves both issues. I\u0026rsquo;m very particular about rounding. You should be too.\n  If you\u0026rsquo;re a C programmer, sprintf seems natural. No one else agrees.\nThe R/broman package is on both github and CRAN.\nR Markdown -\u0026gt; html, in RStudio \\centerline{\\includegraphics[width=\\textwidth]{Figs/rstudio_knitr.png}}\nThe easiest way to convert an R Markdown file to html is with RStudio.\nOpen the R Markdown file in R Studio and click the \u0026ldquo;Knit HTML\u0026rdquo; button (with the ball of yarn and knitting needle).\nNote the little button with a question mark. Click that, and you\u0026rsquo;ll get the \u0026ldquo;Markdown Quick Reference.\u0026rdquo;\nWhat actually happens: The knit function in the knitr package processes all of the code chunks and in-line code and creates a Markdown file and possibly a bunch of figure files. The Markdown file (and any figure files) are sent to Pandoc, which converts them to an HTML file, with embedded figures.\nRStudio is especially useful when you\u0026rsquo;re first learning R Markdown and knitr, as it\u0026rsquo;s easy to create and view the corresponding html file, and you have access to that Markdown Quick Reference.\nR Markdown -\u0026gt; html, in R \u0026gt; library(rmarkdown) \u0026gt; render(\u0026quot;knitr_example.Rmd\u0026quot;)  \u0026gt; rmarkdown::render(\u0026quot;knitr_example.Rmd\u0026quot;)  When you click the \u0026ldquo;Knit HTML\u0026rdquo; button in RStudio, what it actually does is run rmarkdown::render(), which in turn calls knitr::knit() and then runs pandoc.\nYou can do the same thing directly, in R. You do miss out on the immediate preview of the result.\nR Markdown -\u0026gt; html, [GNU make](http://www.gnu.org/software/make)  knitr_example.html: knitr_example.Rmd R -e \u0026quot;rmarkdown::render('knitr_example.Rmd')\u0026quot;  I prefer to do this from the command-line, using a Makefile. Then it\u0026rsquo;s more obvious what\u0026rsquo;s happening.\nIn Windows, it\u0026rsquo;s important that the double-quotes are on the outside and the single-quotes are on the inside.\nNeed pandoc in your PATH RStudio includes pandoc; you just need to add the relevant directory to your PATH.\nMac:\n{\\ttsm /Applications/RStudio.app/Contents/MacOS/pandoc}\nWindows:\n{\\ttsm \u0026ldquo;c:{/}Program Files{/}RStudio{/}bin{/}pandoc\u0026rdquo;}\nTo use the rmarkdown package from the command line, you need access to pandoc. But if you\u0026rsquo;ve installed RStudio (and I { highly recommend that you do), you don\u0026rsquo;t need to do a separate install, as pandoc is included with RStudio.\nYou just need to add the relevant directory (listed above) to your PATH, for example in your {~/.bash_profile} file.\nAt the command line, type type pandoc or pandoc --version to check that it\u0026rsquo;s available.\n}\nReproducible knitr documents  sep8pt Don\u0026rsquo;t use absolute paths like \\vhilit {~/Data/blah.csv} Keep all of the code and data in one directory (and its subdirectories) If you {\\vhilit must} use absolute paths, define the various directories with variables at the top of your document. Use R --vanilla or perhaps \\scriptsize R --no-save --no-restore --no-init-file --no-site-file Use GNU make to document the construction of the final product (tell future users what to do) Include a final chunk with getwd() and devtools::session_info(). For simulations, use set.seed in your first chunk. That you\u0026rsquo;ve used knitr doesn\u0026rsquo;t mean the work is really {\\nvhilit reproducible. The source and data need to be available to others, they need to know what packages were used and how to compile it, and then they need to be able to compile it on their system.  The complicated alternative to R --vanilla is if you want to still load {~/.Renviron}, for example, to define R_LIBS.\nIf you use set.seed at the top of the document, it should be that the random aspects will give exactly the same results. I\u0026rsquo;ll use \\ runif(1, 0, 10{\\textasciicircum8)} and then paste that big number within set.seed().\nTwo anecdotes: The github repository for the Reproducible Research with R and R Studio book uses some absolute paths that basically make it not reproducible.\nEarn et al. (2014) Proc Roy Soc B 281(1778):20132570 has a really nice supplement, written with knitr. But it says, \u0026ldquo;The source code is available upon request.\u0026rdquo; It\u0026rsquo;s not {\\nvhilit really} reproducible, then. }\nControlling figures \u0026quot;`{r test_figure, dev.args=list(pointsize=18)} x \u0026lt;- rnorm(100) y \u0026lt;- 2*x + rnorm(100) plot(x,y) \u0026quot;`  {\\small * sep8pt * The default is for knitr/R Markdown is to use the png() graphics device. * Use another graphics device with the chunk option dev. * Pass arguments to the graphics device via the chunk option dev.args. }\nGraphics in knitr are super easy. For the most part, you don\u0026rsquo;t have to do anything! If a code chunk produces a figure, it will be inserted.\nBut depending on the type of figure, you might want to try different graphics devices. And sometimes you want to pass arguments to the graphics device.\nYesterday (6 Feb 2014), to change the size of axis labels, you couldn\u0026rsquo;t just use the pointsize device argument; you\u0026rsquo;d also need to use something like par(cex.lab=1.5). But I posted a question about it on StackOverflow, and Yihui Xie responded and then immediately fixed the problem. I used a bit of twitter in there too, to get his attention.\nTo download and install the development version of knitr, you can use the install_github function in Hadley Wickham\u0026rsquo;s devtools package. Use install.packages(\u0026quot;devtools\u0026quot;) if you don\u0026rsquo;t already have it installed. Then library(devtools) and install_github(\u0026quot;yihui/knitr\u0026quot;).\nTables \u0026quot;`{r kable} x \u0026lt;- rnorm(100) y \u0026lt;- 2*x + rnorm(100) out \u0026lt;- lm(y ~ x) coef_tab \u0026lt;- summary(out)$coef library(kable) kable(coef_tab, digits=2) \u0026quot;`  \u0026quot;`{r pander} library(pander) panderOptions(\u0026quot;digits\u0026quot;, 2) pander(out, caption=\u0026quot;Regression coefficients\u0026quot;) \u0026quot;`  \u0026quot;`{r xtable, results=\u0026quot;asis\u0026quot;} library(xtable) tab \u0026lt;- xtable(coef_tab, digits=c(0, 2, 2, 1, 3)) print(tab, type=\u0026quot;html\u0026quot;) \u0026quot;`  In informal reports, I\u0026rsquo;ll often just print out a matrix or data frame, rather than create a formal table.\nBut there are multiple ways to make tables with R Markdown that may look a bit nicer. I\u0026rsquo;m not completely happy with any of them, but maybe I\u0026rsquo;ve just not figured out the right set of options.\nkable in the knitr package is simple but can\u0026rsquo;t be customized too much. But it can produce output as pandoc, markdown, html, or latex.\nThe pander package produces pandoc-based tables, which even work if you\u0026rsquo;re making a Word document, and has a bit more control than kable.\nThe xtable package gives you quite complete control, but only produces latex or html output. You need to be sure to use results=\u0026quot;asis\u0026quot; in the code chunk.\nImportant principles \\centering Modify your desires to match the defaults.\nFocus your compulsive behavior on things that matter.\nFocus on the text and the figures before worrying too much about fine details of how they appear on the page.\nAnd consider which is more important: a manuscript, web page, blog, grant, course slides, course handout, report to collaborator, scientific poster.\nYou can spend a ton of time trying to get things to look just right. Ideally, you spend that time trying to construct a general solution. Or you can modify your desires to more closely match what you get without any effort.\nKnitR + LaTeX -\u0026gt; paper This lecture is about how to create reproducible manuscripts, for journal articles. KnitR with R Markdown is great for informal reports. KnitR with AsciiDoc is great for somewhat fancier reports. There are a number of efforts, especially with Pandoc, to use R Markdown for journal articles. But if you want fine control over the appearance of a document, it\u0026rsquo;s hard to beat \\LaTeX, and so I\u0026rsquo;m just going to focus on that.\nI can\u0026rsquo;t hope to explain \\LaTeX/ properly in just this one lecture. My goals are to give the general gist, indicate resources and options, and show how to use KnitR with \\LaTeX.\nI also want to discuss some more general strategies for ensuring that the results described in a journal article are fully reproducible.\n\\LaTeX \\documentclass[12pt]{article} \\usepackage{graphicx} \\title{An example document} \\author{Karl Broman} \\begin{document} \\maketitle \\thispagestyle{empty} \\section{A section} This is a simple example of a \\LaTeX/ document for an article. Here's some in-line math: $y = \\beta_0 + \\beta_1 x + \\epsilon$. And here's a display equation: $$ \\hat{\\beta} = (X'X)^{-1} X'y $$  \\LaTeX/ is like html or Markdown: plain text with special codes to indicate how things are to appear.\nA \\LaTeX/ document always starts with {/documentclass, then a bunch of overall controlling information. The actual document is between {/begin{document}} and {/end{document}}.\n{/usepackage{}} is like library() in R.\nIdeally, you focus on {\\nhilit semantics} rather than {\\nhilit style}: define the {/title{}} and {/author{}} and use {/maketitle} to have them included in the document, and indicate sections and subsections with {/section{}} and {/subsection{}}.\nFor some reason, {/thispagestyle{empty}} (\u0026ldquo;don\u0026rsquo;t show page number on this page\u0026rdquo;) needs to be placed {\\nhilit after} {/maketitle}.\nA key feature of \\LaTeX/ is the mathematics typesetting. There\u0026rsquo;s no better system. And your \\LaTeX/ skills can be immediately transferred to your Markdown documents, with MathJax. }\nWhat I actually do \\documentclass[12pt]{article} \\setlength{\\headheight}{10pt} \\setlength{\\headsep}{15pt} \\setlength{\\topmargin}{-25pt} \\setlength{\\topskip}{0in} \\setlength{\\textheight}{8.7in} \\setlength{\\footskip}{0.3in} \\setlength{\\oddsidemargin}{0.0in} \\setlength{\\evensidemargin}{0.0in} \\setlength{\\textwidth}{6.5in} \\begin{document} \\begin{center} \\textbf{\\large An example document} Karl Broman \\end{center} \\textbf{\\sffamily A section}  In reality, for a paper, I don\u0026rsquo;t use {/maketitle or {/section}, but rather just muck about, hard-coding the placement of things.\nBut mine is not the recommended approach. If, for some reason, you need to change the style, it\u0026rsquo;s easier if your document is defined in terms of {\\nhilit semantics}. }\nWhy \\LaTeX/?  Fine control of document appearance Transparency of how that was achieved Version control (diff/merge) Typesetting equations Markdown\u0026rsquo;s not quite ready, or sufficiently rich  [] (but see the R package rticles)    It\u0026rsquo;s {\\nhilit a lot of work to learn \\LaTeX, so we need to be clear about why we\u0026rsquo;d want to devote the effort to it.\nFor reproducible research, we need some sort of code-based document system (i.e., {\\nvhilit not Word!}), and \\LaTeX/ gives you the most fine-grained control, if you need it. Ultimately, I hope, Markdown will be sufficient, but for now, we often need \\LaTeX/.\nThe code-based control makes what you\u0026rsquo;re trying to do transparent. And you should treat \\LaTeX/ like code: write clearly and simply, and comment the tricky bits.\nThis sort of document also has the advantage of easy treatment of diff and merge in a version control system like git.\nThe real power of \\LaTeX/ is in the typesetting of mathematical equations. And what you learn on that aspect can be transferred to your Markdown documents, using MathJax. (But I already said that, didn\u0026rsquo;t I?) }\n\\begin{frame}[c]{}\n\\centerline{\\Large simple \\quad $\\longleftrightarrow$ \\quad flexible}\n\\onslide{\\centerline\\scriptsize \\lolit {/centerline{{/}Large simple {/}quad \\${/}longleftrightarrow\\$ {/}quad flexible}}}\n\\LaTeX/ sits at the right of the simple-to-flexible spectrum.\n\\begin{frame}[c]{}\n\\centering Modify your desires to match the defaults.\nFocus your compulsive behavior on things that matter.\nI\u0026rsquo;ve said this before, but I like to repeat it.\nFocus on the text and the figures before worrying too much about fine details of how they appear on the page.\nAnd consider which is more important: a manuscript, web page, blog, grant, course slides, course handout, report to collaborator, scientific poster.\nYou can spend a ton of time trying to get things to look just right. Ideally, you spend that time trying to construct a general solution. Or you can modify your desires to more closely match what you get without any effort.\nStuff I use a lot % other fonts \\usepackage{palatino} \\usepackage{times} \\setlength{\\rightskip}{0pt plus 1fil} % makes ragged right \\newcommand{\\LOD}{\\text{LOD}} \\usepackage{setspace} \\setstretch{2.0} \\addtocounter{framenumber}{-1} % make figures S1, S2, ... \\renewcommand{\\thefigure}{\\textbf{S\\arabic{figure}}} \\renewcommand{\\figurename}{\\textbf{Figure}} % bigger space between rows in tables \\renewcommand{\\arraystretch}{1.5} % paragraphs not indented but have space between \\setlength{\\parskip}{6pt} \\setlength{\\parindent}{0pt}  These are bits of \\LaTeX/ code that I use a lot.\nKnitR + \\LaTeX -\u0026gt; Rnw \\documentclass[12pt]{article} \\title{An example Rnw document} \\author{Karl Broman} \\begin{document} \\maketitle \u0026lt;\u0026lt;load_library, echo=FALSE, results=\u0026quot;hide\u0026quot;\u0026gt;\u0026gt;= library(broman) # used for myround() @ \u0026lt;\u0026lt;example_chunk\u0026gt;\u0026gt;= x \u0026lt;- rnorm(100) y \u0026lt;- 5*x + rnorm(100) lm.out \u0026lt;- lm(y ~ x) plot(x,y) abline(lm.out$coef) @ The estimated slope is \\Sexpr{myround(lm.out$coef[2], 1)}.  KnitR + \\LaTeX -\u0026gt; Rnw \\addtocounter{framenumber}{-1}\n\\documentclass[12pt]{article} \\title{An example Rnw document} \\author{Karl Broman} \\begin{document} \\maketitle \u0026lt;\u0026lt;load_library, echo=FALSE, results=\u0026quot;hide\u0026quot;\u0026gt;\u0026gt;= library(broman) # used for myround() @ \u0026lt;\u0026lt;example_chunk, out.width=\u0026quot;0.8\\\\textwidth\u0026quot;\u0026gt;\u0026gt;= x \u0026lt;- rnorm(100) y \u0026lt;- 5*x + rnorm(100) lm.out \u0026lt;- lm(y ~ x) plot(x,y) abline(lm.out$coef) @ The estimated slope is \\Sexpr{myround(lm.out$coef[2], 1)}.  KnitR works well with LaTeX.\nMost of what you learned about KnitR with R Markdown transfers directly to working with LaTeX.\nThe main difference is the way in which code chunks are indicated. You use \u0026lt;\u0026lt;\u0026gt;\u0026gt;= and @ for chunks, and {/Sexpr{} for in-line code.\nKnitR basically does a search-and-replace for code chunks. Different patterns will be easier, depending on the nature of the surrounding code.\nThe chunk options are the same. Here, I used out.width=\u0026quot;0.8{/textwidth\u0026rdquo;} to make the figure appear as 80\\% of the width of the page.\nout.width and out.height need units as in \\LaTeX/ (built into {//textwidth}; otherwise \u0026quot;in\u0026quot; or \u0026quot;cm\u0026quot; or \u0026quot;pt\u0026quot; or whatever).\nfig.width and fig.height are as in R, with implied units. }\nLyX \\figh{Figs/lyx.png}{0.7}\nlyx.org\nI create \\LaTeX/ documents in emacs. If you want something WYSIWYG, consider LyX. KnitR is built in, and Yihui Xie strongly endorses it. (LyX is not really \u0026ldquo;WYSIWYG\u0026rdquo; but rather \u0026ldquo;WYSIWYM,\u0026rdquo; but that\u0026rsquo;s what you want most, anyway.)\nAlso  Overleaf ShareLaTeX Authorea Verbosus  There are a bunch of online tools for creating LaTeX documents, collaboratively.\nI have no experience with these, but I\u0026rsquo;ve heard good things about Overleaf (formerly WriteLaTeX).\nFlavors of \\LaTeX  \\LaTeX pdflatex xelatex lualatex  In addition to regular \\LaTeX, there\u0026rsquo;s pdflatex (which I mostly use). It has the advantage of being able to include pdf, jpg, and png figures, and produces a PDF file directly.\nXeLaTeX and LuaLaTeX are great for fonts and Unicode.\nI\u0026rsquo;ve not mentioned that behind the scenes is \\TeX, which is the source of all of this. Believe or not, \\LaTeX/ exists because \\TeX/ is even harder. PdfLaTeX, XeLaTeX, and LuoLaTeX, really derive from PdfTeX, XeTex, and LuoTex.\nGetting help  Google tex.stackexchange.com Ask a friend Look at others\u0026rsquo; documents Resign yourself to something less-than-ideal  There is {\\nhilit a ton of online information about \\LaTeX. Start with google. It\u0026rsquo;s highly unlikely that you have a completely unique question or problem.\nMy last point here is basically that one way to help yourself is by learning to let things go. }\nFigure captions and floats \u0026lt;\u0026lt;fig_with_caption, fig.cap=\u0026quot;Scatterplot of $y$ vs $x$\u0026quot;\u0026gt;\u0026gt;= x \u0026lt;- rnorm(100) y \u0026lt;- 5*x + rnorm(100) lm.out \u0026lt;- lm(y ~ x) plot(x,y) abline(lm.out$coef) @  \\begin{figure}[] \\includegraphics{figure/fig_with_caption} \\caption{Scatterplot of $y$ vs $x$\\label{fig:fig_with_caption}} \\end{figure}  If you use the chunk option fig.cap, the figure will get a caption.\nBut it will also be embedded within a figure \u0026ldquo;environment.\u0026rdquo; (That is, between {/begin{figure} and {/end{figure}}.)\nThis makes it a \u0026ldquo;float.\u0026rdquo; \\LaTeX/ decides where it\u0026rsquo;s going to be placed. The placement of floats is {\\nvhilit the biggest pain} in using \\LaTeX.\nThe figure also gets a label, from the chunk name. (The {/label{}} bit.) This allows you to cross-reference the figure, to have the figure number determined automatically.\nThe cross reference would be with {/ref{fig:fig_with_caption}}.\nWhen you use cross references, you need to run \\LaTeX/ twice: once to establish where things will sit on the page and how they are numbered, and a second time to insert the cross references. }\nTables in \\LaTeX \\begin{tabular}{rrrrr} \\hline \u0026amp; Estimate \u0026amp; Std. Error \u0026amp; t value \u0026amp; Pr($\u0026gt;$$|$t$|$) \\\\ \\hline (Intercept) \u0026amp; 0.04 \u0026amp; 0.11 \u0026amp; 0.4 \u0026amp; 0.69 \\\\ x \u0026amp; 0.98 \u0026amp; 0.10 \u0026amp; 10.0 \u0026amp; 0.00 \\\\ \\hline \\end{tabular}  Tables in \\LaTeX/ are a pain, but they offer extremely fine control.\nBut writing this sort of code (\\\u0026amp; indicates breaks between columns, {/{/} indicates the end of a row) {\\nhilit reproducibly} is hard. }\nxtable \u0026lt;\u0026lt;generate_and_fit\u0026gt;\u0026gt;= x \u0026lt;- rnorm(100) y \u0026lt;- x + rnorm(100) lm.out \u0026lt;- lm(y ~ x) @ \u0026lt;\u0026lt;table, results=\u0026quot;asis\u0026quot;\u0026gt;\u0026gt;= library(xtable) xtable(lm.out, digits=c(0,2,2,1,2)) @ % a non-floating version \u0026lt;\u0026lt;table, results=\u0026quot;asis\u0026quot;\u0026gt;\u0026gt;= library(xtable) xtab \u0026lt;- xtable(lm.out, digits=c(0,2,2,1,2)) print(xtab, floating=FALSE) @  xtable is a superb R package for producing \\LaTeX/ tables. You don\u0026rsquo;t have complete control, but you do have a ton of control. The xtableGallery vignette shows you much of what can be done.\nNote that a lot of the options are for print.xtable, so look at the help files for both xtable and print.xtable.\nFor example, if you {\\nhilit don\u0026rsquo;t want a table to be \u0026ldquo;floating,\u0026rdquo; (within a table environment, between {/begin{table}} and {/end{table}}), you need to use print.table with floating=FALSE. }\nRead proofs carefully As submitted\n\\figw{Figs/rigenome_as_submitted.png}{0.5}\nAs printed\n\\figw{Figs/rigenome_error.png}{0.5}\n\\scriptsize Broman (2005) Genetics 169:1133{\\textendash1146}\nSome journals re-type a bunch of your manuscript, sometimes introducing errors.\nSo read proofs {\\nhilit carefully. (What pain!) And post a preprint, say to arXiv.org or bioRxiv.org.\nThe above is the most important equation in the paper, and I missed that they\u0026rsquo;d introduced a mistake. }\nRe-type that! \\figh{Figs/preCC_table.png}{0.65}\n\\scriptsize Broman (2012) Genetics 190:403{\\textendash412}\nI have a few papers with {\\nhilit a lot of equations. I hope they\u0026rsquo;re not trying to re-type these. I generated them from code. }\nBibTeX for bibliographies %bibliography format \\usepackage[authoryear]{natbib} \\bibpunct{(}{)}{;}{a}{}{,} A number of investigators have developed methods for identifying such sample mix-ups \\citep{Westra2011, Schadt2012, Lynch2012, Ekstrom2012}, and a similar approach was applied by \\citet{Baggerly2008, Baggerly2009} in their forensic... \\bibliographystyle{genetics} \\renewcommand*{\\refname}{\\centerline{\\normalsize\\sffamily \\textbf{Literature Cited}}} \\bibliography{samplemixups}  @article{Baggerly2008, author = {Baggerly, Keith A. and Coombes, Kevin R.}, journal = {J. Clin. Oncol.}, pages = {1186--1187}, title = {Run batch effects potentially compromise...}, volume = {26}, year = {2008} }  References with \\LaTeX/ are via BibTeX, which is fabulous once you get used to it. Most software to track references will produce BibTeX files for you.\nThe formatting of citations and the reference listings, to match what the journal wants, can be painful. But I\u0026rsquo;ve figured out how to produce what **Genetics/ wants, and I send all of my papers there.\nThe first box is the sort of code that would appear in your \\LaTeX/ file: the bit at the top goes in the header (before {/begin{document**}). The bit in the middle shows how to cite papers: use {/citep} to get the whole thing in parentheses, and use {/citet} to get a reference like \u0026ldquo;\u0026hellip;applied by Baggerly and Coombes (2008, 2009)\u0026hellip;\u0026rdquo; The last bit in the first box produces the actual list of references.\nThe second box is the BibTeX format for a particular reference.\nWhen you use BibTeX, you tend to run pdflatex, then {\\tt bibtex}, and then pdflatex a couple of more times. }\nOrganizing analyses  Directory for the main analysis project  [] {~/Projects/Blah} Directory for a paper [] {~/Docs/Papers/Blah} Paper directory may have an analysis directory [] {~/Docs/Papers/Blah/Analysis} Symbolic links to .RData files [] ln -s {~/Projects/Blah/DerivedData/blah.RData .} Each part well organized and fully reproducible.  R Markdown reports documenting different aspects. Analysis with the paper may be re-done \u0026ldquo;properly.\u0026rdquo;  This is how I organize a paper related to a larger project.\nSome of the work in the main project may be re-done a bit differently (or cleaner) in the analysis with the paper.\nYou don\u0026rsquo;t want to re-do {\\nhilit all analyses for the paper, but it\u0026rsquo;d also be nice to have the data and code related to the paper be a bit more self-contained.\nAnd usually when you\u0026rsquo;re sitting down to write the paper, you have better ideas about how to re-do things properly, and so it might be a good idea to go ahead and re-do things.\nIdeally, you\u0026rsquo;d separate out each aspect of the analysis: data manipulation, data cleaning, and different parts of the analysis.\nHave an R Markdown document describing each aspect, with the actual manuscript and its figures and tables drawing from the results of those R Markdown documents. }\nMake every number reproducible. \u0026lt;\u0026lt;define_numbers, echo=FALSE\u0026gt;\u0026gt;= numbers \u0026lt;- c(\u0026quot;one\u0026quot;, \u0026quot;two\u0026quot;, \u0026quot;three\u0026quot;, \u0026quot;four\u0026quot;, \u0026quot;five\u0026quot;, \u0026quot;six\u0026quot;, \u0026quot;seven\u0026quot;, \u0026quot;eight\u0026quot;, \u0026quot;nine\u0026quot;, \u0026quot;ten\u0026quot;) cap \u0026lt;- function(vec) paste0(toupper(substr(vec, 1, 1)), substr(vec, 2, nchar(vec))) Numbers \u0026lt;- cap(numbers) n \u0026lt;- sample(1:10, 1) @ Then if I want to talk about a number, like \\Sexpr{n}, I can refer to it by name: \\Sexpr{numbers[n]}. And I can start a sentence with it. \\Sexpr{Numbers[n]} grasshoppers walked into a bar\\dots But be careful about singular vs. plural, and so write \\Sexpr{Numbers[n]} grasshopper\\Sexpr{ifelse(n\u0026gt;1, \u0026quot;s\u0026quot;, \u0026quot;\u0026quot;)} walked\\dots  Every statistic, figure and table in your manuscript should be fully reproducible. So when you\u0026rsquo;re citing statistics, use {/Sexpr{} liberally.\nThis should inhibit you from writing numbers as words, though the \\LaTeX/ code can get a bit ugly.\nThere\u0026rsquo;s a bit of fanciness here about capitalization and about ensuring that singular or plural nouns are correct. If {/Sexpr{}} produces a character string, it ends up as plain text in your document\nI\u0026rsquo;ll use a lot of myround() from my R/broman package, too.\nLong explanations or descriptions of figures can\u0026rsquo;t be fully reproducible, but the figures themselves and any statistics you mention should be. }\nKeep the figures separate # simple make file mypaper.pdf: mypaper.tex Figs/fig1.pdf Figs/fig2.pdf pdflatex mypaper Figs/fig1.pdf: R/fig1.R cd R;R CMD BATCH fig1.R fig1.Rout Figs/fig2.pdf: R/fig2.R cd R;R CMD BATCH fig2.R fig2.Rout  \\clearpage \\includegraphics{Figs/fig1.pdf} \\clearpage \\includegraphics{Figs/fig2.pdf}  While you {\\nhilit could include all code in your .Rnw file, I prefer to pull out the code for my figures as separate files, and then write a Makefile for the manuscript construction and include them with {/includegraphics}.\nThe advantage of this is the ability to reuse the figures in talks or whatever. Also, journals will generally want the figures as separate files. Finally, the code for my figures is often incredibly long and ugly, so it\u0026rsquo;s best to separate it out.\nIdeally, the code for a figure would be structured as a function and then a function call. Put a bit more effort into the code, so that you can reuse it later for a similar figure with different data. At the very least, you should write the repeated bits as functions.\nIf your function takes arguments that define the placement of things (padding for text and so forth), then the fine adjustments of the figure appearance would be easier. }\nVersion Control  Your manuscript is under version control, right? \\onslide{* Local or private repository for the whole thing  including reviewers\u0026rsquo; reports and my response PDF of submitted and final manuscript Snapshot of the final version as a public repository I don\u0026rsquo;t really want to show the whole history }   Git is as good for tracking manuscripts and data analyses as it is for tracking code. Use it!\nBut I don\u0026rsquo;t want to make {\\nhilit everything public, and I want to include private stuff in my repository.\nI\u0026rsquo;ve been using just a local repository, but I\u0026rsquo;m moving towards having a private repository hosted on BitBucket.\nI\u0026rsquo;ll put a snapshot of the final version, and maybe a few final changes, on GitHub. }\nWord  With papers led by a collaborator, I\u0026rsquo;m usually stuck with Word. But my analyses and figures are fully reproducible. Create an R Markdown document with the detailed results.  Often, you\u0026rsquo;ll be stuck with Word. And you can\u0026rsquo;t reproducibly insert numbers into Word.\nSo have a separate R Markdown report with the detailed results, including every statistic that will get inserted into Word.\nAnd take control of the figures and ensure that they are reproducible (and respectable).\nTeach your collaborators to at least have their figures be reproducible?\nSummary  \\LaTeX/ is brilliant for fine control and for equations Floating figures and tables can be a pain You use KnitR with \\LaTeX/ much the same way as you\u0026rsquo;d used it with Markdown. Ensure that every statistic, figure, and table in your paper are fully reproducible. Use xtable to make tables. Separate out the code for the figures. Use version control!  Summaries are helpful.\nPresentations and posters It\u0026rsquo;s arguably less critical that presentation slides or a poster be reproducible. Nevertheless, there can be great personal advantage to the automated generation of figures and such in slides or a poster: if the primary data should change, or if some analysis mistake is discovered, it will be easier to revise the presentation.\nMy primary goal is to get you to ditch Powerpoint/Keynote in favor of reproducible alternatives. I will primarily focus on the Beamer package for LaTeX, for both slides and posters. But I will also touch upon the use of slidify to make Markdown-based slides for a talk.\nPowerpoint/Keynote  [+] Standard [+] Easy to share slides [+] WYSIWYG (mostly) [+] Fancy animations   [-] Font problems [-] Lots of copy-paste [-] Hard to get equations [-] Not reproducible  Powerpoint and Keynote do have their advantages, the principal one being that everyone is using these tools, which makes it easy to share slides with friends.\nBut we\u0026rsquo;ve all seen terrible font problems in important presentations, mostly due to incompatibilities between Windows and Mac versions of Powerpoint: fonts should be, but aren\u0026rsquo;t, embedded in the presentation.\nAnd insertion of figures requires tedious copy-paste, usually followed by manual resizing and adjustment of figure placement. And if the figures are revised (because the data changed or some mistake was found in the analysis), we\u0026rsquo;ll have to repeat all of that.\n\\LaTeX/ Beamer package \\figh{Figs/Copenhagen-default-default-01.png}{0.75}\nUntil recently, I\u0026rsquo;d been making \\LaTeX/ slides using the {\\tt article document class, just revising the page size and make the fonts big.\nThe Beamer package for \\LaTeX/ is easier, but I was turned off by the standard slides that people were producing with Beamer, such as the one shown: far too much junk on the screen, and on every single slide.\nYou can get rid of all of that. All of the slides I\u0026rsquo;m making for this course are produced with Beamer.\nThere\u0026rsquo;s good facility for adding simple animations (progressively showing or hiding different elements on the slide).\nBut you {\\nhilit are} writing \\LaTeX, so the coding can be a bit verbose. }\nGet rid of the junk \\usetheme{default} \\beamertemplatenavigationsymbolsempty  The first thing to do is to get rid of all of the junk.\nIt\u0026rsquo;s surprisingly easy: default theme and remove navigation symbols.\nChange colors \\definecolor{foreground}{RGB}{255,255,255} \\definecolor{background}{RGB}{24,24,24} \\definecolor{title}{RGB}{107,174,214} \\definecolor{subtitle}{RGB}{102,255,204} \\definecolor{hilit}{RGB}{102,255,204} \\definecolor{lolit}{RGB}{155,155,155} \\setbeamercolor{titlelike}{fg=title} \\setbeamercolor{subtitle}{fg=subtitle} \\setbeamercolor{institute}{fg=lolit} \\setbeamercolor{normal text}{fg=foreground,bg=background} \\setbeamercolor{item}{fg=foreground} % color of bullets \\setbeamercolor{subitem}{fg=lolit} \\setbeamercolor{itemize/enumerate subbody}{fg=lolit} \\setbeamertemplate{itemize subitem}{{-}} \\setbeamerfont{itemize/enumerate subbody}{size=\\footnotesize} \\setbeamerfont{itemize/enumerate subitem}{size=\\footnotesize} \\newcommand{}{\\color{hilit}} \\newcommand{\\lolit}{\\color{lolit}}  I prefer light text on a dark background.\nThe tricky part is that Beamer has special names for everything.\nIt would be best if I created a new theme, but I don\u0026rsquo;t want to take the time to figure that out.\nAlso, slide numbers and fonts % slide number \\setbeamertemplate{footline}{% \\raisebox{5pt}{\\makebox[\\paperwidth]{\\makebox[20pt]{\\lolit \\scriptsize\\insertframenumber}}}\\hspace*{5pt}} % font \\usepackage{fontspec} % http://www.gust.org.pl/projects/e-foundry/tex-gyre/ % ... heros/qhv2.004otf.zip \\setsansfont [ ExternalLocation = ../fonts/ , UprightFont = *-regular , BoldFont = *-bold , ItalicFont = *-italic , BoldItalicFont = *-bolditalic ]{texgyreheros} % Palatino for notes \\setbeamerfont{note page}{family*=pplx,size=\\footnotesize}  I also want the slide number in the bottom-right, and I want a different font: something a bit more blocky, which I think is easier to read on the screen.\nTitle slide \\title{Put title here} \\subtitle{And maybe a subtitle} \\author{Author name} \\institute{Biostatistics \\\u0026amp; Medical Informatics, UW{-}Madison} \\date`\\scriptsize biostat.wisc.edu/{~`kbroman} \\begin{document} { \\setbeamertemplate{footline}{} % no slide number here \\frame{ \\titlepage Summary of the talk, as a note. } }  The title slide is created with {/titlepage, having first defined {/title}, {/author}, etc.\nThe extra curly braces are to get the \u0026ldquo;no slide number\u0026rdquo; to apply just to the title slide. You can put notes on slides and then make a version that has the slide above the notes. See what I do with the slides for this course, or ask me for help. }\n% this is to get within the lstlisting environment % See http://tex.stackexchange.com/questions/73366/ \\newsavebox{\\codeboxone} \\begin{lrbox}{\\codeboxone}\n## Title of slide * Bullet 1 * Bullet 2 * Bullet 3 Put a note here  \\end{lrbox}\nTypical slide \\usebox{\\codeboxone}\n% this is to get within the lstlisting environment % See http://tex.stackexchange.com/questions/73366/ \\newsavebox{\\codeboxtwo} \\begin{lrbox}{\\codeboxtwo}\n## Title of slide \\begin{itemize} * sep8pt * Bullet 1 * Bullet 2 * Bullet 3 \\end{itemize} Put a note here  \\end{lrbox}\nTypical slide \\addtocounter{framenumber}{-1}\n\\usebox{\\codeboxtwo}\nA typical slide is set between {/begin{frame\\{title}} and {/end{frame}}.\nYou get bullet points with the itemize environment. I\u0026rsquo;ll mess around a bit with {/vspace} and {/itemsep}. And I\u0026rsquo;ll create shortcuts with {/newcommand} for these. }\n% this is to get within the lstlisting environment % See http://tex.stackexchange.com/questions/73366/ \\newsavebox{\\codeboxthree} \\begin{lrbox}{\\codeboxthree}\n## Title of slide \\figh{Figs/a_figure.png}{0.75} Put a note here  \\end{lrbox}\nSlide with a figure \\usebox{\\codeboxthree}\n% this is to get within the lstlisting environment % See http://tex.stackexchange.com/questions/73366/ \\newsavebox{\\codeboxfour} \\begin{lrbox}{\\codeboxfour}\n## Title of slide \\centerline{\\includegraphics[height=0.75\\textheight]{% Figs/a_figure.png}} Put a note here  \\end{lrbox}\nSlide with a figure \\addtocounter{framenumber}{-1}\n\\usebox{\\codeboxfour}\nI\u0026rsquo;d typically generate figures externally and include them with {/includegraphics. }\nFigures with KnitR \u0026lt;\u0026lt;knitr_options, echo=FALSE\u0026gt;\u0026gt;= opts_chunk$set(echo=FALSE, fig.height=7, fig.width=10) change_colors \u0026lt;- function(bg=rgb(24,24,24, maxColorValue=255), fg=\u0026quot;white\u0026quot;) par(bg=bg, fg=fg, col=fg, col.axis=fg, col.lab=fg, col.main=fg, col.sub=fg) @ \u0026lt;\u0026lt;pdf_figure\u0026gt;\u0026gt;= change_colors() par(las=1) n \u0026lt;- 100 x \u0026lt;- rnorm(n) y \u0026lt;- 2*x + rnorm(n) plot(x, y, pch=16, col=\u0026quot;slateblue\u0026quot;) @  You could use a knitr code chunk, in the same way we discussed for manuscripts, in the last lecture.\nFigures with KnitR % \u0026lt;\u0026lt; \u0026gt;\u0026gt;= all on one line! \u0026lt;\u0026lt;png_figure, dev=\u0026quot;png\u0026quot;, fig.align=\u0026quot;center\u0026quot;, dev.args=list(pointsize=30), fig.height=15, fig.width=15, out.height=\u0026quot;0.75\\\\textheight\u0026quot;, out.width=\u0026quot;0.75\\\\textheight\u0026quot;\u0026gt;\u0026gt;= change_colors(bg=rgb(32,32,32,maxColorValue=255)) par(las=1) n \u0026lt;- 251 x \u0026lt;- y \u0026lt;- seq(-pi, pi, len=n) z \u0026lt;- matrix(ncol=n, nrow=n) for(i in seq(along=x)) for(j in seq(along=y)) z[i,j] \u0026lt;- sin(x[i]) + cos(y[j]) image(x,y,z) @  To create a PNG figure (which can give much smaller file sizes for things like an image or a dense scatterplot), use the chunk option dev=\u0026quot;png\u0026quot;.\nFor some reason, RGB colors don\u0026rsquo;t match well between PNG files and the PDF, so I have to muck about to get the background of the PNG to match the background on the slides.\nIt\u0026rsquo;s also a bit of work to get the resolution and text size just right.\nI split the initial line defining the code chunk across multiple lines here, so it could all be seen, but in practice the whole \u0026lt;\u0026lt; \u0026gt;\u0026gt;= bit needs to be on one line.\nSlides with notes \\documentclass[12pt,t]{beamer} \\setbeameroption{hide notes} \\setbeamertemplate{note page}[plain]  \\documentclass[12pt,t,handout]{beamer} \\setbeameroption{show notes} \\setbeamertemplate{note page}[plain] \\def\\notescolors{1}  \\ifx\\notescolors\\undefined % slides \\definecolor{foreground}{RGB}{255,255,255} \\definecolor{background}{RGB}{24,24,24} \\else % notes \\definecolor{background}{RGB}{255,255,255} \\definecolor{foreground}{RGB}{24,24,24} \\fi  To create a version of your slides with notes, include {/note{ } on every slide.\nI then include the code in the top box in the slide version, the middle box in the note version, and the stuff at the bottom in both. The bit at the bottom selects colors to be light text on a dark background in the slides and dark text on a light background in the notes version.\nI wrote a ruby script to create a notes version from the slide version (replace the code in the top box with the code in the middle box).\nI then use pdfnup (part of PDFjam) to make 2-up pages (slides at the top, notes at the bottom). The only problem with pdfnup is that it strips off all of the hyperlinks. }\n% this is to get within the lstlisting environment % See http://tex.stackexchange.com/questions/73366/ \\newsavebox{\\codeboxfive} \\begin{lrbox}{\\codeboxfive}\n## Bullets entering one at a time * Bullet 1 \\onslide\u0026lt;2-\u0026gt;{* Bullet 2} \\onslide\u0026lt;3-\u0026gt;{* Bullet 3} \\onslide\u0026lt;4-\u0026gt;{* Bullet 4} Do this sparingly.  \\end{lrbox}\nSimple animations \\usebox{\\codeboxfive}\nIt\u0026rsquo;s easy to add a bit of animation, such as with bullets appearing one by one. Use {/onslide or {/only}.\nHere, the bullets will appear one at a time.\nBeamer just expands the PDF, with this slide becoming multiple pages. }\n% this is to get within the lstlisting environment % See http://tex.stackexchange.com/questions/73366/ \\newsavebox{\\codeboxsix} \\begin{lrbox}{\\codeboxsix}\n## Bullets entering one at a time * \\only\u0026lt;1\u0026gt;{\\color{foreground} Bullet 1} * \\only\u0026lt;2\u0026gt;{\\color{foreground} Bullet 2} * \\only\u0026lt;3\u0026gt;{\\color{foreground} Bullet 3} * \\only\u0026lt;4\u0026gt;{\\color{foreground} Bullet 4} Do this sparingly.  \\end{lrbox}\nSimple animations \\usebox{\\codeboxsix}\nIn this version, the bullets will go from dim to bright, one at a time.\nSlidify and R Markdown \\figh{Figs/slidify.png}{0.75}\nSlidify and R Markdown \\addtocounter{framenumber}{-1}\n## Slide title - Bullet 1 - Bullet 2 - Bullet 3 - Bullet 4 --- ## A figure \u0026quot;`{r a_figure, echo=FALSE, fig.align=\u0026quot;center\u0026quot;} par(las=1) n \u0026lt;- 100 x \u0026lt;- rnorm(n) y \u0026lt;- 2*x + rnorm(n) plot(x, y, pch=16, col=\u0026quot;slateblue\u0026quot;) \u0026quot;`  Slidify makes it super easy to create html-based slides with R Markdown. Three dashes separate slides, and two pound symbols (section heading) indicate the slide title.\nThe chief advantage is that you can make nice slides with very little markup. And there are a ton of options, like having embedded quizzes.\nThe disadvantage is that it\u0026rsquo;s a bit harder to get fine control of the layout. And I\u0026rsquo;ve found it a bit risky to use html-based slides for a presentation. PDF is more trustworthy.\nIn principle, you can use pandoc to convert the slides to PDF, but I\u0026rsquo;ve not been happy with the result. You could also print them from the browser, but I only got a good result with Safari. (Firefox included some links on the first page, and Chrome produced total garbage.)\nPersonally, I\u0026rsquo;m going to stick with Beamer for important presentations, but slidify seems good for informal presentations (e.g., to collaborators) or for a course.\nUsing slidify library(devtools) install_github(\u0026quot;slidify\u0026quot;, \u0026quot;ramnathv\u0026quot;) install_github(\u0026quot;slidifyLibraries\u0026quot;, \u0026quot;ramnathv\u0026quot;) library(slidify) setwd(\u0026quot;~/Docs/Talks/\u0026quot;) author(\u0026quot;slidify_example\u0026quot;) # edit ~/Docs/Talks/slidify_example/index.Rmd slidify(\u0026quot;index.Rmd\u0026quot;) browseURL(\u0026quot;index.html\u0026quot;)  To use slidify, download the slidify and slidifyLibraries packages from GitHub, and use author() to create the file to edit, and then slidify() to compile the result.\nNote that author(\u0026quot;slidify_example\u0026quot;) changes the working directory.\nYAML header --- title : Slidify example subtitle : Tools for reproducible research author : Karl Broman job : Biostatistics \u0026amp; Medical Informatics, UW-Madison framework : io2012 # {io2012, html5slides, shower, ...} highlighter : highlight.js # {highlight.js, prettify, highlight} hitheme : tomorrow # widgets : [mathjax] # {mathjax, quiz, bootstrap} mode : standalone # {selfcontained, standalone, draft} ---  There\u0026rsquo;s a bit at the top of the file to define the slide title and layout.\nframework defines the slide style. highlighter is the method to give syntax highlighting. With mode \u0026ldquo;standalone,\u0026rdquo; some otherwise-external files are embedded in the html file.\nYAML is a \u0026ldquo;human-readable data serialization format.\u0026rdquo; (Serialization means it can be easily transmitted over a network.) It\u0026rsquo;s a well-defined way of describing potentially complex data objects.\nChange the title slide colors \u0026lt;style\u0026gt; .title-slide { background-color: #EEE; } .title-slide hgroup \u0026gt; h1, .title-slide hgroup \u0026gt; h2 { color: #005; } \u0026lt;/style\u0026gt;  The default colors for the title slide with framework io2012 are really terrible. Include a bit of CSS code in your .Rmd file to fix that.\nThere are a bunch of named colors in html, or you can use codes like \u0026ldquo;\\#005;\u0026rdquo; or \u0026ldquo;\\#000055;\u0026rdquo; for RGB (R=00, G=00, B=55).\nBeamer-based posters \\figh{Figs/mathbio2011.png}{0.75}\n\\tt \\lolit \\scriptsize github.com/kbroman/Poster_SampleMixups\nBeamer-based posters \\addtocounter{framenumber}{-1}\n\\figh{Figs/enar2014.png}{0.75}\n\\tt \\lolit \\scriptsize github.com/kbroman/Poster_ENAR2014\nBeamer-based posters \\addtocounter{framenumber}{-1}\n\\documentclass[final,plain]{beamer} \\usepackage[size=custom,width=152.4,height=91.44,scale=1.2]{% beamerposter} \\newlength{\\sepwid} \\newlength{\\onecolwid} \\newlength{\\halfcolwid} \\newlength{\\twocolwid} \\newlength{\\threecolwid} \\setlength{\\sepwid}{0.0192\\paperwidth} \\setlength{\\onecolwid}{0.176\\paperwidth} \\setlength{\\halfcolwid}{0.0784\\paperwidth} \\setlength{\\twocolwid}{0.3712\\paperwidth} \\setlength{\\threecolwid}{0.5664\\paperwidth} \\setlength{\\topmargin}{-0.5in} \\usetheme{confposter}  Beamer can be used to make posters, too. It\u0026rsquo;s a lot of work, but with my latest poster, I\u0026rsquo;m finally convinced of the value of these large-format posters.\nAt UW-Madison, the Digital Media Center will print posters for you (\\$5 per square foot for paper; \\$7 per square foot for cloth). The cloth ones eliminate the need to carry a tube, but they need to be packed gently and probably still require a bit of ironing on the other end.\nwidth and height are in centimeters. Use scale to increase the font sizes overall. Then carefully calculate the {\\tt sepwid, onecolwid, etc., so that everything fits just right. }\n% this is to get within the lstlisting environment % See http://tex.stackexchange.com/questions/73366/ \\newsavebox{\\codeboxseven} \\begin{lrbox}{\\codeboxseven}\n\\title{Data visualizations should be more interactive} \\author{Karl W Broman} \\institute{University of Wisconsin--Madison} ## columns [t] \\begin{column}{\\sepwid}\\end{column} % empty spacer column \\begin{column}{\\onecolwid} \\begin{exampleblock}{\\Large Introduction}{ \\begin{itemize} * sep18pt * Bullet 1 * Bullet 2 \\end{itemize} } \\colonevsep % between blocks \\begin{block}{Barriers}{ } \\end{column} \\end{columns}  \\end{lrbox}\nBasic code for a poster \\usebox{\\codeboxseven}\nThe whole thing is within a single frame environment, and then with columns and column.\nThe blocks within a column are within exampleblock or block environments.\nBetween-block spacing \\newcommand{\\colonevsep}{} \\newcommand{\\coltwovsep}{} \\newcommand{\\colthreevsep}{} \\newcommand{\\colfourvsep}{} \\newcommand{\\colfivevsep}{}  I define different between-block spacings for each column, to get them all to have the same length.\nSummary  Use LaTeX/Beamer or Slidify to create reproducible slides. Use LaTeX/Beamer to create reproducible posters. Include KnitR code chunks to create figures directly. Or keep the code for figures separate.  To make reproducible slides/posters, you need to dump PowerPoint.\nWith each of these approaches, you can use KnitR code chunks. But I still tend to produce the figures separately and include them with {/includegraphics. }\n"
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/git/",
	"title": "Git and GitHub",
	"tags": [],
	"description": "",
	"content": " Introduction This lesson aims to introduce you to using Git and GitHub for reproducible research. The first draft of these materials was a little intimidating and unclear about how to use these tools in practice. Git and GitHub are valuable tools; I want to show you how to use the tools, and you can decide whether you want to use them in your work.\nVersion control software is not strictly necessary for reproducible research. In the short-term it can be a lot of work to learn and to use. But, it has some pretty awesome long-term benefits. I want to ease the short-term pain of learning Git/GitHub and illustrate the long-term gain of adding Git/GitHub to your toolbox.\nWhy use Git? For the moment, think of Git like a camera. You can use Git to take snapshots of your code over time. This is valuable because it lets you:\n see how a project has evolved, restore your code to an old snapshot (this is super useful if you break something, it encourages you to experiment more freely), and see what changed between two snapshots.  At the same time, Git is like Adobe Photoshop. You can use Git to combine snapshots together. This is great for collaborating with other people.\nIt\u0026rsquo;s worth mentioning that Git works best with plain text files (code, markdown, csv, \u0026hellip;). It\u0026rsquo;s less helpful when working with binary files like Microsoft Word or Excel documents.\nWhy use GitHub? GitHub is a web application that facilitates collaboration.\n GitHub is a website that hosts Git repositories.\n It provides a nice graphical user interface (GUI) for exploring Git repositories.\n Source code on GitHub is real open source: anyone can study it and grab it.\n GitHub provides issue tracking.\n GitHub makes it easy to suggest changes to anyone (pull requests).\n  Open source\n Open source means everyone can see my stupid mistakes.\n Version control means everyone can see every stupid mistake I\u0026rsquo;ve ever made.\n  If you store your code on GitHub, everyone can see everything. They can even see everything that ever was.\nI think this openness is a good thing. You may be shy about your code, but probably no one is looking. And if they are looking, that is actually a good thing.\nInstallation A common hurdle when working with new software is installation! To help you clear this hurdle, I\u0026rsquo;ve copied installation instructions from The Carpentries. If these instructions don\u0026rsquo;t work for you, please let me know in the comments at the bottom of this page.\nInstall Git on Windows Video Tutorial\n Download the Git for Windows installer. Run the installer and follow the steps below:\n Click on \u0026ldquo;Next\u0026rdquo; four times (two times if you\u0026rsquo;ve previously installed Git). You don\u0026rsquo;t need to change anything in the Information, location, components, and start menu screens. Select \u0026ldquo;Use the nano editor by default\u0026rdquo; and click on \u0026ldquo;Next\u0026rdquo;. Keep \u0026ldquo;Use Git from the Windows Command Prompt\u0026rdquo; selected and click on \u0026ldquo;Next\u0026rdquo;. If you forgot to do this programs that you need for the workshop will not work properly. If this happens rerun the installer and select the appropriate option. Click on \u0026ldquo;Next\u0026rdquo;. Keep \u0026ldquo;Checkout Windows-style, commit Unix-style line endings\u0026rdquo; selected and click on \u0026ldquo;Next\u0026rdquo;. Select \u0026ldquo;Use Windows\u0026rsquo; default console window\u0026rdquo; and click on \u0026ldquo;Next\u0026rdquo;. Click on \u0026ldquo;Install\u0026rdquo;. Click on \u0026ldquo;Finish\u0026rdquo;.  If your \u0026ldquo;HOME\u0026rdquo; environment variable is not set (or you don\u0026rsquo;t know what this is):\n Open command prompt (Open Start Menu then type cmd and press Enter) Type the following line into the command prompt window exactly as shown:\nsetx HOME \u0026quot;%USERPROFILE%\u0026quot;  Press Enter, you should see SUCCESS: Specified value was saved.\n Quit command prompt by typing exit then pressing Enter.\n   This will provide you with both Git and Git Bash (a command line interface).\nInstall Git on macOS Video Tutorial\nFor OS X 10.9 and higher, install Git for Mac by downloading and running the most recent \u0026ldquo;mavericks\u0026rdquo; installer from this list. Because this installer is not signed by the developer, you may have to right click (control click) on the .pkg file, click Open, and click Open on the pop up window. After installing Git, there will not be anything in your /Applications folder, as Git is a command line program. For older versions of OS X (10.5-10.8) use the most recent available installer labelled \u0026ldquo;snow-leopard\u0026rdquo; available here.\nInstall Git on Linux If Git is not already available on your machine you can try to install it via your distro\u0026rsquo;s package manager. For Debian/Ubuntu run sudo apt-get install git and for Fedora run sudo dnf install git.\nThe command line This lesson focuses on using Git at the command line. (Ideally, we would have gone through the Unix lesson and you\u0026rsquo;d be feeling good about using command line tools. In reality, we haven\u0026rsquo;t done that. I\u0026rsquo;ll try make things accessible and be responsive to any issues you might run into.)\n These materials focus on using Git at the command line (rather than using a GUI) to help you understand how Git works.\n You might find that a GUI works better for you than the command line.\n I\u0026rsquo;m focusing on the command line because it\u0026rsquo;s the same for everyone, and it will help you understand what GUIs are doing under the hood.\n  Back in the Unix lesson, there should be a section discussing shells.\n In computing, a shell is a user interface for access to an operating system\u0026rsquo;s services. In general, operating system shells use either a command-line interface (CLI) or graphical user interface (GUI), depending on a computer\u0026rsquo;s role and particular operation. It is named a shell because it is the outermost layer around the operating system kernel.\n We are going to use Bash as our CLI shell. Bash is a commonly-used shell that gives you the power to do simple tasks more quickly.\n On Windows, I suggest using Git Bash, which was installed when you installed Git.\n The default shell in all versions of macOS is Bash, so no need to install anything. You access Bash from the Terminal (found in /Applications/Utilities/).\n On Unix-like operating systems, the default shell is usually Bash, but if your machine is set up differently you can run it by opening a terminal and typing bash. There is no need to install anything.\n  Git global configuration When you first install Git, it\u0026rsquo;s helpful to configure some global variables. For instance, tell Git your name and email address for it to use when you take snapshots of your code:\n$ git config --global user.name \u0026quot;Jane Doe\u0026quot; $ git config --global user.email \u0026quot;jdoe@hbs.edu\u0026quot;  Tell it to use nice colors when printing to the command line so it\u0026rsquo;s easier to read:\n$ git config --global color.ui true  All of this information gets written to a file in your home directory ~/.gitconfig.\nSign up for GitHub If you don\u0026rsquo;t already have a GitHub account\u0026hellip;\nYou will need an account at github.com for parts of this lesson. The free account for individuals should be fine for most users (you can always upgrade later if you want). It\u0026rsquo;s worth considering what personal information you\u0026rsquo;d like to reveal. For example, you may want to review these instructions for keeping your email address private.\nGit: Linear Workflow The plan for this section is to show you how to take snapshots of your project in a linear fashion. Imagine your project is a mountain. As you make your way up the mountain it makes sense to place anchors along the way, that way if you ever lose your footing you\u0026rsquo;ll only fall to the last anchor not the bottom of the mountain!\nSet up your camera Before you can take pictures of your code, you\u0026rsquo;ll need to initialize your repository using git init. Let\u0026rsquo;s create a new project folder on your desktop and initialize it to be a git repository.\n$ mkdir ~/Desktop/my-project $ cd ~/Desktop/my-project $ git init Initialized empty Git repository in /Users/amarder/Desktop/my-project/.git/  Note that git init creates a .git/ subdirectory in your current working directory. This is where Git saves all your snapshots.\nTake a snapshot Taking a snapshot is a two-step process. We use git add to stage changes and git commit save the snapshot in the hidden .git/ subdirectory.\nLet\u0026rsquo;s do an example. In your project folder create a README.md file.\n# My Example Project Let's see if Andrew has any cool tricks to show us...  After you\u0026rsquo;ve saved that file use git add and git commit to take a snapshot.\n$ git add README.md $ git commit -m \u0026quot;Add README\u0026quot; [master (root-commit) d950cb5] Add README 1 file changed, 3 insertions(+) create mode 100644 README.md  Use git add to tell Git that you want to start keeping track of this file. This is called \u0026ldquo;staging\u0026rdquo;, or you say the file is \u0026ldquo;staged\u0026rdquo;. Use git commit to add the file to the history of the project. The -m argument allows one to enter a message. Without -m, git will spawn a text editor. Use a meaningful message. Message can have multiple lines, but make 1st line like the subject of an email.1\nTake any snapshot Let\u0026rsquo;s create a Bash script to list all the files in your .git/ subdirectory, call it git_list.sh:\nfind .git/ -type f  After you save your new script you can print the results into a new file:\n$ ls README.md\tgit_list.sh $ bash git_list.sh \u0026gt; files.txt $ ls README.md\tfiles.txt\tgit_list.sh  You can use git add -A to stage all changed files in this directory and then use git commit to save your second snapshot.\n$ git add -A \u0026amp;\u0026amp; git commit -m \u0026quot;Wrote a bash script\u0026quot; [master 7b620ba] Wrote a bash script 2 files changed, 23 insertions(+) create mode 100644 files.txt create mode 100644 git_list.sh  What files to include? As you work with Git, you\u0026rsquo;ll start to notice files that haven\u0026rsquo;t been on your radar before. For instance, when you\u0026rsquo;re working interactively in R, R will create an .Rhistory file to record all the commands you type at the console. I suggest that you omit files like these from your snapshots. That way your photos will be easier to read, there won\u0026rsquo;t be a bunch of superfluous files. Git offers a mechanism to handle this. Create a .gitignore file in your project directory and Git will ignore any file that matches a pattern in that file, here\u0026rsquo;s an example file:\n.Rhistory *.Rout  You can even set up a global ignore file using the git config command.\n$ git config --global core.excludesfile ~/.gitignore_global  The general rule for what files to include is to commit the source files, not the derived files (R code not images).\nLike any good rule, this one is often broken. For a manuscript, I might include the pdf at major milestones (at submission, after revision, and upon acceptance), so that I don\u0026rsquo;t have to work as hard to reconstruct them.\nWarning: Ignoring critical files is a common mistake.\nExtras So, you\u0026rsquo;re fully equipped to snapshot your project in a linear fashion. Here\u0026rsquo;s the workflow:\n Edit files Snapshot using git add -A \u0026amp;\u0026amp; git commit -m \u0026quot;\u0026lt;insert-message\u0026gt;\u0026quot; Repeat  There are a whole bunch of additional tools I want to introduce but are not mission-critical.\n When you\u0026rsquo;ve edited files but haven\u0026rsquo;t taken a snapshot yet, git status will show you what files have changed.\n git diff will give you even more information showing what lines have been added/removed to each file.\n To list all your snapshots use git log.\n  For files that are being tracked by git:\n Use git rm instead of just rm. git rm removes the file and stages that change.\n Use git mv instead of just mv. git mv moves the file and stages that change.\n  A general best practice: Use frequent, small commits.\nGit: Nonlinear Workflow So, your project is in good shape. You\u0026rsquo;ve been taking linear snapshots and everything\u0026rsquo;s working fine. You\u0026rsquo;re thinking about adding this new feature. It\u0026rsquo;s going to be very complicated, require a bunch of commits, and you\u0026rsquo;re not sure it\u0026rsquo;s going to work out. This is where branches can be helpful. So, let\u0026rsquo;s see how to branch and merge!\nUse branches to test out new features without breaking code that works. Use the git branch command to create, list, or delete branches. Use git checkout to switch between branches. Below we (1) create a branch, (2) list all branches, and (3) switch to the new branch.\n$ git branch my-experiment $ git branch * master my-experiment $ git checkout my-experiment Switched to branch 'my-experiment'  Once you\u0026rsquo;re on the new branch feel free to experiment freely. Nothing you do here will affect the master branch until you merge it back. Edit some files and create a new commit on the my-experiment branch.\nWhen you\u0026rsquo;re happy with the work, merge it back into your master branch. You do this by (1) switching to the master branch and (2) merging the my-experiment branch into your current branch (master).\n$ git checkout master Switched to branch 'master' $ git merge my-experiment Updating 7b620ba..ead9d4c Fast-forward README.md | 2 ++ 1 file changed, 2 insertions(+)  If you\u0026rsquo;re working alone you probably won\u0026rsquo;t see much value in using branches. When you\u0026rsquo;re working with others they become mission critical\u0026hellip;\nGitHub: Collaboration I have posted this lesson on GitHub because I want to collaborate with you to make these materials better. In this section, I will walk through the tools you\u0026rsquo;ll need so we can work together!\nThere are two different models for contributing to projects on GitHub:\n In the fork and pull model, anyone can fork an existing repository and push changes to their personal fork without needing access to the source repository. The changes can be pulled into the source repository by the project maintainer. This model is popular with open source projects as it reduces the amount of friction for new contributors and allows people to work independently without upfront coordination.\n In the shared repository model, collaborators are granted push access to a single shared repository and topic branches are created when changes need to be made. Pull requests are useful in this model as they initiate code review and general discussion about a set of changes before the changes are merged into the main development branch. This model is more prevalent with small teams and organizations collaborating on private projects.\n  Since I want this lesson to be open and allow anyone to suggest changes, I will be focusing on the fork and pull model.\nSuggest a change One of the best features of GitHub is the ease with which you can suggest changes to others\u0026rsquo; code, either via an Issue, or best of all via a Pull Request.\n Issues = \u0026ldquo;Can you change this?\u0026rdquo; Pull Requests = \u0026ldquo;I changed this. Can you incorporate this change?\u0026rdquo;  Here\u0026rsquo;s the process for submitting a pull request.\n Go to the repository\nUsing your web browser, find the repository on GitHub you want to contribute to. Feel free to use this repository for practice.\n Fork the repository\nThis is done by clicking the \u0026ldquo;Fork\u0026rdquo; button. This copies the repository to your GitHub account.\n Clone the repository\nOn the command line use the git clone command (be sure to insert your GitHub username and the correct repository name).\n$ git clone https://github.com/YOUR-USERNAME/YOUR-REPOSITORY  This copies your repository from GitHub to your local machine.\n Create a new branch\nTo create a new branch and switch to it with one command use git checkout -b.\n$ cd YOUR-REPOSITORY $ git checkout -b BRANCH-NAME  Working on a new branch leaves the master branch unchanged. This will make it easier for you to make additional contributions in the future.\n Create new commit(s)\nMake your desired edits and commit them.\n$ git add -A \u0026amp;\u0026amp; git commit -m \u0026quot;COMMIT-MESSAGE\u0026quot;  Push your changes to GitHub\nUse git push to push your new branch up to GitHub.\n$ git push origin BRANCH-NAME  This updates your copy of the repository on GitHub.\n Create pull request\nGo to your GitHub repository https://github.com/YOUR-USERNAME/YOUR-REPOSITORY and click \u0026ldquo;New pull request\u0026rdquo;. The final steps on GitHub are (hopefully) pretty clear; let me know in the comments if you run into any difficulties. After the pull request has been created, it will be easy for the project maintainer to merge your modifications into the master branch of their repository.\n  Admittedly, this is a lot of work for you. The advantage of this approach is it\u0026rsquo;s very little work for the project maintainer, so they\u0026rsquo;ll be more likely to incorporate your work. Over time, you\u0026rsquo;ll get used to the process and it will be less work for you.\nSuggest another change I want you to be a frequent collaborator on these materials. Ideally, the master branch will be changing often. The instructions in the previous section are great for your first pull request, but I want you to be set up to make many more pull requests in the future. To do this, we\u0026rsquo;ll use the git remote command to tell Git there\u0026rsquo;s another repository you want to track.\ngit remote add rcs https://github.com/hbs-rcs/reproducible-research.git  This will add information to the .git/config file.2\n[remote \u0026quot;origin\u0026quot;] url = https://github.com/your_username/reproducible-research.git fetch = +refs/heads/*:refs/remotes/origin/* [remote \u0026quot;rcs\u0026quot;] url = https://github.com/hbs-rcs/reproducible-research.git fetch = +refs/heads/*:refs/remotes/rcs/*  Once you\u0026rsquo;ve set your remote the process for submitting pull requests looks like this:\n Update your master branch to match mine:\ngit checkout master git pull rcs master  Create a new branch based off of master to work on:\ngit branch new-feature git checkout new-feature  Make your changes and commit them (git add, git commit).\n Push your branch to your GitHub repository:\ngit push origin new-feature  Create a pull request.\n  Merge conflicts At some point in the future, you may be faced with a merge conflict. Here\u0026rsquo;s how they happen:\n You\u0026rsquo;re working on the project on your computer. I\u0026rsquo;m also working on the project and I update the master branch. You go to submit a pull request but your changes can\u0026rsquo;t be merged into my master branch because we made conflicting edits.   Here\u0026rsquo;s how to handle merge conflicts. Update your master branch to match mine.\ngit checkout master git pull rcs master  Merge master into the branch you\u0026rsquo;ve been working on.\ngit checkout new-feature git merge master  You\u0026rsquo;ll see a message letting you know there\u0026rsquo;s a merge conflict.\nAuto-merging README.md CONFLICT (content): Merge conflict in README.md Automatic merge failed; fix conflicts and then commit the result.  Inside the file you\u0026rsquo;ll see:\n\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HEAD A line in my file. ======= A line in my RCS's file. \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; 031389f2cd2acde08e32f0beb084b2f7c3257fff  Edit the file as you see fit (keep your line, keep my line, come up with an entirely new line). Then you\u0026rsquo;re ready for the typical process (git add, git commit, git push origin new-feature).\nLab Objective  I want your help! Let\u0026rsquo;s improve this documentation together.  Process We\u0026rsquo;ll be following the GitHub Flow approach to collaboration.\nDecide where to contribute (step 1 of 5)  Browse through the documentation and determine where you\u0026rsquo;d like to help.\n I need help with: typos, writing, formatting, and content.\n Pick something small you can complete in a few minutes.\n  Installation (step 2 of 5) Go to my GitHub repository and follow the Installation instructions.\n(Notice that those instructions involve forking my repository and using git clone to set up a local copy on your computer)\nEdit the documentation locally (step 3 of 5)  Use git branch to create a branch to experiment on.\n Use git checkout to switch to your new branch.\n Make your edits.\n Use git add and git commit to commit those edits to your branch.\n  Create a pull request (step 4 of 5)  Use git push to push your branch up to GitHub.\n Open a Pull Request on GitHub.\n  I respond to your pull request (step 5 of 5) I will either:\n comment on your pull request, and/or\n merge it into master.\n  Additional Tools RStudio integration  RStudio has great features for using Git and GitHub from within the IDE.\n See RStudio\u0026rsquo;s documentation.\n Check out the RStudio IDE Cheat Sheet.\n  The key thing is that a Project in RStudio is a directory (with an RStudio configuration file, blah.Rproj) your .git folder will be stored in this same directory.\ncode.harvard.edu  https://code.harvard.edu/ Enterprise GitHub Private collaboration for teams  Free accounts on github.com now get unlimited private repositories, but they are limited to 3 collaborators.\ncode.harvard.edu is great for teams at Harvard that want to keep their work private.\nGraphical User Interfaces (GUIs)  GitHub Desktop is user-friendly GitKraken is cross-platform  Personally, I want my editor to have Git integration. I don\u0026rsquo;t like having a separate GUI for dealing with Git.\nIf you are working with Git at the command line a lot, I have enjoyed Bash-it. Their git aliases save a lot of typing.\nReferences  git - the simple guide Git is a Directed Acyclic Graph and What the Heck Does That Mean? 4 branching workflows for Git Oh shit, git! What is version control: centralized vs. DVCS  FAQ  If people were going to adopt version control only for particular projects (e.g., those that were especially complicated, had many collaborators, etc.) is there any rule of thumb that youd give people for when a project gets big enough that it really should be version controlled? I know you version control for all projects, but I think people will likely use some combination of version controlling (project dependent) and not.\n I don\u0026rsquo;t have a clear-cut answer here. The advantage of focusing on small projects is that it\u0026rsquo;s a great opportunity to get familiar with Git. The advantage to focusing on larger projects is they\u0026rsquo;re likely to have longer histories so the benefits of working with version control will become more apparent.\n Can you add commit statements to your code so that it commits as you initialize a session or at various points along the way in a program (not sure if that makes sense as written out)?\n Yes, but this is dangerous. You want to be very explicit with Git. Tell it exactly when you want to take a snapshot of your code. If you start automating that process, you\u0026rsquo;ll likely end up with a less useful history.\n Because people were generally very apprehensive about Git, Im wondering whether it might help to divide up the sections by the very very basic commands and then the more advanced commands that might or might not come up in RCS work so they can focus on at least knowing the basics. People conveyed that they got especially confused when dealing with branching, merging, forking, etc.\n Absolutely! I tried to do this by dividing commands between the Git and GitHub sections.\nIn the Git section, we cover status, diff, log, add, commit, mv, and rm. These commands are necessary for keeping a completely linear history.\nIn the GitHub section, we cover push, pull, fetch, and merge. These are necessary for collaborating with others. We also cover branch and checkout in this section. But, maybe it makes sense to cover branches separately because they\u0026rsquo;re freaking people out.\n I really like the idea of doing some hands on work during the session as youve added at the end. (confession: I am intimidated by forking, cloning, etc. over command line, but Im sure well all be more comfortable by the end of the session)\n No reason to be intimidated. You\u0026rsquo;ve got this!\n How do I delete a repo?\n  On your computer, if you delete the .git subdirectory, the folder will no longer be a Git repository.\n On GitHub, go to the settings for the repository and head down to the \u0026ldquo;Danger Zone\u0026rdquo;.\n   Why is committing a two-step process? Sometimes you\u0026rsquo;ll be editing a whole bunch of files at once and you want to take a few different photos so the project history is easier to read. It\u0026rsquo;s a non-intuitive process but the flexibility is a plus for many Git users.\n[return] Occasionally, I have had to edit this file to fix unwanted behavior. There are three different constructions for the url:\n https://github.com/user/repo git://github.com/user/repo git@github.com:user/repo  With https://, you\u0026rsquo;ll need to enter your GitHub login and password each time. With git://, you\u0026rsquo;ll have read-only access. With git@, you need to set up ssh (more work initially, but you\u0026rsquo;ll get write access without having to enter your login and password).\n[return]   "
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/organization/",
	"title": "Organizing Projects",
	"tags": [],
	"description": "",
	"content": " I\u0026rsquo;m trying to cover two things here: how to organize data analysis projects, so in the end the results will be reproducible and clear, and how to capture the results of exploratory data analysis.\nThe hardest part, regarding organizing projects, concerns how to coordinate collaborative projects: to keep data, code, and results synchronized among collaborators.\nRegarding exploratory data analysis, we want to capture the whole process: what you\u0026rsquo;re trying to do, what you\u0026rsquo;re thinking about, what you\u0026rsquo;re seeing, and what you\u0026rsquo;re concluding and why. And we want to do so without getting in the way of the creative process.\nI\u0026rsquo;ll sketch what I try to do, and the difficulties I\u0026rsquo;ve had. But I don\u0026rsquo;t have all of the answers.\n\\begin{frame}[c]{}\n\\begin{center} \\large File organization and naming are powerful weapons against chaos.\n\\end{center}\n{\\lolit {\\textendash} Jenny Bryan }\nYou don\u0026rsquo;t {\\nhilit need to be organized, but it sure will help others (or yourself, later), when you try to figure out what it was that you did.\nSegregate all the materials for a project in one directory/folder on your harddrive.\nI prefer to separate raw data from processed data, and I put code in a separate directory.\nWrite ReadMe files to explain what\u0026rsquo;s what. }\nOrganizing your stuff Code/d3examples/ /Others/ /PyBroman/ /Rbroman/ /Rqtl/ /Rqtlcharts/ Docs/Talks/ /Meetings/ /Others/ /Papers/ /Resume/ /Reviews/ /Travel/ Play/ Projects/AlanAttie/ /BruceTempel/ /Hassold_QTL/ /Hassold_Age/ /Payseur_Gough/ /PhyloQTL/ /Tar/  This is basically how I organize my hard drive. You want it to be clear where things are. You shouldn\u0026rsquo;t be searching for stuff.\nIn my Projects/ directory, I have a Tar/ directory with tar.gz files of older projects; the same is true for other directories, like Docs/Papers/ and Docs/Talks/.\nOrganizing your projects Projects/Hassold_QTL/ Data/ Notes/ R/ R/Figs/ R/Cache/ Rawdata/ Refs/ Makefile Readme.txt Python/convertGeno.py Python/convertPheno.py Python/combineData.py R/prepData.R R/analysis.R R/diagnostics.Rmd R/qtl_analysis.Rmd  This is how I\u0026rsquo;d organize a simple project.\nSeparate the raw data from processed data.\nSeparate code from data.\nInclude a Readme file and a Makefile.\nI tend to reuse file names. Almost every project will have an {\\tt R/prepData.R script.\nOf course, each project is under version control (with git)!\nR/analysis.R usually has exploratory analyses, and then there\u0026rsquo;ll be separate .Rmd files with more finalized work. }\nOrganizing a paper Docs/Papers/PhyloQTL/ Analysis/ Data/ Figs/ Notes/ R/ SuppFigs/ ReadMe.txt Makefile phyloqtl.tex phyloqtl.bib Submitted/ Reviews/ Revised/ Final/ Proofs/  This is how I organize the material for a paper.\nR/ contains code for figures; Analysis/ contains other analysis code; Data/ contains data; Figs/ contains the figures; Notes/ contains notes or references.\nOf course, a Makefile for compiling the PDF, and perhaps a ReadMe file to explain where things are.\nAnd I\u0026rsquo;ll save the submitted version (and text files with bits for web forms at submission), plus reviews, the revised version plus response to reviews, and then the final submitted version and the proofs.\nOrganizing a talk Docs/Talks/SampleMixups/ Figs/ R/ ReadMe.txt Makefile bmi2013.tex Old/  This is how I organize the material for a talk: much like a paper, but generally a bit simpler.\nAgain, R/ contains code for figures and Figs/ contains the actual figures.\nAnd again, a Makefile for compiling the PDF, and perhaps a ReadMe file to explain where things are.\nAnd I\u0026rsquo;ll save all old versions in Old/\nBasic principles  Develop your own system Put everything in a common directory Be consistent directory structure; names Separate raw from processed data Separate code from data It should be obvious what code created what files, and what the dependencies are. No hand-editing of data files Don\u0026rsquo;t use spaces in file names Use relative paths, not absolute paths [] ../blah \\; not \\; \\vhilit {~/blah} \\; or \\; \\vhilit /users/blah I work on many different projects at the same time, and I\u0026rsquo;ll come back to a project 6 months or a year later.  I don\u0026rsquo;t want to spend much time figuring out where things are and how things were created: have a Makefile, and keep notes. But notes are not necessarily correct while a Makefile would be.\nPlan for the whole deal to ultimately be open to others: will you be proud of the work, or embarrassed by the mess?\n\\begin{frame}[c]{}\n\\centering \\large Your closest collaborator is you six months ago, but you don\u0026rsquo;t reply to emails. I heard this from Paul Wilson, UW-Madison.\nThe original source is a tweet by Karen Cranston, quoting Mark Holder.\nhttps://twitter.com/kcranstn/status/370914072511791104\n\\begin{frame}[c]{}\n\\centering \\large Organization takes time.\nThere\u0026rsquo;s no getting around the fact that doing things properly takes longer, in the short term.\nIf you have a good system and good habits, it won\u0026rsquo;t seem like it takes so long.\nBut definitely, it\u0026rsquo;s a large up-front investment in order to potentially save a lot of time and aggravation later.\nPainful bits  Coming up with good names for things Code as verbs; data as nouns Stages of data cleaning Going back and redoing stuff Clutter of old stuff that you no longer need Keeping track of the order of things dependencies; what gave rise to what Long, messy Makefiles \\only{\\centerline-\u0026gt; Modularity}  I don\u0026rsquo;t have many solutions to these problems. Version control helps. And try to break things down into different stages, in case one aspect needs to be revised. Maybe use different subdirectories for the different stages of data cleaning.\nA point that was raised in the discussion: Have periodic \u0026ldquo;versions\u0026rdquo; for a project, perhaps labeled by date. Move all the good stuff over and retire the stuff that is no longer useful or necessary.\n\\begin{frame}[c]{}\n\\figh{Figs/iso_8601.png}{0.8}\n\\footnotesize \\lolit [xkcd.com/1179](http://xkcd.com/1179/)\nGo with the xkcd format for writing dates, for ease of sorting.\nProblem: Variations across data files  Different files (or parts of files!) may have different formats. Variables (or factor levels) may have different names in different files. The names of files may inconsistent.\n It\u0026rsquo;s tempting to hand-edit the files. {\\vhilit Don\u0026rsquo;t!}\n Create another meta-data file that explains what\u0026rsquo;s what. Scientists aren\u0026rsquo;t trained in how to organize data.\n  Multiple people in a lab might have his/her own system, or an individual\u0026rsquo;s system may change over time (or from the top to the bottom of a file!)\nCreate a separate file with meta-data: \u0026ldquo;These are the files. In this file, the variable is called {\\nhilit blah while in that file it\u0026rsquo;s {\\nhilit blather}.\u0026rdquo;\nThe meta-data file should be structured as data (e.g., as a comma- or tab-delimited file) for easy parsing. }\nTidy data Read Hadley Wickham\u0026rsquo;s paper on Tidy Data.\n Each variable forms a column. Each observation forms a row. Each type of observational unit forms a table. {\\footnotesize \\renewcommand{\\arraystretch}{1.05} \\begin{center} \\begin{tabular}{ccc} \\hline Mouse \u0026amp; Treatment \u0026amp; Response \\ \\hline 1 \u0026amp; control \u0026amp; \u0026ndash; 1 \u0026amp; ttt \u0026amp; 7.4 2 \u0026amp; control \u0026amp; 3.8 2 \u0026amp; ttt \u0026amp; 5.2 3 \u0026amp; control \u0026amp; 5.5 3 \u0026amp; ttt \u0026amp; 6.6 \\ \\hline \\end{tabular} \\end{center} }  Read the paper!\nWhen you convert data into a better form, convert it into the {\\nhilit tidy form.\nAlso, consider Hadley\u0026rsquo;s tools, like dplyr }\nProblem: 80 million side projects $ ls ~/Projects/Attie AimeeNullSims/ Deuterium/ Ping/ AimeeResults/ ExtractData4Gary/ Ping2/ AnnotationFiles/ ForFirstPaper/ Ping3/ Brian/ FromAimee/ Ping4/ Chr10adipose/ GoldStandard/ Play/ Chr6_extrageno/ HumanGWAS/ Proteomics/ Chr6hotspot/ Insulin/ R/ ChrisPlaisier/ Islet_2011-05/ RBM_PlasmaUrine/ Code4Aimee/ Lusis/ R_adipose/ CompAnnot/ MappingProbes/ R_islet/ CondScans/ Microarrays/ Rawdata/ D2O_2012-02-14/ MultiProbes/ Scans/ D2O_Nrm_2012-02-29/ NewMap/ SimsRePower/ D2O_cellcycle/ Notes/ Slco1a6/ D2Ocorr/ NullSims/ StudyLineupMethods/ Data4Aimee/ NullSims_2009-09-10/ eQTLPaper/ Data4Tram/ PepIns_2012-02-09/ transeQTL4Lude/  This is a project-gone-wrong.\nA key problem in research is that you don\u0026rsquo;t really know what you\u0026rsquo;re doing when you get started. It seems best to separate out each side-project as a separate directory, but it can be a nightmare to find things later.\nIf each of these subdirectories was nicely organized and had a ReadMe file, you could grep your way through them.\nI sort of like the idea of separate directories for the different aspects of mucking about. And second versions are always better. Maybe we should plan to muck about separately and then bring a more refined analysis back into a common directory?\nA point raised in the discussion: Put defunct side projects into an Old/ subdirectory, and put active but not yet clearly interesting ones into New/ or Play/. This will help to avoid the clutter.\nSaving intermediate results R Markdown document with details of data cleaning.\n Within the .Rmd file, periodically save the state of things, for further exploratory analysis.\n Put those intermediate files (which might be large) in a common subdirectory.\n The subdirectory could be under separate version control.\n But you\u0026rsquo;ll need to go in there and commit files. I want a reproducible analysis document, but I want to be able to grab objects from the middle of the process for further exploration. So I\u0026rsquo;ll include code chunks to save the state of things, say in a Cache or RData subdirectory.\n  Subdirectories can be their own git repositories: Include that subdirectory in the .gitignore file, and then use git init within the subdirectory.\nA point raised in the discussion: how to synchronize a project between computers? If we don\u0026rsquo;t put the intermediate files in the main repository, we can\u0026rsquo;t rely on GitHub. (For a simple manuscript or talk, it\u0026rsquo;s okay to reconstruct things on another computer, but for big analyses, you wouldn\u0026rsquo;t want to.) I use ChronoSync to synchronize my Mac desktop and laptop. Maybe Dropbox or Google Drive would be useful for this. You\u0026rsquo;d still want to use git and and GitHub, but you could supplement them by having the repository sit in your Dropbox folder.\nProblem: Coordinating with collaborators  Where to put data that multiple people will work with? Where to put intermediate/processed data? Where to indicate the code that created those processed data files? How to divvy up tasks and know who did what? Need to agree on directory structure and file naming conventions Consider symbolic links for shared data directories [] ln -s /z/Proj/blah [] ln -s /z/Proj/blah my_blah Ideally, everything synchronized with git/GitHub.  The keys: planning and regular communication\nSymbolic links are also called \u0026ldquo;soft links.\u0026rdquo; It\u0026rsquo;s just like a file shortcut in Windows.\nProblem: Collaborators who don\u0026rsquo;t use git \\only{\n\\vspace*{48pt}\n\\centerline{Um\\dots} }\n\\only{ * Use git yourself * Copy files to/from some shared space * Ideally, in an automated way * Commit their changes. }\nLife would be easier if all of our analysis collaborators adopted git. Teach them how?!\nWhen I\u0026rsquo;m working with a collaborator on a paper, I may get comments from them as a marked-up PDF. I\u0026rsquo;ll save that in the repository and will incorporate and commit the changes in the source files, on my own.\nExploratory data analysis  what were you trying to do? what you\u0026rsquo;re thinking about? what did you observe? what did you conclude, and why? We want to be able to capture the full outcome of exploratory data analysis.  But we don\u0026rsquo;t want to inhibit the creative flow. How to capture this stuff?\nAvoid  \u0026ldquo;How did I create this plot?\u0026rdquo; \u0026ldquo;Why did I decide to omit those six samples?\u0026rdquo; \u0026ldquo;Where (on the web) did I find these data?\u0026rdquo; \u0026ldquo;What was that interesting gene?\u0026rdquo; I\u0026rsquo;ve said all of these things to myself.  Basic principles  [] Step 1: slow down and document. [] Step 2: have sympathy for your future self. [] Step 3: have a system. I can\u0026rsquo;t emphasize these things enough.  If you\u0026rsquo;re not {\\nhilit thinking about keeping track of things, you won\u0026rsquo;t keep track of things.\nOne thing I like to do: write a set of comments describing my basic plan, and then fill in the code afterwards. It forces you to think things through, and then you\u0026rsquo;ll have at least a rough sense of what you were doing, even if you don\u0026rsquo;t take the time to write further comments. }\nCapturing EDA  copy-and-paste from an R file grab code from the .Rhistory file Write an informal R Markdown file Write code for use with the KnitR function spin() [] Comments like \\; \\tt #\u0026rsquo; This will become text [] Chunk options like so: \\; \\tt #+ chunk_label, echo=FALSE There are a number of techniques you can use to capture the EDA process.  You don\u0026rsquo;t need to save all of the figures, but you do need to save the code and write down your motivation, observations, and conclusions.\nI usually start out with a plain R file and then move to more formal R Markdown or AsciiDoc reports.\nA file to spin() #' This is a simple example of an R file for use with spin(). #' We'll start by setting the seed for the RNG. set.seed(53079239) #' We'll first simulate some data with x ~ N(mu=10, sig=5) and #' y = 2x + e, where e ~ N(mu=0, sig=2) x \u0026lt;- rnorm(100, 10, 5) y \u0026lt;- 2*x + rnorm(100, 0, 2) #' Here's a scatterplot of the data. plot(x, y, pch=21, bg=\u0026quot;slateblue\u0026quot;, las=1)  Here\u0026rsquo;s an example R file for use with spin().\nI almost forgot \\centerline{\\Large Backups}\n\\only{\n\\centerline{Next two weeks: Clear code and R packages} }\nYou {\\nhilit must back up your stuff.\n You can generally rely on your department server, and you should also make use of GitHub. But if you have other stuff on a laptop or at home, you want to be sure to back that up to. Hard drives are cheap. On a Mac, I use the built-in Time Machine, but I also use SuperDuper! to create a bootable clone. Also important in all of this is writing clear, modular code. With R, it's best to pull out reuseable code as an R package. We'll talk about these two topics over the next two weeks.  }\n"
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/clear-code/",
	"title": "Writing Clear Code",
	"tags": [],
	"description": "",
	"content": " \\figh{Figs/i_dont_get_your_code.jpg}{0.8}\n\\footnotesize \\lolit [geekandpoke.typepad.com](http://geekandpoke.typepad.com/geekandpoke/2008/02/the-art-of-prog.html)\nIt\u0026rsquo;s funny because it\u0026rsquo;s true.\nClear code is more likely to be correct.\nClear code is easier to use.\nClear code is easier to maintain.\nClear code is easier to extend.\nBasic principles  Code that works  [] No bugs; efficiency is secondary (or tertiary) Readable [] Fixable; extendible Reusable [] Modular; reasonably general Reproducible [] Re-runnable Think before you code [] More thought $\\implies$ fewer bugs/re-writes Learn from others\u0026rsquo; code [] R itself; key R packages Our first goal should be to get the right answer.   But we also want to write code that we (and others) will be able to use again. Efficiency is way down on the list of needs.\nWrite clearly, and re-write for greater clarity.\nBreak things down into small, re-usable functions, written in a general way, but not {\\nhilit too general. You want to actually solve a particular problem.\nStep 1: stop and think.\nStep 2: write re-usable functions rather than a script.\nBy reproducible/re-runnable, I mean: don\u0026rsquo;t have a bunch of code from which you copy-and-paste in a special way. }\n\\begin{frame}[c]{}\n\\vspace*{30mm}\n\\centerline{\\large Write programs for people, not computers}\n{\\footnotesize Wilson et al. (2014) PLoS Biol 12:e1001745}\nThe computer doesn\u0026rsquo;t care about comments, or style, or the naming of things. But users and collaborators (and yourself six months from now) will.\nBreak code into small functions get_grid_index \u0026lt;- function(vec, step) { grid \u0026lt;- seq(min(vec), max(vec), by=step) index \u0026lt;- match(grid, vec) if(any(is.na(index))) index \u0026lt;- sapply(grid, function(a,b) { d \u0026lt;- abs(a-b) wh \u0026lt;- which(d==min(d)) if(length(wh)\u0026gt;1) wh \u0026lt;- sample(wh, 1) wh }, vec) index }  This is a bit of code that was originally part of a larger function.\nSeparated as a function, the code is more likely to be reused. And the smaller and more focused the function, the more likely you\u0026rsquo;ll use it again.\nAvoid having scripts that are long streams of code. Write functions. Give each function a single, focused task. Break long functions into pieces.\nComment on the input and output; ideally, these are obvious from the names.\nWhat might be improved about this function?\nBreak code into small functions sampleone \u0026lt;- function(vec) ifelse(length(vec)==1, vec, sample(vec, 1)) get_grid_index \u0026lt;- function(vec, step) { grid \u0026lt;- seq(min(vec), max(vec), by=step) index \u0026lt;- match(grid, vec) if(any(is.na(index))) index \u0026lt;- sapply(grid, function(a,b) { d \u0026lt;- abs(a-b) sampleone(which(d == min(d))) }, vec) index }  Ideally, a function does only one thing.\nPulling out the {\\ttsm sampleone code as a separate function makes the body of {\\ttsm get_grid_index} a bit simpler.\nIs the {\\ttsm ifelse} line clear? Would it be better to just write that function with {\\ttsm if} and {\\ttsm else}?\nCould {\\ttsm get_grid_index} be improved further? }\nClarity over efficiency sampleone \u0026lt;- function(vec) ifelse(length(vec)==1, vec, sample(vec, 1)) get_grid_index \u0026lt;- function(vec, step) { grid \u0026lt;- seq(min(vec), max(vec), by=step) index \u0026lt;- match(grid, vec) if(any(is.na(index))) { for(i in seq(along=grid)) { d \u0026lt;- abs(grid[i] - vec) index[i] \u0026lt;- sampleone(which(d==min(d))) } } index }  The use of {\\ttsm sapply is a bit confusing and hard to follow. I think it\u0026rsquo;s a bit more clear to use a {\\ttsm for} loop.\nWhether {\\ttsm apply}-type functions are more readable or less readable depends on the complexity of the function that is applied but also on the experience of the reader. }\nOne last change sampleone \u0026lt;- function(vec) ifelse(length(vec)==1, vec, sample(vec, 1)) get_grid_index \u0026lt;- function(vec, step) { grid \u0026lt;- seq(min(vec), max(vec), by=step) index \u0026lt;- match(grid, vec) missing \u0026lt;- is.na(index) if(any(missing)) { for(i in which(missing)) { d \u0026lt;- abs(grid[i] - vec) index[i] \u0026lt;- sampleone(which(d==min(d))) } } index }  One last change: The loop really only needs to be over the parts of {\\ttsm index that are missing.\nOf course, I should also add a comment for each function, to explain the input and output. }\nAnother example # rmvn: simulate from multivariate normal distribution rmvn \u0026lt;- function(n, mu=0, V=diag(rep(1, length(mu)))) { p \u0026lt;- length(mu) if(any(dim(V) != p)) stop(\u0026quot;Dimension problem!\u0026quot;) D \u0026lt;- chol(V) matrix(rnorm(n*p),ncol=p) %*% D + rep(mu,each=n) }  I\u0026rsquo;m often wanting to simulate from a multivariate normal distribution. It\u0026rsquo;s not hard, but you have to remember: do you do Z \\%*\\% D or D \\%*\\% Z? And in any case, it\u0026rsquo;s a lot easier to just write rmvn(n, mu, V).\nNote the use of default values.\nFurther examples # colors from blue to red revrainbow \u0026lt;- function(n=256, ...) rev(rainbow(start=0, end=2/3, n=n, ...)) # move values above/below quantiles to those quantiles winsorize \u0026lt;- function(vec, q=0.006) { lohi \u0026lt;- quantile(vec, c(q, 1-q), na.rm=TRUE) if(diff(lohi) \u0026lt; 0) lohi \u0026lt;- rev(lohi) vec[!is.na(vec) \u0026amp; vec \u0026lt; lohi[1]] \u0026lt;- lohi[1] vec[!is.na(vec) \u0026amp; vec \u0026gt; lohi[2]] \u0026lt;- lohi[2] vec }  If there\u0026rsquo;s a bit of code that you write a lot, turn it into a function.\nI can\u0026rsquo;t tell you how many times I typed rev(rainbow(start=0, end=2/3, n=256)) before I thought to make it a function. (And I can\u0026rsquo;t tell you how many times I\u0026rsquo;ve typed it since, having forgotten that I wrote the function.)\n(And really, I should forget about {\\ttsm revrainbow; such rainbow color scales are generally considered a bad idea.)\nI learned about Winsorization relatively recently; it\u0026rsquo;s a really useful technique to deal with possible outliers. It\u0026rsquo;s not hard. But how much easier to just write winsorize()! }\nWriting functions  Break large tasks into small units.  Make each discrete unit a function. If you write the same code more than once, make it a function.  If a line/block of code is complicated, make it a function. Modularity is really important for readability.  No long streams of code; rather, a short set of function calls.\nDon\u0026rsquo;t repeat yourself (or others)  Avoid having repeated blocks of code. Create functions, and call those functions repeatedly. This is easier to maintain.  If something needs to be fixed/revised, you just have to do it the one time. Look at others\u0026rsquo; libraries/packages. Don\u0026rsquo;t write what others have already written (especially if they\u0026rsquo;ve done it better than you would have). Don\u0026rsquo;t repeat yourself (DRY) is one of the more important concepts for programmers.   I\u0026rsquo;m particularly bad at making use of others\u0026rsquo; code.\nDon\u0026rsquo;t make things too specific  Write code that is a bit more general than your specific data  Don\u0026rsquo;t assume particular data dimensions. Don\u0026rsquo;t forget about the possibility of missing values\n(even if your data doesn\u0026rsquo;t have any). Aim for re-use. Use function arguments Don\u0026rsquo;t assume particular data file names Don\u0026rsquo;t hard-code tuning parameters R scripts can take command-line arguments:\\[2pt] [] {\\ttsm Rscript myscript.R input_file output_file} \\[2pt] [] {\\ttsm args \u0026lt;- commandArgs(TRUE)} Don\u0026rsquo;t make things too general, either. You want to actually solve your particular problem.   You don\u0026rsquo;t want to have to edit the code for different data.\nWe {\\nhilit will write scripts. But make them a bit more general by allowing command-line arguments to specify input and output file names. You can always include defaults, for your particular files. Even better is to have these specified in your {\\tt Makefile.} }\nNo global variables, ever!  Don\u0026rsquo;t refer directly to objects in your workspace. If a function needs something, pass it as an argument. (But what about really big data sets?) Global variables make code unreadable.  Global variables make code difficult to reuse.\nIt turns out that, when you pass data into a function, R doesn\u0026rsquo;t actually make a copy. If your function makes any change to the data object, {\\nhilit then it will make a copy. But you shouldn\u0026rsquo;t be afraid to pass really big data sets into and between functions. }\nNo magic numbers  Name numbers and use the names  [] max_iter \u0026lt;- 1000 [] tol_convergence \u0026lt;- 0.0001 Even better: include them as function arguments You\u0026rsquo;ll later ask yourself, \u0026ldquo;Why 12?\u0026rdquo;   When you include such things as function arguments, include reasonable default values.\nIndent! # move values above/below quantiles to those quantiles winsorize \u0026lt;- function(vec, q=0.006) { lohi \u0026lt;- quantile(vec, c(q, 1-q), na.rm=TRUE) if(diff(lohi) \u0026lt; 0) lohi \u0026lt;- rev(lohi) vec[!is.na(vec) \u0026amp; vec \u0026lt; lohi[1]] \u0026lt;- lohi[1] vec[!is.na(vec) \u0026amp; vec \u0026gt; lohi[2]] \u0026lt;- lohi[2] vec }  I taught part of a course on statistical programming at Johns Hopkins. It included a lecture like this one. Still, many students submitted their programming assignment flush left.\nMost people recommend indenting by 4 spaces. Still, I prefer 2.\n(4 spaces are easier to track.)\nDon\u0026rsquo;t use tabs, as they get messed up when moving between computers.\nUse white space # move values above/below quantiles to those quantiles winsorize\u0026lt;-function(vec,q=0.006) {lohi\u0026lt;-quantile(vec,c(q,1-q),na.rm=TRUE) if(diff(lohi)\u0026lt;0)lohi\u0026lt;-rev(lohi) vec[!is.na(vec)\u0026amp;vec\u0026lt;lohi[1]]\u0026lt;-lohi[1] vec[!is.na(vec)\u0026amp;vec\u0026gt;lohi[2]]\u0026lt;-lohi[2] vec}  The computer doesn\u0026rsquo;t care about white space, but people do.\nDon\u0026rsquo;t let lines get too long get_grid_index \u0026lt;- function(vec, step) { grid \u0026lt;- seq(min(vec), max(vec), by=step) index \u0026lt;- match(grid, vec) if(any(is.na(index))) index \u0026lt;- sapply(grid, function(a,b) { d \u0026lt;- abs(a-b); sampleone(which(d == min(d))) }, vec) index }  $\u0026lt;$ 72 characters per line\nUse parentheses to avoid ambiguity if( (ndraws1==1) \u0026amp;\u0026amp; (ndraws2\u0026gt;1) ) { ... } leftval \u0026lt;- which( (map - start) \u0026lt;=0 )  Even if they aren\u0026rsquo;t {\\nhilit necessary, they can be helpful to you (or others) later. }\nNames: meaningful  Make names descriptive but concise Avoid tmp1, tmp2, \u0026hellip; Only use i, j, x, y in the simplest situations If a function is named fv, what might it do? If an object is called nms, what could it be? Functions as verbs; objects as nouns I\u0026rsquo;m terrible at coming up with good names. But it\u0026rsquo;s really important.  Descriptive names make code self-documenting.\nIf functions are named by what they do, you may not need to explain further.\nIf the object names are meaningful, the code will be readable as it stands. If the objects are named g, g2, and {\\tt gg, it\u0026rsquo;ll need a lot of comments.\nI admit, I\u0026rsquo;m a terrible offender in this regard, especially when I\u0026rsquo;m coding in a hurry. Do as I say, not as I do. }\nNames: consistent  markers vs mnames\n camelCase vs. pothole_case\n nind vs n.var\n If a function/object has one of these, there shouldn\u0026rsquo;t be a function/object with the other.\n  Consistency makes the names easier to remember.\nIf the names are easier to remember, you\u0026rsquo;re less likely to use the wrong name and so introduce a bug.\nAnd, of course, other users will appreciate the consistency.\nNames: avoid confusion  Don\u0026rsquo;t use both total and totals Don\u0026rsquo;t use both n.cluster and n.clusters Don\u0026rsquo;t use both result and results Don\u0026rsquo;t use both Mat and mat Don\u0026rsquo;t use both g and gg Don\u0026rsquo;t use similar but different names, in the same function or across functions.  It\u0026rsquo;s confusing, and it\u0026rsquo;s prone to bugs from mistyping.\nDon\u0026rsquo;t be cute \\figw{Figs/richierocks_tweet.png}{0.8}\n\\footnotesize \\lolit [@richierocks](https://twitter.com/richierocks/status/388609208293556224)\ndrop_the_bom might actually be quite memorable, which would make it a good choice.\nAnd think of knitr: knit, purl, spin, {\\tt stitch. Those work.\nBut generally I\u0026rsquo;d say: be direct rather than cute. Perhaps remove_byteorder_marks }\nComments  Comment the tricky bits and the major sections Don\u0026rsquo;t belabor the obvious Don\u0026rsquo;t comment bad code; rewrite it Document the input/output and purpose, not the mechanics Don\u0026rsquo;t contradict the code  this happens if you revise the code but don\u0026rsquo;t revise the related comments Comment code as you are writing it (or before)  Plan to spend 1\u0026frasl;4 of your time commenting Some things are tricky and deserve explanation, so that when you come back to it, you have a guide.  But in many cases, code can be rewritten to be clear and self-documenting.\nCommenting takes time. And no one ever goes back and adds comments later. It\u0026rsquo;s another one of those \u0026ldquo;invest for the future\u0026rdquo; type of things.\nError/warning messages {\\small * Explain what\u0026rsquo;s wrong (and where) * {\\ttsm error(\u0026ldquo;nrow(X) != nrow(Y)\u0026rdquo;)} * Suggest corrective action * {\\ttsm \u0026ldquo;You need to first run calc.genoprob().\u0026rdquo;} * Give details * **\u0026quot;nrow(X) (\u0026quot;**, nrX, **\u0026quot;) != nrow(Y) (\u0026quot;**, nrY, **\u0026quot;)\u0026quot;** * Don\u0026rsquo;t give error/warning messages that users won\u0026rsquo;t understand. * {\\ttsm X\u0026rsquo;X is singular.} * Don\u0026rsquo;t let users do something stupid without warning\n Include error checking even in personal code. }  Meaningful error messages are hard.\n Writing them to give detailed information takes time. And of course {\\nhilit you wouldn't be making these errors, right? But remember that you, six months from now, will be like a whole new person.  }\nCheck data integrity  Check that the input is as expected, or give warnings/errors.\n Write these in the first pass (though they\u0026rsquo;re dull).\n You may not remember your assumptions later These are useful for documenting the assumptions. You can\u0026rsquo;t include enough such checks.   Hadley Wickham\u0026rsquo;s assertthat package is great for this. We\u0026rsquo;ll talk about it in a couple of weeks.\nProgram organization  Break code into separate files (say 300 lines?)\n Each file includes related functions\n Files should be named meaningfully\n Include a brief comment at the top. My R/qtl package has some {\\nhilit really long files, like {\\tt util.R}, which seemed like a good idea at the time. I have to use a lot of grep to find things. And there are plenty of functions in there that I have totally forgotten about.\n  I used to make really long boiler-plate comments at the top of each file. That means that you {\\nhilit always} have to page down to get to the code. Now I try to write really minimal comments. }\nCreate an R package!  Make a personal package with bits of your own code Mine is R/broman, \\ttsm github.com/kbroman/broman\n# qqline corresponding to qqplot qqline2 \u0026lt;- function(x, y, probs = c(0.25, 0.75), qtype = 7, ...) { stopifnot(length(probs) == 2) x \u0026lt;- quantile(x, probs, names=FALSE, type=qtype, na.rm = TRUE) y \u0026lt;- quantile(y, probs, names=FALSE, type=qtype, na.rm = TRUE) slope \u0026lt;- diff(y)/diff(x) int \u0026lt;- y[1L] - slope*x[1L] abline(int, slope, ...) invisible(c(intercept=int, slope=slope)) }  R packages are great for encapsulating and distributing R code with documentation.\n  Every R programmer should have a personal package containing the various functions that they use day-to-day.\nWe\u0026rsquo;ll talk about writing R packages next week. It\u0026rsquo;s really rather easy.\nI give an example here of one function in my R/broman package. Some of the earlier examples are in this package.\nComplex data objects  Keep disparate data together in a more complex structure.  lists in R I also like to hide things in object attributes It\u0026rsquo;s easier to pass such objects between functions  Consider object-oriented programming Complex data objects allow you to think about complex data as a single thing. And they make it easier to pass the data into a function.  R has two systems for object-oriented programming: S3 and S4. They can make your code easier to use and understand. But they can also make things more opaque and harder to understand.\nAvoiding bugs  Learn to type well. Think before you type. Consider commenting before coding. Code defensively  Handle cases that \u0026ldquo;can\u0026rsquo;t happen\u0026rdquo; Code simply and clearly  Use modularity to advantage Think through all special cases Don\u0026rsquo;t be in too much of a hurry Writing clear code will make it easier to find bugs, but we also want to avoid introducing bugs in the first place. These are my strategies; They are useful when applied.  Basic principles  Code that works  [] No bugs; efficiency is secondary (or tertiary) Readable [] Fixable; extendible Reusable [] Modular; reasonably general Reproducible [] Re-runnable Think before you code [] More thought $\\implies$ fewer bugs/re-writes Learn from others\u0026rsquo; code [] R itself; key R packages It seems useful to repeat these basic principles.   Summary  Get the correct answers. Find a clear style and stick to it. Plan for the future. Be organized. Don\u0026rsquo;t be too hurried. Learn from others. Our primary goal, as scientific programmers, is to get the right answer.  We all write differently. That\u0026rsquo;s okay. But be consistent. If every function is named in a different way, it will be that much harder to remember.\nPlan for the future: write code that is readable and easy to maintain. And write things in a modular and reasonably general way, so that you (or others) will be more likely to be able to re-use your work.\nBe organized. Well-organized software is easier to understand and maintain.\nIt\u0026rsquo;s when I\u0026rsquo;m hurried that I write crap code. It means more work later.\nThere are a lot of great programmers out there; read their code and learn from it.\n"
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/r-packages/",
	"title": "Writing R packages",
	"tags": [],
	"description": "",
	"content": " R packages and the Comprehensive R Archive Network (CRAN) are important features of R. R packages provide a simple way to distribute R code and documentation. And they really are rather simple to create.\nWrite an R package to keep track of the misc.\\ R functions that you write and reuse. Write an R package to distribute the data and software that accompany a paper.\nThe most painful part of writing an R package is the construction of the documentation files, which are in a special .Rd format. But the Roxygen2 tool makes this rather easy: you write comments preceding each R function, in a specially structured way, and then use the Roxygen2 tool to create the .Rd files for you.\nWhy write an R package?  To distribute R code and documentation To keep track of the misc.\\ R functions you write {\\only{}and reuse} To distribute data and software accompanying a paper. R packages can be big and important.\nBut that shouldn\u0026rsquo;t scare you off. Assembling a few R functions within a package will make it vastly easier for {\\nhilit you to use them regularly. You don\u0026rsquo;t even need to distribute the package.\nAnd really, the R package system is an incredibly important feature of R. Packages on CRAN are basically guaranteed to be installable, as they are regularly built, installed, and tested on multiple systems. }\n  \\begin{frame}[c]{}\n\\figh{Figs/R-exts.png}{0.9}\nThe Writing R Extensions manual is the key source for the specifications of R packages. It\u0026rsquo;s rough going in parts, but if you want to get a package on CRAN, you should read it.\nA simple example: RSkittleBrewer \\figh{Figs/RSkittleBrewer.png}{0.8}\n\\footnotesize \\lolit [alyssafrazee.com/RSkittleBrewer.html](http://alyssafrazee.com/RSkittleBrewer.html)\nI was going to write a short example R package, but Alyssa Frazee saved me the effort. Her package is a perfect little example to illustrate how to write a package.\nIt\u0026rsquo;s also a great example of a small but really useful package. One small function could be widely useful; you just need to package it and tell people about it.\nR package contents \u0026ldquo;` RSkittleBrewer/\nDESCRIPTION NAMESPACE R/RSkittleBrewer.R R/plotSkittles.R R/plotSmarties.R man/RSkittleBrewer.Rd man/plotSkittles.Rd man/plotSmarties.Rd  \u0026ldquo;`\nIn the simplest form, an R package is a directory containing: a DESCRIPTION file (describing the package), a NAMESPACE file (indicating which functions are available to users), an R/ directory containing R code in .R files, and a man/ directory containing documentation in .Rd files.\nDESCRIPTION file \u0026ldquo; Package: RSkittleBrewer Version: 1.1 Author: Alyssa Frazee Maintainer: Alyssa Frazee \u0026lt;afrazee@jhsph.edu\u0026gt; License: MIT + file LICENSE Title: fun with R colors Description: for those times you want to make plots with... URL: https://github.com/alyssafrazee/RSkittleBrewer \u0026quot;\nThe DESCRIPTION file is pretty self-explanatory. It just contains basic information about the package and its author.\nThe simplest way to create this sort of file is to copy and edit one from some other package.\nThe only part that might be unclear is the License field. You need to choose a license. We\u0026rsquo;ll talk about this on the very last day of the course.\nFor now, I\u0026rsquo;d suggest choosing between the GPL-3 (the GNU Public License v3) and MIT licenses. GPL-3 has a \u0026ldquo;pass-through\u0026rdquo; provision: software that incorporates GPL-3 code must also be licensed as GPL-3. This is a good but restrictive thing. The MIT license is the most bare-bones license possible: it basically just says \u0026ldquo;Do what you want, but don\u0026rsquo;t blame me.\u0026rdquo;\nAn R package with the MIT license needs to also include a {\\tt LICENSE file or R will complain; copy and edit the one from the RSkittleBrewer package. }\nNAMESPACE file \u0026ldquo; export(RSkittleBrewer) export(plotSkittles) export(plotSmarties) \u0026quot;\nThe NAMESPACE file is a bit technical: it tells R what functions that will be accessible to users.\nThe point of this is to keep different packages from stepping on each others\u0026rsquo; toes.\nAn .Rd file \u0026ldquo; \\name{RSkittleBrewer} \\alias{RSkittleBrewer} \\title{Candy-based color palettes} \\description{Vectors of colors corresponding to different candies.} \\usage{RSkittleBrewer(flavor = c(\u0026quot;original\u0026quot;, \u0026quot;tropical\u0026quot;, \u0026quot;wildberry\u0026quot;, \u0026quot;M\u0026amp;M\u0026quot;, \u0026quot;smarties\u0026quot;)) } \\arguments{ * {flavor}{Character string for candy-based color palette.} } \\value{Vector of character strings representing the chosen set of colors.} \\examples{ plotSkittles() plotSmarties() } \\keyword{hplot} \\seealso{ \\code{\\link{plotSkittles}}, \\code{\\link{plotSmarties}} } \u0026quot;\nThe R documentation format is very LaTeX-like.\nIt describes what the function does, what its arguments are, and what output it produces.\nYou can further provide examples (which can also serve as tests) and links to related functions. The examples need to be {\\nhilit fast ($\\ll$ 5 sec), because they\u0026rsquo;re run frequently. (CRAN checks every package every day on multiple systems.)\nWriting these help files is tedious! That\u0026rsquo;s where Roxygen2 comes in. }\nBuilding, installing, and checking \u0026ldquo;` R CMD build RSkittleBrewer R CMD INSTALL RSkittleBrewer_1.1.tar.gz R CMD check RSkittleBrewer_1.1.tar.gz\nR CMD check \u0026ndash;as-cran RSkittleBrewer_1.1.tar.gz\nR CMD INSTALL \u0026ndash;library=~/Rlibs RSkittleBrewer_1.1.tar.gz\n(~/.Renviron file contains R_LIBS=~/Rlibs) On windows: R CMD INSTALL \u0026ndash;build RSkittleBrewer_1.1.tar.gz \u0026ldquo;`\n\u0026ldquo;`\nalso consider (within R): library(devtools) build(\u0026ldquo;/path/to/RSkittleBrewer\u0026rdquo;) build(\u0026ldquo;/path/to/RSkittleBrewer\u0026rdquo;, binary=TRUE) \u0026ldquo;`\nTo install your package, you first need to {\\nhilit build it. {\\tt R CMD build} creates the .tar.gz source file that you\u0026rsquo;d distribute.\nYou then use R CMD INSTALL to install the package. You may want to use --library=/some/dir to install to a different directory, in which case you need to set R_LIBS in your {\\tt {~}/.Renviron} file.\nR CMD check runs extensive checks of the package, including running all of the examples in the help files. Pay attention to any errors, warnings, or notes, and revise the package to avoid them. This is particularly true if you want to submit the package to CRAN.\nBefore submitting to CRAN, be sure to run R CMD check with the lesser-known --as-cran flag, which checks further things.\nOn Windows, use R CMD INSTALL --build to create a .zip file of the \u0026ldquo;compiled\u0026rdquo; package. You can also use the build() function from the devtools package. }\nRoxygen2 comments \u0026ldquo;`\nRSkittleBrewer #\u0026rsquo; Candy-based color palettes #\u0026rsquo; #\u0026rsquo; Vectors of colors corresponding to different candies. #\u0026rsquo; #\u0026rsquo; @param flavor Character string for candy-based color palette. #\u0026rsquo; #\u0026rsquo; @export #\u0026rsquo; @return Vector of character strings representing the chosen\u0026hellip; #\u0026rsquo; #\u0026rsquo; @examples #\u0026rsquo; plotSkittles() #\u0026rsquo; plotSmarties() #\u0026rsquo; #\u0026rsquo; @seealso \\code{\\link{plotSkittles}}, #\u0026rsquo; \\code{\\link{plotSmarties}} #\u0026rsquo; @keywords hplot RSkittleBrewer \u0026lt;- \u0026hellip; \u0026ldquo;`\nThe .Rd files are a pain to create and maintain. Plus, you\u0026rsquo;ll end up writing documentation in two places: in the R code, and then in the separate .Rd files.\nRoxygen2 is a system for writing structured comments in the R code that get converted to .Rd files. You just maintain the documentation in one place: in the R code. The Roxygen comments start with \\#'. The first line is the title; the second is the description.\nThe individual {/item\u0026rsquo;s within arguments become {\\tt @param}. The {/value} field becomes @return.\nYou still need to know a bit about the .Rd format; for example, the {/code{{/}link{blah}}}.\nRoxygen2 will across create the NAMESPACE file. That\u0026rsquo;s what @export is for.\nYou may also be interested in the Rd2roxygen package, for converting a package with hand-written .Rd files to the Roxygen2 system. }\nMakefile \u0026ldquo;`\nbuild package documentation doc: R -e \u0026ldquo;devtools::document()\u0026rdquo; \u0026ldquo;`\nHere\u0026rsquo;s the Makefile I use to build the .Rd files: I use the document() function within the devtools package.\nlstlisting Makefile \u0026ldquo;`\nYou\u0026rsquo;ll want to include a .Rbuildignore file in your package directory, containing the single line \u0026ldquo;{\\tt Makefile\u0026rdquo;. This tells R about files to {\\nhilit not} include in the .tar.gz file it builds. You\u0026rsquo;d get complaints from R CMD check otherwise. }\nInclude a README or README.md file \u0026ldquo;`\nfun with R Colors If you want high-quality, scientifically-researched color schemes for your R plots, check out RColorBrewer. If you want your plots to be colored the same way as packs of Skittles (or M\u0026amp;Ms), then this package (RSkittleBrewer) is the way to go.\ninstall with devtools:\n\u0026ldquo;S devtools::install_github('RSkittleBrewer', 'alyssafrazee') \u0026quot;\nuse There are only three functions in this package.\nCall RSkittleBrewer on a flavor to get a vector of R color names that correspond to that Skittle flavor. \u0026hellip; \u0026ldquo;`\nInclude a README or README.md file; R will ignore it, but it will show up on GitHub.\nYou can use ReadMe or ReadMe.md, but then you need to include it in the .Rbuildignore file or R will complain.\n\\begin{frame}[c]{}\n\\centerline{That\u0026rsquo;s it!}\nThat\u0026rsquo;s all you need to make a package.\nThere\u0026rsquo;s more, but you should start with just that stuff.\nPackage vignettes  Include vignettes/ to show how to use your package. It\u0026rsquo;s simplest to use R Markdown.\n Create a vignettes/ subdirectory. Place a .Rmd file there. The name of the file becomes the name of the vignette. Include the following in the .Rmd file\u0026rsquo;s YAML header: [] \\ttfn output: rmarkdown::html_vignette [] \\ttfn vignette: \u0026gt; [] \\ttfn \\%{/VignetteIndexEntry{Intro to RSkittleBrewer}} [] \\ttfn \\%{/VignetteEngine{knitr::rmarkdown}} [] \\ttfn {/usepackageutf8} Load the package in an initial chunk\n [] \\ttfn library(RSkittleBrewer) In my experience, people tend not to read detailed documentation, but they like tutorials. This is what **vignettes/ are for.\n   It\u0026rsquo;s easiest to write vignettes in R Markdown (allowed in R 3.0 onwards). You create a subdirectory vignettes/ and put the usual sort of .Rmd files there. You just need to add a few special lines in the YAML header for the file.\nAlso, in your .Rmd file, you\u0026rsquo;ll need to load the package before you start using it. **\nPackage vignettes  In the DESCRIPTION file, include:\n [] \\ttfn Suggests: knitr, rmarkdown [] \\ttfn VignetteBuilder: knitr The following lists the vignettes for a package and then opens a selected vignette.\n [] \\ttfn library(RSkittleBrewer)\n [] \\ttfn vignette(package=\u0026ldquo;RSkittleBrewer\u0026rdquo;)\n [] \\ttfn vignette(\u0026ldquo;RSkittleBrewer\u0026rdquo;, \u0026ldquo;RSkittleBrewer\u0026rdquo;) There\u0026rsquo;s one other little detail: you need to mention knitr as the vignette builder in your DESCRIPTION file. You also need to include knitr and rmarkdown in Suggests:.\n   If you submit the package to CRAN, links to the vignettes will be listed on web page for the package. You might also refer to them in your {\\tt README file.\nWithin R, you can use the vignette function to view the available vignettes for a package and to open a particular vignette. (With R Markdown vignettes, the compiled html file will be opened in your web browser.) }\nOptional stuff  NEWS file describing changes in each version of the package. inst/CITATION file describing how to cite your package. inst/doc/ directory any sort of misc.\\ documentation (e.g., pre-compiled computationally heavy vignettes) data/ directory containing data src/ directory containing C/C++/Fortran code demo/ directory with demonstrations (like vignettes, but to be executed in real-time). tests/ and/or inst/tests/ containing tests. We could go on and on about packages. Here are some of the key additional things to consider adding to your package.  The inst/ subdirectory is ignored by R but its contents get moved back to the root directory for the package when installed. If you want your README file to be named ReadMe, you\u0026rsquo;d put it here.\nIt\u0026rsquo;s useful to include example data sets. Your package could contain {\\nhilit only data! You should document them. See http://stackoverflow.com/questions/2310409\n}\ndevtools Get to know the devtools package.\n dev_mode() load_all() install_github(), install_bitbucket, \\dots document() build() check() check_doc() run_examples() test() \\, (next time) I\u0026rsquo;ve tried to characterize package development as super simple, but all coding involves considerable testing, debugging, and exploration, and re-building and re-installing R packages can be tedious.  The devtools package reduces a lot of the hassle. Try it out!\ndev_mode() puts you in \u0026ldquo;development mode\u0026rdquo; so that package installs won\u0026rsquo;t affect your regular R package directory. load_all() does the equivalent of installing and reloading a package. (Otherwise, you might exit R, re-build the package, re-install, invoke R, and re-load the package.)\nYou can use install_github() to install a package directly from GitHub.\nOn Mac OSX Mavericks (10.9), gnutar is gone, and devtools gives an error. The simplest solution is to make a symbolic link from tar to gnutar:\n\\qquad sudo ln -s /usr/bin/tar /usr/bin/gnutar\nSummary  R packages really aren\u0026rsquo;t that hard. R packages are really useful.  Distributing software and data Organizing code for a paper Organizing your misc.\\ R functions Look at others\u0026rsquo; packages, and learn from them.  Adopt the tools in the devtools package. It\u0026rsquo;s surprising to me just how many plain R scripts are sitting on people\u0026rsquo;s web pages. Getting them into a package is really not that hard and would make the material {\\nhilit much more useful. }  "
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/test-debug/",
	"title": "Software Testing and Debugging",
	"tags": [],
	"description": "",
	"content": " We spend a lot of time debugging. We\u0026rsquo;d spend a lot less time if we tested our code properly.\nWe want to get the right answers. We can\u0026rsquo;t be sure that we\u0026rsquo;ve done so without testing our code.\nSet up a formal testing system, so that you can be confident in your code, and so that problems are identified and corrected early.\nEven with a careful testing system, you\u0026rsquo;ll still spend time debugging. Debugging can be frustrating, but the right tools and skills can speed the process.\n\\begin{frame}[c]{}\n\\centerline{\u0026ldquo;I tried it, and it worked.\u0026rdquo;}\nThis is about the limit of most programmers\u0026rsquo; testing efforts.\n{\\nhilit But: Does it {\\nvhilit still} work? Can you reproduce what you did? With what variety of inputs did you try? }\n\\begin{frame}[c]{}\n\u0026ldquo;It\u0026rsquo;s not that we don\u0026rsquo;t test our code, it\u0026rsquo;s that we don\u0026rsquo;t store our tests so they can be re-run automatically.\u0026rdquo;\n\\lolit {\\textendash} Hadley Wickham\n{\\footnotesize R Journal 3(1):5{\\textendash10, 2011} }\nThis is from Hadley\u0026rsquo;s paper about his testthat package.\nTypes of tests \\onslide{* Check inputs * Stop if the inputs aren\u0026rsquo;t as expected. } * Unit tests * For each small function: does it give the right results in specific cases? * Integration tests * Check that larger multi-function tasks are working. * Regression tests * Compare output to saved results, to check that things that worked continue working. Your first line of defense should be to include checks of the inputs to a function: If they don\u0026rsquo;t meet your specifications, you should issue an error or warning. But that\u0026rsquo;s not really {\\nhilit testing.\nYour main effort should focus on {\\nhilit unit tests}. For each small function (and your code {\\nhilit should} be organized as a series of small functions), write small tests to check that the function gives the correct output in specific cases.\nIn addition, create larger {\\nhilit integration tests} to check that larger features are working. It\u0026rsquo;s best to construct these as {\\nhilit regression tests}: compare the output to some saved version (e.g. by printing the results and comparing files). This way, if some change you\u0026rsquo;ve made leads to a change in the results, you\u0026rsquo;ll notice it automatically and immediately. }\nCheck inputs winsorize \u0026lt;- function(x, q=0.006) { if(!is.numeric(x)) stop(\u0026quot;x should be numeric\u0026quot;) if(!is.numeric(q)) stop(\u0026quot;q should be numeric\u0026quot;) if(length(q) \u0026gt; 1) { q \u0026lt;- q[1] warning(\u0026quot;length(q) \u0026gt; 1; using q[1]\u0026quot;) } if(q \u0026lt; 0 || q \u0026gt; 1) stop(\u0026quot;q should be in [0,1]\u0026quot;) lohi \u0026lt;- quantile(x, c(q, 1-q), na.rm=TRUE) if(diff(lohi) \u0026lt; 0) lohi \u0026lt;- rev(lohi) x[!is.na(x) \u0026amp; x \u0026lt; lohi[1]] \u0026lt;- lohi[1] x[!is.na(x) \u0026amp; x \u0026gt; lohi[2]] \u0026lt;- lohi[2] x }  The winsorize function in my R/broman package hadn\u0026rsquo;t included any checks that the inputs were okay.\nThe simplest thing to do is to include some if statements with calls to stop or warning.\nThe input x is supposed to be a numeric vector, and q is supposed to be a single number between 0 and 1.\nCheck inputs winsorize \u0026lt;- function(x, q=0.006) { stopifnot(is.numeric(x)) stopifnot(is.numeric(q), length(q)==1, q\u0026gt;=0, q\u0026lt;=1) lohi \u0026lt;- quantile(x, c(q, 1-q), na.rm=TRUE) if(diff(lohi) \u0026lt; 0) lohi \u0026lt;- rev(lohi) x[!is.na(x) \u0026amp; x \u0026lt; lohi[1]] \u0026lt;- lohi[1] x[!is.na(x) \u0026amp; x \u0026gt; lohi[2]] \u0026lt;- lohi[2] x }  The stopifnot function makes this a bit easier.\n\\tt assertthat package #' import assertthat winsorize \u0026lt;- function(x, q=0.006) { if(all(is.na(x)) || is.null(x)) return(x) assert_that(is.numeric(x)) assert_that(is.number(q), q\u0026gt;=0, q\u0026lt;=1) lohi \u0026lt;- quantile(x, c(q, 1-q), na.rm=TRUE) if(diff(lohi) \u0026lt; 0) lohi \u0026lt;- rev(lohi) x[!is.na(x) \u0026amp; x \u0026lt; lohi[1]] \u0026lt;- lohi[1] x[!is.na(x) \u0026amp; x \u0026gt; lohi[2]] \u0026lt;- lohi[2] x }  Hadley Wickham\u0026rsquo;s assertthat package adds some functions that simplify some of this.\nHow is the assertthat package used in practice? Look at packages which depend on it, such as dplyr. Download the source for dplyr and try grep assert_that dplyr/R/* and you\u0026rsquo;ll see a bunch of examples if its use.\nAlso try grep stopifnot dplyr/R/* and you\u0026rsquo;ll see that both are being used.\n\\tt R CMD check is run, they also serve as crude tests that the package is working. But note that this is only testing for things that {\\nhilit halt the code\n; it\u0026rsquo;s not checking that the code is {\\nvhilit giving the right answers}.\nThings that you put in the tests directory can be more rigorous: these are basically regression tests. The R output (in the file some_test.Rout) will be compared to a saved version, if available.\nIf you want your package on CRAN, you\u0026rsquo;ll need all of these tests and examples to be {\\nhilit fast}, as CRAN runs R CMD check on every package every day on multiple systems.\nThe R CMD check system is another important reason for assembling R code as a package. }\nAn example example #' @examples #' x \u0026lt;- sample(c(1:10, rep(NA, 10), 21:30)) #' winsorize(x, 0.2)  This example doesn\u0026rsquo;t provide a proper {\\nhilit test. You\u0026rsquo;ll get a note if it gives an error, but you don\u0026rsquo;t get any indication about whether it\u0026rsquo;s giving the right answer. }\nA tests/ example library(qtl) # read data csv \u0026lt;- read.cross(\u0026quot;csv\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;listeria.csv\u0026quot;) # write write.cross(csv, \u0026quot;csv\u0026quot;, filestem=\u0026quot;junk\u0026quot;) # read back in csv2 \u0026lt;- read.cross(\u0026quot;csv\u0026quot;, \u0026quot;\u0026quot;, \u0026quot;junk.csv\u0026quot;, genotypes=c(\u0026quot;AA\u0026quot;, \u0026quot;AB\u0026quot;, \u0026quot;BB\u0026quot;, \u0026quot;not BB\u0026quot;, \u0026quot;not AA\u0026quot;)) # check for a change comparecrosses(csv, csv2) unlink(\u0026quot;junk.csv\u0026quot;)  An advantage of the tests/ subdirectory is that you can more easily test input/output.\nA useful technique here: if you have a pair of functions that are the inverse of each other (e.g., write and read), check that if you apply one and then the other, you get back to the original.\nNote that unlink(\u0026quot;junk.csv\u0026quot;) deletes the file.\n\\tt testthat package  Expectations  [] expect_equal(10, 10 + 1e-7) [] expect_identical(10, 10) [] expect_equivalent(c(\u0026quot;one\u0026quot;=1), 1) [] expect_warning(log(-1)) [] expect_error(1 + \u0026quot;a\u0026quot;) Tests [] test_that(\u0026quot;winsorize small vectors\u0026quot;, \\{ ... \\)} Contexts [] context(\u0026quot;Group of related tests\u0026quot;) Store tests in tests/testthat  tests/testthat.R file containing  [] library(testthat) \\\\ test_check(\u0026quot;**mypkg**\u0026quot;)   The testthat package simplifies unit testing of code in R packages.\nThere are a bunch of functions for defining \u0026ldquo;expectations.\u0026rdquo; Basically, for testing whether something worked as expected. (It can be good to check that something gives an error when it\u0026rsquo;s supposed to give an error.)\nYou then define a set of tests, with a character string to explain where the problem is, if there is a problem.\nYou can group tests into \u0026ldquo;contexts.\u0026rdquo; When the tests run, that character string will be printed, so you can see what part of the code is being tested.\nPut your tests in .R files within tests/testthat. Put another file within tests/ that will ensure that these tests are run when you do R CMD check.\nExample testthat test context(\u0026quot;winsorize\u0026quot;) test_that(\u0026quot;winsorize works for small vectors\u0026quot;, { x \u0026lt;- c(2, 3, 7, 9, 6, NA, 5, 8, NA, 0, 4, 1, 10) result1 \u0026lt;- c(2, 3, 7, 9, 6, NA, 5, 8, NA, 1, 4, 1, 9) result2 \u0026lt;- c(2, 3, 7, 8, 6, NA, 5, 8, NA, 2, 4, 2, 8) expect_identical(winsorize(x, 0.1), result1) expect_identical(winsorize(x, 0.2), result2) })  These are the sort of tests you might do with the testthat package. The value of this: finally, we are checking whether the code is giving the right answer!\nIdeally, you include tests like this for every function you write.\nIt\u0026rsquo;s not really clear that the second test here is needed. If the first test is successful, what\u0026rsquo;s the chance that the second will fail?\nExample testthat test test_that(\u0026quot;winsorize works for a long vector\u0026quot;, { set.seed(94745689) n \u0026lt;- 1000 nmis \u0026lt;- 10 p \u0026lt;- 0.05 input \u0026lt;- rnorm(n) input[sample(1:n, nmis)] \u0026lt;- NA quL \u0026lt;- quantile(input, p, na.rm=TRUE) quH \u0026lt;- quantile(input, 1-p, na.rm=TRUE) result \u0026lt;- winsorize(input, p) middle \u0026lt;- !is.na(input) \u0026amp; input \u0026gt;= quL \u0026amp; input \u0026lt;= quH low \u0026lt;- !is.na(input) \u0026amp; input \u0026lt;= quL high \u0026lt;- !is.na(input) \u0026amp; input \u0026gt;= quH expect_identical(is.na(input), is.na(result)) expect_identical(input[middle], result[middle]) expect_true( all(result[low] == quL) ) expect_true( all(result[high] == quH) ) })  Here\u0026rsquo;s a bigger, more interesting test.\nThe code to test a function will generally be longer than the function itself.\nWorkflow  Write tests as you\u0026rsquo;re coding. Run test()  [] with devtools, and working in your package directory Consider auto_test(\u0026quot;R\u0026quot;, \u0026quot;tests\u0026quot;) [] automatically runs tests when any file changes Periodically run R CMD check [] also R CMD check --as-cran   Read Hadley\u0026rsquo;s paper about testthat. It\u0026rsquo;s pretty easy to incorporate testing into your development workflow.\nIt\u0026rsquo;s really important to write the tests {\\nhilit as you\u0026rsquo;re coding. You\u0026rsquo;re will check to see if the code works; save the test code as a formal test. }\nWhat to test?  You can\u0026rsquo;t test everything. Focus on the boundaries  (Depends on the nature of the problem) Vectors of length 0 or 1 Things exactly matching Things with no matches Test handling of missing data. [] NA, Inf, -Inf Automate the construction of test cases Create a table of inputs and expected outputs Run through the values in the table    You want your code to produce the correct output for {\\nhilit any input, but you can\u0026rsquo;t test {\\nhilit all} possible inputs, and you don\u0026rsquo;t really need to.\nThis is an experimental design question: what is the minimal set of inputs to verify that the code is correct, with little uncertainty?\nWe generally focus on boundary cases, as those tend to be the places where things go wrong.\nThe mishandling of different kinds of missing data is also a common source of problems and so deserving of special tests. }\nAnother example test_that(\u0026quot;running mean with constant x or position\u0026quot;, { n \u0026lt;- 100 x \u0026lt;- rnorm(n) pos \u0026lt;- rep(0, n) expect_equal( runningmean(pos, x, window=1), rep(mean(x), n) ) expect_equal( runningmean(pos, x, window=1, what=\u0026quot;median\u0026quot;), rep(median(x), n) ) expect_equal( runningmean(pos, x, window=1, what=\u0026quot;sd\u0026quot;), rep(sd(x), n) ) x \u0026lt;- rep(0, n) pos \u0026lt;- runif(n, 0, 5) expect_equal( runningmean(pos, x, window=1), x) expect_equal( runningmean(pos, x, window=1, what=\u0026quot;median\u0026quot;), x) expect_equal( runningmean(pos, x, window=5, what=\u0026quot;sd\u0026quot;), rep(0, n)) })  Here\u0026rsquo;s another example of unit tests, for a function calculating a running mean.\nWriting these tests revealed a bug in the code: with constant $x$\u0026rsquo;s, the code should give SD = 0, but it was giving {\\tt NaN\u0026rsquo;s due to round-off error that led to $\\sqrt{\\epsilon}$ for $\\epsilon \u0026lt; 0$.\nThis situation {\\nhilit can} come up in practice, and this is exactly the sort of boundary case where problems tend to arise. }\nDebugging tools  cat, print traceback, browser, debug RStudio breakpoints Eclipse/StatET gdb  I\u0026rsquo;m going to say just a little bit about debugging.\nI still tend to just insert cat or print statements to isolate a problem.\nR does include a number of debugging tools, and RStudio has made these even easier to use.\nEclipse/StatET is another development environment for R; it seems hard to set up.\nThe GNU project debugger (gdb) is useful for compiled code.\nDebugging \\centerline{Step 1: Reproduce the problem}\n\\onslide{\\centerline{Step 2: Turn it into a test}}\nTry to create the minimal example the produces the problem. This helps both for refining your understanding of the problem and for speed in testing.\nOnce you\u0026rsquo;ve created a minimal example that produces the problem, {\\nhilit add that to your battery of automated tests! The problem may suggest related tests to also add. }\nDebugging \\centerline{Isolate the problem: where do things go bad?}\nThe most common strategy I use in debugging involves a sort of divide-and-conquer: if R is crashing, figure out exactly where. If data are getting corrupted, step back to find out where it\u0026rsquo;s gone from good to bad.\nDebugging \\centerline{Don\u0026rsquo;t make the same mistake twice.}\nIf you figure out some mistake you\u0026rsquo;ve made, search for all other possible instances of that mistake.\nThe most pernicious bugs \\centerline{The code is right, but your thinking is wrong.}\n\\onslide{\\centerline{You were mistaken about what the code would do.}}\n\\onslide{\\centerline-\u0026gt; Write trivial programs to test your understanding.}\nA number of times I\u0026rsquo;ve combed through code looking for the problem, but there really wasn\u0026rsquo;t a problem in the code: the problem was just that my understanding of the algorithm was wrong.\nOr I was mistaken about some basic aspect of R.\nIt can be useful to write small tests, to check your understanding.\nFor an EM algorithm, always evaluate the likelihood function. It should be non-decreasing.\nSummary  If you don\u0026rsquo;t test your code, how do you know it works? If you test your code, save and automate those tests. Check the input to each function. Write unit tests for each function. Write some larger regression tests. Turn bugs into tests.  Testing isn\u0026rsquo;t fun. Like much of this course: somewhat painful initial investment, with great potential pay-off in the long run.\n"
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/big-jobs/",
	"title": "Big Jobs",
	"tags": [],
	"description": "",
	"content": " Reproducibility can be considerably harder for computational tasks that take more than just a couple of hours, and in some cases, the computations for a project may require years of CPU time (split across many computers).\nThe problems are: (a) it\u0026rsquo;s hard for someone to re-do all of that work, and (b) large-scale calculations tend to be organized in a system-dependent way, so even if time weren\u0026rsquo;t a factor, it\u0026rsquo;d be harder to transfer the calculations to another system.\nSimulations have some special issues (e.g., saving the seeds for random number generators), and they are notoriously irreproducible.\nWe at least want to fully document the process: we want capture the exact code and workflow, so the results can be reproduced on the same system. And we want that code to be modular and readable, so that it {\\nhilit could be restructured for a different system, if necessary.\nI must admit that I don\u0026rsquo;t always do this well. Part of this lecture is more of a sketch of what I think one should do rather than what I actually do.\nBut first\u0026hellip; Suppose I\u0026rsquo;ve just written an R function and it seems to work, and suppose I noticed a simple way to speed it up.\nWhat should I do first?  Make it an R package Write a test or two Commit it to a git repository  My aim here is to reinforce the things we\u0026rsquo;ve been covering in the course.\nEverything will be a lot easier if you put the code into an R package. For the minimal package, all you need are the {\\tt DESCRIPTION and NAMESPACE files.\nAnd before you start editing the code, you should write a small test. Then you\u0026rsquo;ll have evidence that it currently works, and the tests will help show that it\u0026rsquo;s still working after your modifications.\nAnd before you start editing the code, {\\nvhilit git commit} what you have so far! If it turns out that your new idea doesn\u0026rsquo;t work, will you be able to get back to what you had originally?\nSo what\u0026rsquo;s the big deal?  You don\u0026rsquo;t want knitr running for a year.\n You don\u0026rsquo;t want to re-run things if you don\u0026rsquo;t have to.\n  It may not seem like \u0026ldquo;big jobs\u0026rdquo; are that big of a deal, but in my mind this is the only real difficulty in reproducible research.\nOkay, there\u0026rsquo;s also the difficulty that some data can\u0026rsquo;t be generally distributed due to confidentiality issues.\nBut aside from subjects\u0026rsquo; confidentiality, the main problem is how to manage and capture the really long-running computational analyses where even on the same system it can be a gargantuan effort to reproduce the results.\nUnix basics  [] nice +19 R CMD BATCH input.R output.txt \\\u0026amp;\n [] fg\n [] ctrl-Z\n [] bg\n [] ps ux\n [] top\n [] kill\n [] kill -9\n [] pkill\n  Use R CMD BATCH to run an R job in the background.\nUse \\\u0026amp; to put it in the background.\nUse nice +19 to give it low priority.\nUse fg to bring a job back into the foreground.\nUse ctrl-Z to suspend a current job; then use bg to put it in the background.\nUse ps ux or top to view current jobs.\nUse kill or kill -9 with a process ID (PID in the output of ps and top) to kill a job. Use pkill to kill multiple jobs at once, using a pattern.\nNote: In my experience, Windows sucks at managing multiple processes. Windows XP was not bad at this, but Windows XP is dead.\nDisk thrashing In computer science, thrashing occurs when a computer\u0026rsquo;s virtual memory subsystem is in a constant state of paging\\onslide{, rapidly exchanging data in memory for data on disk, to the exclusion of most application-level processing.}\n{\\textendash} Wikipedia\nA common problem is having multiple jobs on a machine attempt to use more than the available memory on the machine, so then the machine starts swapping data from RAM to disk, and all the jobs slow to a crawl.\nIf a machine starts disk thrashing, it can be hard to log on and kill the jobs.\nThe solution: anticipate (and then watch) memory usage.\nAnother thing I did: miscalculated the number of files to be produced by a job, by a couple of orders of magnitude. It turns out that if you go beyond some limit on the number of files in a directory, you can totally kill a storage system.\nBiggish jobs in knitr  Manual caching Built-in cache=TRUE Split the work and write a Makefile  You can put big computations within an R Markdown file, but {\\nhilit personally I don\u0026rsquo;t want to wait more than a couple of minutes for it to compile. If it\u0026rsquo;s going to take longer than that, I\u0026rsquo;ll split things up.\nAnd if you are going to have some large computations with knitr, you won\u0026rsquo;t want to re-run {\\nhilit all} of them every time you make even the smallest change to the text!\nThat\u0026rsquo;s where you want to {\\nhilit cache} some computations: save the results and just load the results rather than re-run the code. Unless the {\\nhilit code} changes, in which case you {\\nhilit do} want to re-run it (and any other code that may depend on the results). }\nManual caching \u0026quot;`{r a_code_chunk} file \u0026lt;- \u0026quot;cache/myfile.RData\u0026quot; if(file.exists(file)) { load(file) } else{ .... save(object1, object2, object3, file=file) } \u0026quot;`  This is the \u0026ldquo;by hand\u0026rdquo; approach. If the file doesn\u0026rsquo;t exist, run the relevant code and save the needed results to the file. If the file does exist, just load the file and skip the code.\nIf you want (or need) to re-run the code, you need to delete the file {\\nhilit manually.\nOne issue: if you want the code to actually be {\\nhilit shown}, you need to repeat the code: in a chunk that is shown but isn\u0026rsquo;t run, and then in this chunk that is run but isn\u0026rsquo;t shown.\nYou need to be very careful about dependencies. }\nChunk references \u0026quot;`{r not_shown, eval=FALSE} code_here \u0026lt;- 0 \u0026quot;` \u0026quot;`{r a_code_chunk, echo=FALSE} file \u0026lt;- \u0026quot;cache/myfile.RData\u0026quot; if(file.exists(file)) { load(file) } else{ \u0026lt;\u0026lt;not_shown\u0026gt;\u0026gt; save(code_here, file=file) } \u0026quot;`  Here\u0026rsquo;s how I\u0026rsquo;d avoid repeated code: use chunk references.\nThe \u0026lt;\u0026lt;not_shown\u0026gt;\u0026gt; is replaced by the code from that chunk with that label.\nA cache gone bad \\figh{Figs/cache_gone_bad.png}{0.65}\nThis is the sort of thing that can happen with manual caching.\nThis is Fig.\\ 11.14 from my book, A guide to QTL mapping with R/qtl.\nI saw it immediately upon flipping through my first paper copy of the printed book.\nI\u0026rsquo;d cached some results, but then changed the underlying software in a fundamental way and didn\u0026rsquo;t update the cache.\nKnitr\u0026rsquo;s cache system \u0026quot;`{r chunk_name, cache=TRUE} load(\u0026quot;a_big_file.RData\u0026quot;) med \u0026lt;- apply(object, 2, median, na.rm=TRUE) \u0026quot;`   Chunk is re-run if edited. Otherwise, objects from previous run are loaded. Don\u0026rsquo;t cache things with side effects  [] e.g., options(), par()    Knitr has a nice built-in system for caching.\nA chunk with cache=TRUE will be run once and then all objects saved. In future runs, the code won\u0026rsquo;t be run, but rather the cached objects will be loaded.\nBut some things {\\nvhilit shouldn\u0026rsquo;t be cached. \u0026ldquo;Side effects\u0026rdquo; change the state of things; mostly, this is changing global variables. If these are placed in a cached chunk, the side effects won\u0026rsquo;t be captured when the cache is loaded. }\nCache dependencies Manual dependencies\n\u0026quot;`{r chunkA, cache=TRUE} Sys.sleep(2) x \u0026lt;- 5 \u0026quot;` \u0026quot;`{r chunkB, cache=TRUE, dependson=\u0026quot;chunkA\u0026quot;} Sys.sleep(2) y \u0026lt;- x + 1 \u0026quot;` \u0026quot;`{r chunkC, cache=TRUE, dependson=\u0026quot;chunkB\u0026quot;} Sys.sleep(2) z \u0026lt;- y + 1 \u0026quot;`  You can indicate dependencies among chunks. Here, if chunkA is re-run, the other two will be as well.\nCache dependencies Automatic dependencies\n\u0026quot;`{r setup, include=FALSE} opts_chunk$set(autodep = TRUE) dep_auto() \u0026quot;`  There\u0026rsquo;s also an automatic system for determining dependencies among chunks.\nI\u0026rsquo;ve not used it.\nParallel computing If your computer has multiple processors, use library(parallel) to make use of them.\n detectCores() RNGkind(\u0026quot;L'Ecuyer-CMRG\u0026quot;) and mclapply (Unix/Mac) makeCluster, clustersetRNGStream, {\\tt clusterApply}, and stopCluster (Windows)  R has a built-in package for performing parallel computations. A number of instances of R are invoked, calculations begun, and then the results brought back together.\nThe code can be a bit ugly, but it\u0026rsquo;s not so bad once you get used to it.\nSee the links on the resources page, http://kbroman.github.io/Tools4RR/pages/resources.html\nSystems for distributed computing  HTCondor and the UW-Madison CHTC Other condor-like systems \u0026ldquo;By hand\u0026rdquo;  e.g., perl script + template R script    For really big jobs, you\u0026rsquo;ll want to distribute the computations across multiple computers.\nAt UW-Madison, the main place to look is the Center for High Throughput Computing (CHTC), and the HTCondor software. This provides a way of distributing enormous numbers of jobs across a heterogeneous set of computers and collecting the results. The CHTC provides great user support.\nThere exists other, similar systems for distributing and managing jobs across clusters of computers. But at Madison, everyone uses HTCondor.\nMy own approach is more primitive: I have a Perl script that converts a template R script into a bunch of R input files (by replacing every instance of \u0026ldquo;SUB\u0026rdquo; in the template with a job-specific index). It also creates a script to set those running.\nIn either case, you\u0026rsquo;d write another R script to combine the results from the multiple jobs.\nSimulations  Computer simulations require RNG seeds (.Random.seed in R). Multiple parallel jobs need different seeds. Don\u0026rsquo;t rely on the current seed, or on having it generated from the clock. Use something like \\tt set.seed(91820205 + i) An alternative is create a big batch of simulated data sets in advance.  RNG = Random number generator\nSimulations split across multiple CPUs each need their own seed. In R, the seed is saved as .Random.seed; if you start all of the simulations from the same directory, they could all get exactly the same seed.\nI tend to include a call to set.seed at the top of each R script, with the seed being some big number plus an index for the job.\nYou could, alternatively, generate all of the simulated data sets in advance. An advantage of this is that it\u0026rsquo;d be easier to reproduce the results later. Just be sure to save (and document) the code you used to generate the data.\nSave everything  RNG seeds input output version numbers, with sessionInfo() raw results script to combine results combined results ReadMe describing the point  This stuff (particularly code input \u0026amp; text output) doesn\u0026rsquo;t take up much space. Compartmentalize it and save it.\nOne Makefile to rule them all  Separate directory for each batch of big computations. Makefile that controls the combination of the results (and everything else). KnitR-based documents for the analysis/use of those results.  This is what I\u0026rsquo;m thinking, for projects that involve big computations: compartmentalize those big computations into chunks, each in a separate directory to contain all of the materials and results.\nHave one Makefile that handles the combination of those results as well as the compilation of any KnitR-based files that describe and carry out the further analyses.\nThe Makefile won\u0026rsquo;t capture the entire workflow, but it will indicate almost all of it, and the big jobs will be compartmentalized as subdirectories, and the source of the major results will be indicated in the Makefile.\nPotential problems  Forgetting save() in your distributed jobs A bug in the save() command make clobbers some important results  Scripts should refuse to overwrite output files    These are common mistakes I make.\nI forget the save command and so run a ton of computations and then get no results.\nOr my save command has an error and so I run a ton of computations and then it dies on the last line of the script.\nOr I run make and it starts re-running some analysis that I don\u0026rsquo;t want it to re-run, and it clobbers some important result and then I {\\nhilit have to re-run it. }\nSummary  Careful organization and modularization. Save everything. Document everything. Learn the basic skills for distributed computing.  It\u0026rsquo;s important to always end with a summary.\nResearch with long-running computations are hard to make fully reproducible. Modularize the big jobs and document their purpose. And document the relationships among things.\n"
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/python/",
	"title": "Python",
	"tags": [],
	"description": "",
	"content": " I\u0026rsquo;m a big proponent of the use of multiple programming languages: use different languages for different types of tasks.\nStatisticians, in particular, should be proficient in some \u0026ldquo;scripting language\u0026rdquo; (e.g., Perl, Python, or Ruby). These types of languages give you far more flexibility for manipulating data files.\nI\u0026rsquo;ve long used Perl, but I\u0026rsquo;ve switched to Ruby, and I\u0026rsquo;m trying to also be proficient in Python. I prefer Ruby to Python, but Python is much more widely used, and so if you\u0026rsquo;re going to just learn one such language, learn Python.\nWhy python?  Manipulating data files Simulations using others\u0026rsquo; programs Web-related stuff} Alternative to R for data analysis and graphics Jupyter notebooks  For statisticians, the most important use of Python is for the manipulation of data files. This sort of script language is great for manipulating text, and data files are mostly plain text files.\nIn addition, I find a scripting language critical for performing simulations to evaluate others\u0026rsquo; command-line-based programs. They\u0026rsquo;re also good for web-related stuff.\nPython can also serve as an alternative to R for data analysis and graphics. And Jupyter notebooks are a big deal for reproducible research (and they can be used more broadly than Python).\nPython 2 vs Python 3  Many people are using Python version 2.7 Python 3 was introduced in {\\vhilit 2008}  A number of large changes Some important Python programs haven\u0026rsquo;t been ported Few people seem to be using it day-to-day Ideally, go straight for Python 3 But be aware of differences    The biggest annoyance about Python is the two competing versions, Python 2 and Python 3. For now, you should probably stick with Python 2.\nPython 3 is much better than Python 2, but it hasn\u0026rsquo;t penetrated the Python community sufficiently. But it\u0026rsquo;s definitely getting better.\nI\u0026rsquo;m still a bit torn about whether to recommend Python 2 or Python 3 when you\u0026rsquo;re starting out, but at this point it probably is better to go straight for Python 3, but be aware of the differences as they may bite you now and then.\nInstalling Python  On Mac or Unix, Python 2 should be pre-installed  [] python --version For Windows (or to be current, or to alternate between Python 2 and 3), install Anaconda [] Includes NumPy, SciPy, Pandas, iPython, Matplotlib, \\dots [] \\tt continuum.io/downloads    When you\u0026rsquo;re just starting to learn, you can just stick with the pre-installed version of Python, if you are on some flavor of Unix.\nLong term, I recommend Anaconda, which is an easy-to-install Python with basically all of the scientific packages you\u0026rsquo;d want. Installing these by hand seems really painful; installing Anaconda is easy.\nAlso, with Anaconda, it\u0026rsquo;s easy to switch between Python 2 and Python 3.\nLearning a new language  Find a good book Have good example tasks/problems Play around Force yourself to use the new language Develop a script illustrating different language features  It takes time to learn a new programming language. The only way you\u0026rsquo;ll learn it is by forcing yourself to use it regularly. You need good, realistic problems to tackle. And it might take you just 30 minutes with the language you know but all afternoon in the new language. But if you don\u0026rsquo;t force yourself, you\u0026rsquo;ll never learn.\nIf you go away from it for a week, you\u0026rsquo;ll be quite rusty when you come back. I\u0026rsquo;ve found it useful to develop a script that illustrates the various language features. (\u0026ldquo;How do I write a loop again? How do I define a function?\u0026ldquo;) Looking through that, you\u0026rsquo;ll pick it all up again quickly. It\u0026rsquo;s harder to look back through a book in the same way.\nInto the thick of it Learn Python through one example\n$\\qquad$ markers.txt\n$\\qquad$ families.txt $\\qquad \\longrightarrow \\qquad$ data.gen\n$\\qquad$ genotypes.txt\nI can\u0026rsquo;t really hope to teach you Python in 50 minutes, but I\u0026rsquo;ll try. In this crash course, I\u0026rsquo;ll go through a medium-sized script to combine a few data files and convert them into a different form.\nmarkers.txt contains a list of ordered genetic markers. {\\tt families.txt contains information about subjects\u0026rsquo; familial relationships. genotypes.txt contains subjects\u0026rsquo; genotypes.\nWe\u0026rsquo;re going to convert these data into the form used by the CRI-MAP program (an old program for constructing genetic maps). }\nInput: \\tt markers.txt D20S103 D20S482 D20S851 D20S604 D20S1143 D20S470 D20S477 D20S478 D20S481 D20S159 D20S480 D20S451 D20S171 D20S164  This is the markers.txt file. It just has one marker name per line.\nInput: \\tt families.txt Family Individual Father Mother Sex 1 1 0 0 1 1 2 0 0 2 1 3 1 2 1 1 4 1 2 2 1 5 1 2 2 2 1 0 0 1 2 2 0 0 2 2 3 1 2 1 2 4 1 2 1 3 1 0 0 1 3 2 0 0 2 3 3 1 2 2 3 4 1 2 1 3 5 1 2 1 3 6 1 2 2 ... 5 6 1 2 2 5 7 1 2 1  This is the families.txt file; each line is one subject. In the Father and Mother columns, 0 indicates missing: a founding individual in that family. In the Sex column, 2 = female and 1 = male.\nInput: \\tt genotypes.txt Marker 1-1 1-2 1-3 1-4 1-5 2-1 2-2 ... D20S103 100/98 98/98 98/98 98/98 100/100100/96 ... D20S1143 176/172180/176176/180 172/180172/176172/172 ... D20S159 350/358366/354350/354350/354358/366354/350366/354 ... D20S164 191/207207/207215/191215/207191/207207/215 ... D20S171 141/135141/137141/141141/137135/137141/139143/135 ... D20S451 324/308320/316324/316308/320 308/324312/316 ... D20S470 306/302302/306302/306306/302302/302302/294310/266 ... D20S477 256/252260/252252/252 256/252256/252 ... D20S478 267/263263/263263/263263/267255/271263/247 ... D20S480 304/284 304/284304/284296/296300/300 ... D20S481 229/237241/237237/237229/237237/237245/245 ... D20S482 155/159159/167159/159155/167159/167147/155159/155 ... D20S604 151/147 147/135151/143151/143 147/143 ... D20S851 132/140148/144132/144132/148132/148 144/140 ...  The genotypes.txt file is a bit ugly. Rows are markers and individuals are in fixed-width columns, with the genotypes being two numeric alleles separated by a slash. Blank fields correspond to missing data.\nOutput: \\tt data.gen 5 14 D20S103 D20S482 ... D20S171 D20S164 1 5 1 0 0 1 0 0 155 159 132 140 151 147 176 172 306 302 256 252 0 0 ... 2 0 0 0 100 98 159 167 148 144 0 0 180 176 302 306 260 252 267 ... 3 2 1 1 98 98 159 159 132 144 147 135 176 180 302 306 252 252 ... 4 2 1 0 98 98 155 167 132 148 151 143 0 0 306 302 0 0 263 263 ... 5 2 1 0 98 98 159 167 132 148 151 143 172 180 302 302 256 252 ... 2 4 ...  The file we\u0026rsquo;re converting to, data.gen in the format used by CRI-MAP, is a bit weird: Number of families, number of markers, the marker names in order, and then for each family, the family ID, the number of subjects in that family, and then the subjects. For each subject, there\u0026rsquo;s a line with individual, mom, dad, and sex (0 = female, 1 = male), and then a line with genotype data, with two numbers for each marker, with 0\u0026rsquo;s for missing values.\nThe top of the Python script #!/usr/bin/env python # Combine the data in \u0026quot;genotypes.txt\u0026quot;, \u0026quot;markers.txt\u0026quot; and # \u0026quot;families.txt\u0026quot; and convert them into a CRI-MAP .gen file # # This is the python 2 version def read_markers (filename): \u0026quot;Read an ordered list of marker names from a file.\u0026quot; with open(filename, 'r') as f: lines = f.readlines() return [line.strip() for line in lines] class Person: \u0026quot;Person class, to contain the data on a subject.\u0026quot; def __init__ (self,family, id, dad, mom, sex): self.family = family self.id = id self.dad = dad self.mom = mom self.sex = \u0026quot;0\u0026quot; if sex == \u0026quot;2\u0026quot; else sex # convert 1/2 -\u0026gt; 1/0 self.famid = family + '-' + id self.gen = {}  The first line (\\#!/usr/bin/env python) makes it so you can run this script from the command line by just typing its name. Using /usr/bin/env allows that Python might be located in a different place on different systems.\nTo make the script executable (on unix), type chmod +x convert2.py\nIn python, comments begin with \\# (as in R).\nInstead of using braces to delineate blocks of code, Python uses indentation. I was initially turned off by this, but I\u0026rsquo;ve been converted to the idea (mostly from having written a lot of CoffeeScript code). You\u0026rsquo;re going to indent anyway; why not have that indentation be meaningful?\nYou define functions with def name (param):\nUnlike R, functions must have a return statement if you want to return a value.\nThe bottom of the Python script if __name__ == '__main__': # file names gfile = \u0026quot;genotypes.txt\u0026quot; # genotype data mfile = \u0026quot;markers.txt\u0026quot; # list of markers, in order ffile = \u0026quot;families.txt\u0026quot; # family information ofile = \u0026quot;data.gen\u0026quot; # output file # read the data markers = read_markers(mfile) people = read_families(ffile) read_genotypes(gfile, people) # write the data write_genfile(ofile, people, markers)  The convert2.py script is just a bunch of function definitions (and one class).\nThis bit at the bottom is executed only if the script is run from the command line. It does all of the real work: read in the data and then write it back out as a .gen file.\nWrite functions \u0026amp; modules not scripts  Write a set of reusable functions Your code will be easier to read You may actually reuse the code, this way  With Python (and R), there\u0026rsquo;s a tendency to write a long mess of a script. It\u0026rsquo;s better to focus on writing a set of reusable functions.\nWith the given example script, you can use import convert2 to load the functions into python. This is similar to library() in R.\nTry it out $ convert2.py $ diff data.gen data_save.gen  $ python # (or ipython) \u0026gt;\u0026gt;\u0026gt; import convert2 \u0026gt;\u0026gt;\u0026gt; help(convert2) \u0026gt;\u0026gt;\u0026gt; help(convert2.read_markers) \u0026gt;\u0026gt;\u0026gt; markers = convert2.read_markers(\u0026quot;markers.txt\u0026quot;) \u0026gt;\u0026gt;\u0026gt; markers[0] \u0026gt;\u0026gt;\u0026gt; len(markers) \u0026gt;\u0026gt;\u0026gt; markers[-1] \u0026gt;\u0026gt;\u0026gt; markers[0:2] \u0026gt;\u0026gt;\u0026gt; markers[0:-1] \u0026gt;\u0026gt;\u0026gt; markers[5:] \u0026gt;\u0026gt;\u0026gt; markers[:5] \u0026gt;\u0026gt;\u0026gt; markers[0:7:2] \u0026gt;\u0026gt;\u0026gt; quit()  If you type convert2.py from the command line, it will run the script and create the data.gen file, which you\u0026rsquo;ll see (with diff) matches the target data_save.gen file.\nOr you can type python (or ipython) at the command line and then import the module and run some of the functions by hand. You\u0026rsquo;ll need to refer to the functions with the names preceded by convert2., or you can use from convert2 import * and then skip the convert2. part.\nThe read.markers function reads in the ordered list of markers as a vector. (In Python, they call it a list.) Vectors in Python are indexed starting at 0. You can use the len function (like {\\tt length() in R) to get the length.\nYou can grab slices with :, but note that they {\\nhilit don\u0026rsquo;t} include the last element in the range.\nAnd negative values are from the end, with -1 being the {\\nhilit last} value.\nAlso, you can use startby. Remember that end is {\\nhilit not} included. }\nRead the marker names def read_markers (filename): \u0026quot;Read an ordered list of marker names from a file.\u0026quot; with open(filename, 'r') as f: lines = f.readlines() return [line.strip() for line in lines]  This is the function to read the ordered list of markers. It takes a single argument: the name of the file.\nThe first line (between the double-quotes) is a description that will be shown if you import the module and type help(convert2.read_markers). Strings in Python can be defined using single- or double-quotes, just like in R.\nThe with business looks a bit odd, but it ensures that the file will be closed if anything goes wrong. I could just as well have written lines=open(filename).readlines() (The 'r', for reading, is the default.) After that bit of code, lines contains a vector with one marker name per line.\nThe last line contains a \u0026ldquo;list comprehension.\u0026rdquo; It\u0026rsquo;s a sort of one-line for loop, which applies the strip() function to each element of the lines vector (removing any end-of-line character).\nread_markers and open are ordinary functions, much like those in R. readlines and strip are object-oriented \u0026ldquo;methods.\u0026rdquo; Think of them as functions where the first argument precedes the function name.\nlstlisting class Person: \u0026ldquo;Person class, to contain the data on a subject.\u0026rdquo; def init (self, family, id, dad, mom, sex): self.family = family self.id = id self.dad = dad self.mom = mom self.sex = \u0026ldquo;0\u0026rdquo; if sex == \u0026ldquo;2\u0026rdquo; else sex # convert 1\u0026frasl;2 -\u0026gt; 1\u0026frasl;0 self.famid = family + \u0026lsquo;-\u0026rsquo; + id self.gen = {}\n Example use:  ind = Person(\u0026ldquo;1\u0026rdquo;, \u0026ldquo;3\u0026rdquo;, \u0026ldquo;1\u0026rdquo;, \u0026ldquo;2\u0026rdquo;, \u0026ldquo;2\u0026rdquo;)\n I first define a class to contain the data for a single subject. It contains a function `__init__` for initializing an instance of the class (the data object for a single subject). Within that function, `self` refers to the newly defined instance of the class, and `self.family`, etc., are the way to refer to the elements of the class object. We create a new `Person` object by calling \\\\ `Person(family, id, dad, mom, sex)` ## lstlisting def read_families (filename): \u0026quot;Read family info and return a hash of people.\u0026quot; with open(filename, 'r') as file: file.readline() # header row people = {} for line in file: vals = line.strip().split() person = Person(vals[0],vals[1],vals[2],vals[3],vals[4]) people[person.famid] = person return people  This is the function to read the family information. I again use with open() as file: to open the file. If anything goes wrong, the file will be automatically \u0026ldquo;closed.\u0026rdquo;\nI use readline to read (but ignore) the header line.\npeople = \\{\\ initializes a \u0026ldquo;hash.\u0026rdquo; This is like an unordered vector that is indexed by strings rather than numeric indices. (In Python, it\u0026rsquo;s called a \u0026ldquo;dictionary.\u0026rdquo;) I\u0026rsquo;m going to create a hash of Person objects, indicated by strings like \u0026quot;1-2\u0026quot; for individual 2 in family 1.\nI use a for loop over lines in the file. For each line, I strip off any end-of-line character and then split it at the white space, into a vector.\nI first call Person to define the person object, as then person.famid is defined, and I want to use that as the \u0026ldquo;hash key.\u0026rdquo; }\nlstlisting def parse_genotype (string): \u0026ldquo;Clean up string -\u0026gt; genotype\u0026rdquo; string = string.replace(\u0026rsquo; \u0026lsquo;, \u0026ldquo;) string = \u0026ldquo;0/0\u0026rdquo; if string == \u0026ldquo;\u0026rdquo; else string return string.replace(\u0026lsquo;/\u0026rsquo;, \u0026lsquo; \u0026lsquo;)\ndef read_genotypes (filename, people): \u0026ldquo;Read genotype data, fill in genotypes within people hash\u0026rdquo; with open(filename, \u0026lsquo;r\u0026rsquo;) as file:\nheader = file.readline().strip().split() header = header[1:] # omit the first field, \u0026quot;Marker\u0026quot; for line in file: marker = line[:9].replace(' ', \u0026quot;) line = line[9:] for i in range(len(header)): person = header[i] start = i*7 people[person].gen[marker] = \\ parse_genotype(line[start:(start+7)])   The first function here cleans up a genotype string a bit. It strips off any white space, substitutes `0/0` in the case of a blank, and replaces the slash with a space. So a string like `\u0026quot;78/125 \u0026quot;` will be converted to `\u0026quot;78 125\u0026quot;`. The `replace` function for strings is for doing text substitutions: it replaces every instance of its first argument with its second argument. In the `read_genotypes` function, I grab all but the first element of the header line, which match what I'm using as the keys for my `people` hash. I then go through the rest of the file, one line at a time: I grab the marker name (getting rid of any spaces) and then go through the rest of the line, 7 characters at a time. `range(n)` returns the vector `[0,` `1,` \\dots{\\tt , n-1]. If you look back at the `Person` class, you'll see that I'd initialized `gen` as a hash (with `self.gen = \\{\\`}). I'm filling this in, indexed by marker names. The `read_genotypes` function doesn't return anything, because it modifies the input `person` object (as a \u0026quot;side effect\u0026quot;). Note the backslash in the second-to-last line; this allows me to split a long line into two. If I left it off, Python would give an error. } ## Some helper functions  def get_families (people): \u0026ldquo;Return a vector of distinct families\u0026rdquo; return set([people[key].family for key in people])\ndef get_family_members (people, family): \u0026ldquo;Return a vector of famids for subjects within a family.\u0026rdquo; return [key for key in people if people[key].family == family]\ndef writeln (file, line, end=\u0026rdquo;\\n\u0026rdquo;): \u0026ldquo;Write a single line to a file.\u0026rdquo; file.write(str(line) + end)\n Here are a few helper functions that I use in the last {\\tt write_genfile function. `get_families` returns a vector of {\\nhilit distinct} family IDs. I use a {\\nhilit list comprehension} again, which gives a vector with all of the family names. Then `set` turns this into a \u0026quot;set\u0026quot; of distinct values. It acts here sort of like {\\tt unique()} in R. `get_family_members` returns a vector of the famid codes for the family members in a given family. I'm using a list comprehension again, but with an additional `if` qualification. `writeln` is just a little wrapper for the `write` function, to write a string to a file. Note that in this function, the `end` argument has a default value, much like in R functions. } ## lstlisting def write_genfile (filename, people, markers): \u0026quot;Write genotype data to a file, in CRI-MAP format.\u0026quot; with open(filename, 'w') as file: families = sorted(get_families(people)) writeln(file, len(families)) writeln(file, len(markers)) for marker in markers: writeln(file, marker) for family in families: writeln(file, family) members = sorted(get_family_members(people, family), \\ key=lambda famid: int(people[famid].id)) writeln(file, len(members)) for famid in members: person = people[famid] writeln(file, \u0026quot;%s %s %s %s\u0026quot; % (person.id, \\ person.mom, person.dad, person.sex)) for marker in markers: writeln(file, person.gen[marker], \u0026quot; \u0026quot;) writeln(file, \u0026quot;\u0026quot;)  This is the function to write the CRI-MAP file. There are two interesting bits here.\nFirst, sorted() returns a sorted version of a vector. I use it twice, the second time with key=lambda famid: int(people[famid].id) which is an anonymous function for sorting Person objects by their individual IDs (numerically).\nThe second interesting bit is\n\u0026quot;\\%s \\%s \\%s \\%s\u0026quot; \\% (person.id, person.mom, person.dad, person.sex) which is like sprintf, in that I\u0026rsquo;m formatting a bunch of stuff as a string.\nThe bottom of the Python script if __name__ == '__main__': # file names gfile = \u0026quot;genotypes.txt\u0026quot; # genotype data mfile = \u0026quot;markers.txt\u0026quot; # list of markers, in order ffile = \u0026quot;families.txt\u0026quot; # family information ofile = \u0026quot;data.gen\u0026quot; # output file # read the data markers = read_markers(mfile) people = read_families(ffile) read_genotypes(gfile, people) # write the data write_genfile(ofile, people, markers)  We made it through the whole file; here\u0026rsquo;s the bit at the bottom again. This bit is run only if you\u0026rsquo;re executing the script from the command line.\nI define the file names, read in the data, and then write it back out in a different form.\nBasic types  float  [] x = 0.3 int [] m = 5 string [] s = \u0026quot;blah\u0026quot; bool [] x = True [] y = False None [] x = None complex [] x = 5+0j    These are the basic types. Python 2 also distinguishes between {\\tt int and long, while Python 3 has just int.\nNone is a null object; you can use it as NA. }\nConverting between types, and such n = 5 type(n) s = str(n) x = float(n) \u0026quot;%s %s %s\u0026quot; % (n, s, x) \u0026quot;%d %d %d\u0026quot; % (n, int(s), x) \u0026quot;%.2f %.2f %.2f\u0026quot; % (n, float(s), x) dir(s) dir(x) s = \u0026quot;blah\u0026quot; len(s) s[2:] s[:-1] for ch in s: print ch  It\u0026rsquo;s important to be able to convert between types, particularly for converting between strings and floats.\nThe \\% operator is particularly handy for formatted output, or just to convert things into strings. With \\%s, numbers will be converted to strings, but with \\%d and \\%f, strings will {\\nhilit not automatically be converted to numbers {\\textendash} you\u0026rsquo;ll get an error.\nThe type function returns the type of an object. The dir function returns a list of the methods\nYou can use len on strings, and you can subset them like a vector (aka list), and you can even loop over the characters in a string. }\nMulti-element types  list  [] x = [1, 2, 3, None, \u0026quot;blah\u0026quot;] [] y = [ [ 1, 2 ], [ 3, 4, 5 ], 6 ] dictionary [] h = \\{'x': 3, 'y': 5, 'name':\u0026quot;Karl\u0026quot;\\} tuple [] x = (1, [2,3]) set [] S = set([5, 3, 5, 1, 2, 1])    Lists are like lists in R: they\u0026rsquo;re ordered vectors whose elements can be basically anything.\nDictionaries are what I call hashes: an un-ordered list indexed by strings (called \u0026ldquo;keys\u0026rdquo;).\nTuples are like lists, but the contents can\u0026rsquo;t be changed. They\u0026rsquo;re useful as return values from a function.\nSets are lists with only unique values.\nmatrices as lists of lists x = [ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12] ] x[1][3]  The simplest way to handle a matrix is as a list of lists. They\u0026rsquo;d typically be stored by rows, as you\u0026rsquo;d then index the thing as mat[row][col].\nAlso see numpy (numpy.org), for formal matrices and matrix methods.\nfor loops vec = range(4) for x in vec: print (x+1)**2 import math for i in xrange(len(vec)): print math.log( vec[i] + 1 ) h = {'x':3, 'y':4, 'z':2} for k in h: print k, h[k] for k in sorted(h.keys()): print k, h[k] for k,v in h.iteritems(): print k, v for v in h.itervalues(): print v  for loops over lists successively take each possible value in the list. If you want the indices, you need to create a vector of indices with range. Or use xrange, which is avoids actually creating the vector, but rather creates the elements when they\u0026rsquo;re needed.\nxrange is in Python2 only; in Python 3, just use {\\tt range. The Python3 range is really the Python2 xrange. So actually in Python3, the first line needs to be vec = list(range(4)).\nfor loops over dictionaries (aka hashes) successively take each possible key. You can use iteriterms() to iterate over key-value pairs or itervalues() to iterate over just the values. Like xrange, these generate the vector of iteracted values as needed rather than in advance. There\u0026rsquo;s also a {\\tt iterkeys()} method, which is what is used as the default for {\\tt for} loops with dictionaries.\nNote that these loops with dictionaries will be in arbitrary order. If you want a particular order, you first need to create a sorted vector of keys.\nIn Python3, use .items() and .values() in place of .iteritems() and .itervalues(), respectively. }\nlist comprehensions vec = range(10) [v**2 for v in vec if v \u0026gt; 5] h = {'x':3, 'y':4, 'zz':2} [h[k]**2 for k in h] [h[k]**2 for k in h if len(k) == 1] [[k, v**3] for k,v in h.iteritems()] dict( [[k, v**3] for k,v in h.iteritems()] ) x = [k+1 for k in range(6)] y = [True, False, True, False, False, False] [x[i] for i in range(len(x)) if y[i]]  List comprehensions are really useful for transformations or subsetting.\nThe dict function will convert a list of key-value pairs into a dictionary.\nYou can get by without them. But they can provide compact but readable code.\nNote, again, that in Python3 you should use .items() in place of .iteritems().\nMore with strings x = \u0026quot;bread and jam\u0026quot; y = x.split(\u0026quot; \u0026quot;) z = \u0026quot; \u0026quot;.join(y) dir(x) help(x.index) x.endswith(\u0026quot;jam\u0026quot;) x.startswith(\u0026quot;bre\u0026quot;) x.count(\u0026quot;a\u0026quot;) x.find(\u0026quot;and\u0026quot;) x.find(\u0026quot;jelly\u0026quot;) x.index(\u0026quot;and\u0026quot;) x.index(\u0026quot;jelly\u0026quot;) x.replace(\u0026quot;jam\u0026quot;, \u0026quot;jelly\u0026quot;) x.capitalize() x.title() x.upper() x.upper().lower()  Python has also sorts of methods for doing things with strings.\nUse dir to get a list, eg dir(str), dir(\u0026quot;\u0026quot;), or dir(x) where x is a string.\nUse help to get a description of one of the methods, eg help(str.find), help(\u0026quot;\u0026quot;.find), or help(x.find).\nRegular expressions import re x = \u0026quot;Bread and Jam\u0026quot; re.findall(r'[A-Z]', x) re.split(r'[A-Z]', x) re.sub(r'[A-Z]', \u0026quot;, x) ph = \u0026quot;555-12-3456\u0026quot; re.findall(r'-', ph) re.findall(r'\\d+', ph) re.split(r'\\D', ph) re.sub(r'\\D', \u0026quot;, ph)  A big reason to use scripting languages is for regular expression facilities. But I find regular expressions cumbersome in Python relative to Perl or Ruby.\nThere is some messiness about backslashes, so it\u0026rsquo;s best with regular expressions to use \u0026ldquo;raw strings\u0026rdquo; by preceding the string with an r, as so: r'blah'.\nUnit tests: Nose # This is nosetest_convert2.py # # At command line, type \u0026quot;nosetests nosetest_convert2.py\u0026quot; from nose.tools import assert_equal from convert2 import * def test_parse_genotype(): assert_equal(parse_genotype(\u0026quot; \u0026quot;), \u0026quot;0 0\u0026quot;) assert_equal(parse_genotype(\u0026quot;100/98 \u0026quot;), \u0026quot;100 98\u0026quot;) assert_equal(parse_genotype(\u0026quot;90/96 \u0026quot;), \u0026quot;90 96\u0026quot;) assert_equal(parse_genotype(\u0026quot;90/ 96 \u0026quot;), \u0026quot;90 96\u0026quot;) assert_equal(parse_genotype(\u0026quot; 3 / 8 \u0026quot;), \u0026quot;3 8\u0026quot;)  Unit tests are important for ensuring the correctness of Python code, as with any other programming effort.\nNose is simple tool for making Python unit tests. The above example is a minimal use of the tool.\nAt the command line, with tests in the file {\\tt nosetest_convert2.py, you\u0026rsquo;d type nosetests nosetest_convert2.py\nThe nose.tools module contains a bunch of assertion functions. Here, I\u0026rsquo;m just using assert_equal. }\nUnit tests: unittest #!/usr/bin/env python # Test one of the functions in convert2.py # # on the command line, type \u0026quot;test_convert2.py\u0026quot; import unittest from convert2 import * class check_parse_genotype(unittest.TestCase): def test_parse_genotype(self): self.assertEqual(parse_genotype(\u0026quot; \u0026quot;), \u0026quot;0 0\u0026quot;) self.assertEqual(parse_genotype(\u0026quot;100/98 \u0026quot;), \u0026quot;100 98\u0026quot;) self.assertEqual(parse_genotype(\u0026quot;90/96 \u0026quot;), \u0026quot;90 96\u0026quot;) self.assertEqual(parse_genotype(\u0026quot;90/ 96 \u0026quot;), \u0026quot;90 96\u0026quot;) self.assertEqual(parse_genotype(\u0026quot; 3 / 8 \u0026quot;), \u0026quot;3 8\u0026quot;) if __name__ == '__main__': unittest.main()  Python also has a built-in unittest module, but its use requires a bit more gunk. I don\u0026rsquo;t totally understand all of this.\nLike Nose, there are a variety of different assertion functions that you can use.\nSummary  Learn a scripting language, like Python  Not just for manipulating data files, but worth the effort just for that. Force yourself to use it   Applied statisticians need to be savvy with data file manipulation.\nDon\u0026rsquo;t let your scientific collaborators do any copy-paste to move data around; any data file manipulation should be done with a computer program.\nIn the long run, knowing Python, you\u0026rsquo;ll be more self-sufficient and versatile.\nLab For a bit of in-class python coding, we\u0026rsquo;ll fill out a Python script to read in a comma-delimited file, and calculate, for each column:\n Number of non-missing values Mean Median SD  Here are the relevant files:\n data.csv \u0026mdash; the data file to consider stats.py \u0026mdash; the script to be filled in stats2.py \u0026mdash; partly-filled version: just fill in the functions to calculate the statistics test_stats.py \u0026mdash; unit tests for functions in stats.py  Also relevant:\n create_example_data.py \u0026mdash; the Python script I wrote to generate the data.csv file. solns_DONT_PEEK.py \u0026mdash; my solutions (don\u0026rsquo;t look at this until later)  "
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/licenses/",
	"title": "Licenses",
	"tags": [],
	"description": "",
	"content": " An often neglected aspect in discussions of reproducible research: software and data need to be licensed. If you want your software and data to be reused, you need to provide an explicit license that explains exactly how the software and data may be reused.\nI\u0026rsquo;m going to try to explain the issues and give suggestions about licenses to consider. But I\u0026rsquo;m no expert, and I\u0026rsquo;m definitely {\\nhilit not a lawyer. I don\u0026rsquo;t guarantee that this is entirely correct.\nIf you will be sharing data on human subjects, or, for that matter, just working with data on human subjects, you need to be extra careful. I\u0026rsquo;ll try to sketch the basic concerns.\nI\u0026rsquo;m talking just about the United States here. I understand US law a bit, but I don\u0026rsquo;t know the law in other countries much at all.\nCourse summary  Make everything you do script-based  code + data -\u0026gt; product Use version control (git and GitHub/Bitbucket)  Take your time; organize Write clear code; write functions; make R packages Write unit tests Capture exploratory data analysis  what you did, saw, and thought (and why) knitr + Markdown for reports  knitr + \\LaTeX\\ for papers and talks and posters Use licenses to make reusability clear\nKarl -- this is very interesting, however you used an old version of the data (n=143 rather than n=226). I'm really sorry you did all that work on the incomplete dataset. Bruce   This is an edited version of an email I got from a collaborator, in response to an analysis report that I had sent him.\nI try to always include some brief data summaries at the start of such reports. By doing so, he immediately saw that I had an old version of the data.\nBecause I\u0026rsquo;d set things up carefully, I could just substitute in the newer dataset, type \u0026ldquo;make\u0026rdquo;, and get the revised report.\nThis is a reproducibility success story. But it took me a long time to get to this point.\nIntellectual property \\addtocounter{framenumber}{-1}\n Manuscripts/journal articles Books Software Data sets Ideas, inventions Lab/research notebooks Instructional materials Web sites  Intellectual property is property (ie, someone can own it) that is not an actual thing but more the idea of the thing. For example, it\u0026rsquo;s not the actual physical book, but the text in the book. It\u0026rsquo;s not an actual physical art work, but any depiction of the content of that artwork. This can get pretty complicated; it\u0026rsquo;s best to move on.\nMost of what academics produce is intellectual property.\nIP protection  Copyright Patents Trademarks, Trade \u0026ldquo;dress\u0026rdquo; Trade secrets  Different kinds of intellectual property are protected in different ways. I\u0026rsquo;m going to focus on copyright.\nAn important point to mention here is that an {\\nhilit idea, {\\nhilit fact} or {\\nhilit algorithm} can\u0026rsquo;t be copyrighted. Ideas and algorithms can be protected with a patent, but {\\nhilit facts} (including individual data points) can be neither copyrighted nor patented.\nSo, for example, copyright protection says that you may not be able to copy and use someone\u0026rsquo;s exact code, but you can grab all of the ideas and re-implement them in your own way. Unless the ideas or algorithms are patented, and then you have to get their permission to use them, even if it\u0026rsquo;s your own implementation. }\nCopyright  Copyright is automatic In \u0026ldquo;works for hire,\u0026rdquo; the employer holds the copyright In academics, it is customary that researchers control copyright \\onslide{* At UW-Madison:\n\\begin{quote} \\footnotesize \\lolit \u0026ldquo;Except as required by funding agreements or other university policies, the university does not claim ownership rights in the intellectual property generated during research by its faculty, staff, or students.\u0026rdquo; \\end{quote} }\n  Since 1978, works you create are automatically copyrighted. That includes data, software, papers, books, talks, posters, course syllabi, and lecture notes. Since 1989, you don\u0026rsquo;t need to include a copyright notice.\nIn many job situations, your employer will own the copyright on works that you create as part of your employment. But in academic settings, it is traditional that researchers retain copyright on the works they create. But there are exceptions, and universities are increasingly interested in generating income from the intellectual property of their faculty.\nAlso, students may be treated somewhat differently from other employees. Academic staff may be treated differently from faculty, for that matter. Ideally, the exact rules are written down somewhere. Find the rules and read them, and ask questions about them.\nAt UW-Madison, faculty, staff, and students own the intellectual property of the works they produce except in some special cases, such as instructional materials produced with significant university resources. And if you\u0026rsquo;re going to patent something, it must be through the Wisconsin Alumni Research Foundation (WARF).\nExclusive rights under copyright  To make copies of the work To distribute/sell copies of the work To create derivative works To perform the work To display the work publicly  Copyright protection gives the author exclusive rights to the work and to derivatives of the work.\nThus, the default is that no one can copy, modify, or redistribute your code.\nFair use Reproduction for criticism/commentary, teaching, and research\n sep8pt For non-commercial or nonprofit educational purposes Can\u0026rsquo;t be a substantial portion of the work Can\u0026rsquo;t affect the value/market of the original work  There are important limitations to copyright protection.\nWe are allowed to reproduce portions of a work as part of a criticism or commentary, in teaching, or for research.\nThe rules aren\u0026rsquo;t precise.\nQuoting from a work is okay. Posting the full thing on the web is not.\n\\begin{frame}[c]{}\n\\centerline{Breaking copyright \\quad $\\longleftrightarrow$ \\quad plagiarism}\n\\onslide{\\centerline{These are totally different things.}}\nJust in case it\u0026rsquo;s not clear: the tradition of citing one\u0026rsquo;s sources is really totally different from following copyright law.\nWorks in the public domain should still be appropriately cited.\nSoftware licenses  Critical if you want your code to be reused. Also important to protect yourself from lawsuits. I choose between the MIT license and the GPL. Don\u0026rsquo;t use Creative Commons licenses for software!  If you don\u0026rsquo;t indicate a license for your software, others { can\u0026rsquo;t reuse it. You need to be explicit about whether and how your software may be reused, by providing a license.\nI choose between the MIT license and the GNU General Public License (GPL). The MIT license is as open as possible: do whatever you want, just don\u0026rsquo;t sue me. The GPL is \u0026ldquo;viral\u0026rdquo; (they say \u0026ldquo;copyleft\u0026rdquo;) in that extends to derivative works: software that incorporates code under the GPL must also be under the GPL.\nThe Creative Commons licenses may work for data, but they\u0026rsquo;re more for text, music, video, and such things. They {\\nhilit should not} be used for code, as they are not compatible with other standard software licenses, such as the GPL. That means that you wouldn\u0026rsquo;t be able to mix in code that was licensed under the GPL. }\n\\begin{frame}[c]{}\n\\centerline{\\large Pick a license, any license}\n{\\textendash} Jeff Atwood\nI can\u0026rsquo;t emphasize this enough. If you release your software without a license, no one can modify it or incorporate it into their own software, as it\u0026rsquo;s under copyright protection.\nIf you want your software to be reused, pick a license, and make the licensing absolutely clear.\nMIT license Copyright (C) \u0026lt;year\u0026gt; \u0026lt;copyright holders\u0026gt; Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \u0026quot;Software\u0026quot;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \u0026quot;AS IS\u0026quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  The MIT license basically says: do whatever you want with the software, but be sure to include this notice, and don\u0026rsquo;t sue me.\nThose are the key things you want: protect yourself from liability, and make plain that people can be free to reuse the software.\nGPL-3  Use, modify, distribute, \\dots Don\u0026rsquo;t hold the author liable. Distributions must include the source code. Software incorporating the work must also be under GPL-3.  There is an older GPL-2. Use the GPL-3. It was updated to close some loopholes.\nKey additions vs MIT license: distributions of the work or derivatives must include source code, and derivatives must also be licensed under GPL-3.\nFor GPL-3, include this \u0026lt;line with the program's name and a brief idea of what it does.\u0026gt; Copyright (C) \u0026lt;year\u0026gt; \u0026lt;name of author\u0026gt; This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details. You should have received a copy of the GNU General Public License along with this program. If not, see \u0026lt;http://www.gnu.org/licenses/\u0026gt;.  To license your software under the GPL, include a notice like this.\nCreative Commons licenses  CC0 (Public Domain) CC BY (Attribution) CC BY-SA (Attribution-ShareAlike) CC BY-ND (Attribution-NoDerivs) CC BY-NC (Attribution-NonCommercial) CC BY-NC-SA (Attribution-NonCommercial-ShareAlike) CC BY-NC-ND (Attribution-NonCommercial-NoDerivs)  The Creative Commons licenses are really useful for things like manuscripts, data files, videos, web sites, and such.\nYou shouldn\u0026rsquo;t use them for software, as they can\u0026rsquo;t be mixed with the GPL, and they make no explicit mention of source or object code. In the FAQ at Creative Commons, they explicitly recommend {\\nhilit against the use of CC licenses for software.\nBY means people must cite you as the originator.\nSA means that derivative works must be distributed under the same license.\nND means the work must be distributed in its entirety, without any changes.\nNC means the work can\u0026rsquo;t be used in a commercial setting. }\nCC licenses: issues to consider  BY may be an unnecessary hassle. CC-BY on a paper would allow a company to include it in a book  but maybe you don\u0026rsquo;t care ND is {\\vhilit really} restrictive all or none no modifications at all NC means people in a company can\u0026rsquo;t use it at all might not be usable within a course   There are a lot of issues to consider.\nI\u0026rsquo;d recommend avoiding ND and probably also NC.\nPersonally, I\u0026rsquo;m going with CC0 (by academic tradition, people should still cite you) or CC-BY. It means that a company could grab my stuff and make money off of it. But I\u0026rsquo;m fine with that. I\u0026rsquo;d rather see the results of my efforts put to further use.\nData copyright  Individual data points are generally considered facts  Can\u0026rsquo;t be copyrighted Compilations of data can be copyrighted Involves some creativity, so an \u0026ldquo;original work of authorship\u0026rdquo; But someone can just extract and reformat the data  Can assign a license to the data files to prevent extraction and redistribution See \\tt bitlaw.com/copyright/database.html  Data are viewed as facts and so they {\\nhilit can\u0026rsquo;t be copyrighted.\nYour data file or database, though, {\\nhilit can} be copyrighted, if its compilation involves some creativity, and that would generally be true for scientific data files.\nSo people can\u0026rsquo;t redistribute your data files unless you say it\u0026rsquo;s okay. But they may be able to extract and reformat the data and then distribute that.\nIf you want to prevent extraction and redistribution, the data files need a license, which would say the end user is prohibited from extracting data for uses other than intended. }\nKeep data open  Cite the source; cite the relevant papers Talk to the originator of the data  Even if redistribution is legal, don\u0026rsquo;t piss them off. For your own data, use CC0 (public domain)  If you want more control, talk to a lawyer  Statisticians, in particular, should want data to be openly available. And so you should cite the source of data and any relevant papers, not just because that\u0026rsquo;s the academic tradition, but also because we want to reward, as much as possible, people who make data accessible.\nEven if it\u0026rsquo;s perfectly legal for you to re-distribute data, you should talk to the originator of the data before doing so. You don\u0026rsquo;t want them to get annoyed and then stop distributing data in the future.\nIf you want data to be reused, just put it in the public domain. Don\u0026rsquo;t add any extra complexities, like CC-BY.\nIf you want to control reuse or redistribution, talk to a lawyer. It seems really complicated.\nHuman subjects research \\only{* Avoid human subjects research * [] \\only{\\color{background} (just kidding!)} } \\only{ * If there are humans involved, they\u0026rsquo;re human subjects * e.g., surveys * Human subjects research must be reviewed by an Institutional Review Board (IRB) * Not everything is research * e.g., data used solely in a course * Most things are research * If you publish a paper about it, it\u0026rsquo;s research * Anonymized data may be exempt * But the IRB wants to make that determination }\nAny research on human subjects must be reviewed by an IRB.\nIf you\u0026rsquo;re considering publishing a paper about it, and if humans are involved, it\u0026rsquo;s human subjects research. That includes things like surveys. So informed consent, and review by IRB, with a clearly defined protocol and protection of data.\nYou can do a survey {\\nhilit within a class and it may not be research, but then you can\u0026rsquo;t publish the results.\nThe NIH considers the analysis of anonymized human data to be \u0026ldquo;not human subjects research,\u0026rdquo; and it may be exempt from full IRB review, but IRBs generally want to make such determinations themselves: you need to fill out some amount of paperwork. }\nHIPAA  HIPAA = Health Insurance Portability and Accountability Act of 1996 Special rules about medical data with any identifying information  Private Secure Full zip code may be considered identifying information.  Dates of test results are considered identifying information.  HIPAA is really important, but it\u0026rsquo;s also a real pain.\nThe key thing is that medical data with {\\nhilit any identifying information needs a whole bunch of paperwork if transferred/disclosed, and there need to be special security measures.\nAnd the definition of \u0026ldquo;identifying information\u0026rdquo; is surprisingly broad. }\nSummary  Pick a license, any license Use MIT or GPL for software Use CC0 for data Cite sources of software and data Talk to the source of data Be careful with human data  If you\u0026rsquo;re unsure, ask for help    If you don\u0026rsquo;t {\\nhilit license your software, it can\u0026rsquo;t be modified or reused.\nMake data open, and be sure to reward those who make data and software accessible.\nBe careful with human data, particularly if there\u0026rsquo;s anything remotely identifiable. }\n"
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://hbs-rcs.github.io/reproducible-research/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]